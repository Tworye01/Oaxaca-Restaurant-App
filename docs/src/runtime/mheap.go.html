<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Source: mheap.go in package runtime</title>
<link href="../../css/dark-v0.6.8.css" rel="stylesheet">
<script src="../../jvs/golds-v0.6.8.js"></script>
<body onload="onPageLoad()"><div>

<pre id="header"><code><span class="title">Source File</span>
	mheap.go

<span class="title">Belonging Package</span>
	<a href="../../pkg/runtime.html">runtime</a>
</code></pre>

<pre class="line-numbers">
<span class="codeline" id="line-1"><code>// Copyright 2009 The Go Authors. All rights reserved.</code></span>
<span class="codeline" id="line-2"><code>// Use of this source code is governed by a BSD-style</code></span>
<span class="codeline" id="line-3"><code>// license that can be found in the LICENSE file.</code></span>
<span class="codeline" id="line-4"><code></code></span>
<span class="codeline" id="line-5"><code>// Page heap.</code></span>
<span class="codeline" id="line-6"><code>//</code></span>
<span class="codeline" id="line-7"><code>// See malloc.go for overview.</code></span>
<span class="codeline" id="line-8"><code></code></span>
<span class="codeline" id="line-9"><code>package runtime</code></span>
<span class="codeline" id="line-10"><code></code></span>
<span class="codeline" id="line-11"><code>import (</code></span>
<span class="codeline" id="line-12"><code>	"internal/cpu"</code></span>
<span class="codeline" id="line-13"><code>	"internal/goarch"</code></span>
<span class="codeline" id="line-14"><code>	"internal/goexperiment"</code></span>
<span class="codeline" id="line-15"><code>	"runtime/internal/atomic"</code></span>
<span class="codeline" id="line-16"><code>	"runtime/internal/sys"</code></span>
<span class="codeline" id="line-17"><code>	"unsafe"</code></span>
<span class="codeline" id="line-18"><code>)</code></span>
<span class="codeline" id="line-19"><code></code></span>
<span class="codeline" id="line-20"><code>const (</code></span>
<span class="codeline" id="line-21"><code>	// minPhysPageSize is a lower-bound on the physical page size. The</code></span>
<span class="codeline" id="line-22"><code>	// true physical page size may be larger than this. In contrast,</code></span>
<span class="codeline" id="line-23"><code>	// sys.PhysPageSize is an upper-bound on the physical page size.</code></span>
<span class="codeline" id="line-24"><code>	minPhysPageSize = 4096</code></span>
<span class="codeline" id="line-25"><code></code></span>
<span class="codeline" id="line-26"><code>	// maxPhysPageSize is the maximum page size the runtime supports.</code></span>
<span class="codeline" id="line-27"><code>	maxPhysPageSize = 512 &lt;&lt; 10</code></span>
<span class="codeline" id="line-28"><code></code></span>
<span class="codeline" id="line-29"><code>	// maxPhysHugePageSize sets an upper-bound on the maximum huge page size</code></span>
<span class="codeline" id="line-30"><code>	// that the runtime supports.</code></span>
<span class="codeline" id="line-31"><code>	maxPhysHugePageSize = pallocChunkBytes</code></span>
<span class="codeline" id="line-32"><code></code></span>
<span class="codeline" id="line-33"><code>	// pagesPerReclaimerChunk indicates how many pages to scan from the</code></span>
<span class="codeline" id="line-34"><code>	// pageInUse bitmap at a time. Used by the page reclaimer.</code></span>
<span class="codeline" id="line-35"><code>	//</code></span>
<span class="codeline" id="line-36"><code>	// Higher values reduce contention on scanning indexes (such as</code></span>
<span class="codeline" id="line-37"><code>	// h.reclaimIndex), but increase the minimum latency of the</code></span>
<span class="codeline" id="line-38"><code>	// operation.</code></span>
<span class="codeline" id="line-39"><code>	//</code></span>
<span class="codeline" id="line-40"><code>	// The time required to scan this many pages can vary a lot depending</code></span>
<span class="codeline" id="line-41"><code>	// on how many spans are actually freed. Experimentally, it can</code></span>
<span class="codeline" id="line-42"><code>	// scan for pages at ~300 GB/ms on a 2.6GHz Core i7, but can only</code></span>
<span class="codeline" id="line-43"><code>	// free spans at ~32 MB/ms. Using 512 pages bounds this at</code></span>
<span class="codeline" id="line-44"><code>	// roughly 100Âµs.</code></span>
<span class="codeline" id="line-45"><code>	//</code></span>
<span class="codeline" id="line-46"><code>	// Must be a multiple of the pageInUse bitmap element size and</code></span>
<span class="codeline" id="line-47"><code>	// must also evenly divide pagesPerArena.</code></span>
<span class="codeline" id="line-48"><code>	pagesPerReclaimerChunk = 512</code></span>
<span class="codeline" id="line-49"><code></code></span>
<span class="codeline" id="line-50"><code>	// physPageAlignedStacks indicates whether stack allocations must be</code></span>
<span class="codeline" id="line-51"><code>	// physical page aligned. This is a requirement for MAP_STACK on</code></span>
<span class="codeline" id="line-52"><code>	// OpenBSD.</code></span>
<span class="codeline" id="line-53"><code>	physPageAlignedStacks = GOOS == "openbsd"</code></span>
<span class="codeline" id="line-54"><code>)</code></span>
<span class="codeline" id="line-55"><code></code></span>
<span class="codeline" id="line-56"><code>// Main malloc heap.</code></span>
<span class="codeline" id="line-57"><code>// The heap itself is the "free" and "scav" treaps,</code></span>
<span class="codeline" id="line-58"><code>// but all the other global data is here too.</code></span>
<span class="codeline" id="line-59"><code>//</code></span>
<span class="codeline" id="line-60"><code>// mheap must not be heap-allocated because it contains mSpanLists,</code></span>
<span class="codeline" id="line-61"><code>// which must not be heap-allocated.</code></span>
<span class="codeline" id="line-62"><code>type mheap struct {</code></span>
<span class="codeline" id="line-63"><code>	_ sys.NotInHeap</code></span>
<span class="codeline" id="line-64"><code></code></span>
<span class="codeline" id="line-65"><code>	// lock must only be acquired on the system stack, otherwise a g</code></span>
<span class="codeline" id="line-66"><code>	// could self-deadlock if its stack grows with the lock held.</code></span>
<span class="codeline" id="line-67"><code>	lock mutex</code></span>
<span class="codeline" id="line-68"><code></code></span>
<span class="codeline" id="line-69"><code>	pages pageAlloc // page allocation data structure</code></span>
<span class="codeline" id="line-70"><code></code></span>
<span class="codeline" id="line-71"><code>	sweepgen uint32 // sweep generation, see comment in mspan; written during STW</code></span>
<span class="codeline" id="line-72"><code></code></span>
<span class="codeline" id="line-73"><code>	// allspans is a slice of all mspans ever created. Each mspan</code></span>
<span class="codeline" id="line-74"><code>	// appears exactly once.</code></span>
<span class="codeline" id="line-75"><code>	//</code></span>
<span class="codeline" id="line-76"><code>	// The memory for allspans is manually managed and can be</code></span>
<span class="codeline" id="line-77"><code>	// reallocated and move as the heap grows.</code></span>
<span class="codeline" id="line-78"><code>	//</code></span>
<span class="codeline" id="line-79"><code>	// In general, allspans is protected by mheap_.lock, which</code></span>
<span class="codeline" id="line-80"><code>	// prevents concurrent access as well as freeing the backing</code></span>
<span class="codeline" id="line-81"><code>	// store. Accesses during STW might not hold the lock, but</code></span>
<span class="codeline" id="line-82"><code>	// must ensure that allocation cannot happen around the</code></span>
<span class="codeline" id="line-83"><code>	// access (since that may free the backing store).</code></span>
<span class="codeline" id="line-84"><code>	allspans []*mspan // all spans out there</code></span>
<span class="codeline" id="line-85"><code></code></span>
<span class="codeline" id="line-86"><code>	// Proportional sweep</code></span>
<span class="codeline" id="line-87"><code>	//</code></span>
<span class="codeline" id="line-88"><code>	// These parameters represent a linear function from gcController.heapLive</code></span>
<span class="codeline" id="line-89"><code>	// to page sweep count. The proportional sweep system works to</code></span>
<span class="codeline" id="line-90"><code>	// stay in the black by keeping the current page sweep count</code></span>
<span class="codeline" id="line-91"><code>	// above this line at the current gcController.heapLive.</code></span>
<span class="codeline" id="line-92"><code>	//</code></span>
<span class="codeline" id="line-93"><code>	// The line has slope sweepPagesPerByte and passes through a</code></span>
<span class="codeline" id="line-94"><code>	// basis point at (sweepHeapLiveBasis, pagesSweptBasis). At</code></span>
<span class="codeline" id="line-95"><code>	// any given time, the system is at (gcController.heapLive,</code></span>
<span class="codeline" id="line-96"><code>	// pagesSwept) in this space.</code></span>
<span class="codeline" id="line-97"><code>	//</code></span>
<span class="codeline" id="line-98"><code>	// It is important that the line pass through a point we</code></span>
<span class="codeline" id="line-99"><code>	// control rather than simply starting at a 0,0 origin</code></span>
<span class="codeline" id="line-100"><code>	// because that lets us adjust sweep pacing at any time while</code></span>
<span class="codeline" id="line-101"><code>	// accounting for current progress. If we could only adjust</code></span>
<span class="codeline" id="line-102"><code>	// the slope, it would create a discontinuity in debt if any</code></span>
<span class="codeline" id="line-103"><code>	// progress has already been made.</code></span>
<span class="codeline" id="line-104"><code>	pagesInUse         atomic.Uintptr // pages of spans in stats mSpanInUse</code></span>
<span class="codeline" id="line-105"><code>	pagesSwept         atomic.Uint64  // pages swept this cycle</code></span>
<span class="codeline" id="line-106"><code>	pagesSweptBasis    atomic.Uint64  // pagesSwept to use as the origin of the sweep ratio</code></span>
<span class="codeline" id="line-107"><code>	sweepHeapLiveBasis uint64         // value of gcController.heapLive to use as the origin of sweep ratio; written with lock, read without</code></span>
<span class="codeline" id="line-108"><code>	sweepPagesPerByte  float64        // proportional sweep ratio; written with lock, read without</code></span>
<span class="codeline" id="line-109"><code></code></span>
<span class="codeline" id="line-110"><code>	// Page reclaimer state</code></span>
<span class="codeline" id="line-111"><code></code></span>
<span class="codeline" id="line-112"><code>	// reclaimIndex is the page index in allArenas of next page to</code></span>
<span class="codeline" id="line-113"><code>	// reclaim. Specifically, it refers to page (i %</code></span>
<span class="codeline" id="line-114"><code>	// pagesPerArena) of arena allArenas[i / pagesPerArena].</code></span>
<span class="codeline" id="line-115"><code>	//</code></span>
<span class="codeline" id="line-116"><code>	// If this is &gt;= 1&lt;&lt;63, the page reclaimer is done scanning</code></span>
<span class="codeline" id="line-117"><code>	// the page marks.</code></span>
<span class="codeline" id="line-118"><code>	reclaimIndex atomic.Uint64</code></span>
<span class="codeline" id="line-119"><code></code></span>
<span class="codeline" id="line-120"><code>	// reclaimCredit is spare credit for extra pages swept. Since</code></span>
<span class="codeline" id="line-121"><code>	// the page reclaimer works in large chunks, it may reclaim</code></span>
<span class="codeline" id="line-122"><code>	// more than requested. Any spare pages released go to this</code></span>
<span class="codeline" id="line-123"><code>	// credit pool.</code></span>
<span class="codeline" id="line-124"><code>	reclaimCredit atomic.Uintptr</code></span>
<span class="codeline" id="line-125"><code></code></span>
<span class="codeline" id="line-126"><code>	_ cpu.CacheLinePad // prevents false-sharing between arenas and preceding variables</code></span>
<span class="codeline" id="line-127"><code></code></span>
<span class="codeline" id="line-128"><code>	// arenas is the heap arena map. It points to the metadata for</code></span>
<span class="codeline" id="line-129"><code>	// the heap for every arena frame of the entire usable virtual</code></span>
<span class="codeline" id="line-130"><code>	// address space.</code></span>
<span class="codeline" id="line-131"><code>	//</code></span>
<span class="codeline" id="line-132"><code>	// Use arenaIndex to compute indexes into this array.</code></span>
<span class="codeline" id="line-133"><code>	//</code></span>
<span class="codeline" id="line-134"><code>	// For regions of the address space that are not backed by the</code></span>
<span class="codeline" id="line-135"><code>	// Go heap, the arena map contains nil.</code></span>
<span class="codeline" id="line-136"><code>	//</code></span>
<span class="codeline" id="line-137"><code>	// Modifications are protected by mheap_.lock. Reads can be</code></span>
<span class="codeline" id="line-138"><code>	// performed without locking; however, a given entry can</code></span>
<span class="codeline" id="line-139"><code>	// transition from nil to non-nil at any time when the lock</code></span>
<span class="codeline" id="line-140"><code>	// isn't held. (Entries never transitions back to nil.)</code></span>
<span class="codeline" id="line-141"><code>	//</code></span>
<span class="codeline" id="line-142"><code>	// In general, this is a two-level mapping consisting of an L1</code></span>
<span class="codeline" id="line-143"><code>	// map and possibly many L2 maps. This saves space when there</code></span>
<span class="codeline" id="line-144"><code>	// are a huge number of arena frames. However, on many</code></span>
<span class="codeline" id="line-145"><code>	// platforms (even 64-bit), arenaL1Bits is 0, making this</code></span>
<span class="codeline" id="line-146"><code>	// effectively a single-level map. In this case, arenas[0]</code></span>
<span class="codeline" id="line-147"><code>	// will never be nil.</code></span>
<span class="codeline" id="line-148"><code>	arenas [1 &lt;&lt; arenaL1Bits]*[1 &lt;&lt; arenaL2Bits]*heapArena</code></span>
<span class="codeline" id="line-149"><code></code></span>
<span class="codeline" id="line-150"><code>	// arenasHugePages indicates whether arenas' L2 entries are eligible</code></span>
<span class="codeline" id="line-151"><code>	// to be backed by huge pages.</code></span>
<span class="codeline" id="line-152"><code>	arenasHugePages bool</code></span>
<span class="codeline" id="line-153"><code></code></span>
<span class="codeline" id="line-154"><code>	// heapArenaAlloc is pre-reserved space for allocating heapArena</code></span>
<span class="codeline" id="line-155"><code>	// objects. This is only used on 32-bit, where we pre-reserve</code></span>
<span class="codeline" id="line-156"><code>	// this space to avoid interleaving it with the heap itself.</code></span>
<span class="codeline" id="line-157"><code>	heapArenaAlloc linearAlloc</code></span>
<span class="codeline" id="line-158"><code></code></span>
<span class="codeline" id="line-159"><code>	// arenaHints is a list of addresses at which to attempt to</code></span>
<span class="codeline" id="line-160"><code>	// add more heap arenas. This is initially populated with a</code></span>
<span class="codeline" id="line-161"><code>	// set of general hint addresses, and grown with the bounds of</code></span>
<span class="codeline" id="line-162"><code>	// actual heap arena ranges.</code></span>
<span class="codeline" id="line-163"><code>	arenaHints *arenaHint</code></span>
<span class="codeline" id="line-164"><code></code></span>
<span class="codeline" id="line-165"><code>	// arena is a pre-reserved space for allocating heap arenas</code></span>
<span class="codeline" id="line-166"><code>	// (the actual arenas). This is only used on 32-bit.</code></span>
<span class="codeline" id="line-167"><code>	arena linearAlloc</code></span>
<span class="codeline" id="line-168"><code></code></span>
<span class="codeline" id="line-169"><code>	// allArenas is the arenaIndex of every mapped arena. This can</code></span>
<span class="codeline" id="line-170"><code>	// be used to iterate through the address space.</code></span>
<span class="codeline" id="line-171"><code>	//</code></span>
<span class="codeline" id="line-172"><code>	// Access is protected by mheap_.lock. However, since this is</code></span>
<span class="codeline" id="line-173"><code>	// append-only and old backing arrays are never freed, it is</code></span>
<span class="codeline" id="line-174"><code>	// safe to acquire mheap_.lock, copy the slice header, and</code></span>
<span class="codeline" id="line-175"><code>	// then release mheap_.lock.</code></span>
<span class="codeline" id="line-176"><code>	allArenas []arenaIdx</code></span>
<span class="codeline" id="line-177"><code></code></span>
<span class="codeline" id="line-178"><code>	// sweepArenas is a snapshot of allArenas taken at the</code></span>
<span class="codeline" id="line-179"><code>	// beginning of the sweep cycle. This can be read safely by</code></span>
<span class="codeline" id="line-180"><code>	// simply blocking GC (by disabling preemption).</code></span>
<span class="codeline" id="line-181"><code>	sweepArenas []arenaIdx</code></span>
<span class="codeline" id="line-182"><code></code></span>
<span class="codeline" id="line-183"><code>	// markArenas is a snapshot of allArenas taken at the beginning</code></span>
<span class="codeline" id="line-184"><code>	// of the mark cycle. Because allArenas is append-only, neither</code></span>
<span class="codeline" id="line-185"><code>	// this slice nor its contents will change during the mark, so</code></span>
<span class="codeline" id="line-186"><code>	// it can be read safely.</code></span>
<span class="codeline" id="line-187"><code>	markArenas []arenaIdx</code></span>
<span class="codeline" id="line-188"><code></code></span>
<span class="codeline" id="line-189"><code>	// curArena is the arena that the heap is currently growing</code></span>
<span class="codeline" id="line-190"><code>	// into. This should always be physPageSize-aligned.</code></span>
<span class="codeline" id="line-191"><code>	curArena struct {</code></span>
<span class="codeline" id="line-192"><code>		base, end uintptr</code></span>
<span class="codeline" id="line-193"><code>	}</code></span>
<span class="codeline" id="line-194"><code></code></span>
<span class="codeline" id="line-195"><code>	// central free lists for small size classes.</code></span>
<span class="codeline" id="line-196"><code>	// the padding makes sure that the mcentrals are</code></span>
<span class="codeline" id="line-197"><code>	// spaced CacheLinePadSize bytes apart, so that each mcentral.lock</code></span>
<span class="codeline" id="line-198"><code>	// gets its own cache line.</code></span>
<span class="codeline" id="line-199"><code>	// central is indexed by spanClass.</code></span>
<span class="codeline" id="line-200"><code>	central [numSpanClasses]struct {</code></span>
<span class="codeline" id="line-201"><code>		mcentral mcentral</code></span>
<span class="codeline" id="line-202"><code>		pad      [(cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize) % cpu.CacheLinePadSize]byte</code></span>
<span class="codeline" id="line-203"><code>	}</code></span>
<span class="codeline" id="line-204"><code></code></span>
<span class="codeline" id="line-205"><code>	spanalloc              fixalloc // allocator for span*</code></span>
<span class="codeline" id="line-206"><code>	cachealloc             fixalloc // allocator for mcache*</code></span>
<span class="codeline" id="line-207"><code>	specialfinalizeralloc  fixalloc // allocator for specialfinalizer*</code></span>
<span class="codeline" id="line-208"><code>	specialprofilealloc    fixalloc // allocator for specialprofile*</code></span>
<span class="codeline" id="line-209"><code>	specialReachableAlloc  fixalloc // allocator for specialReachable</code></span>
<span class="codeline" id="line-210"><code>	specialPinCounterAlloc fixalloc // allocator for specialPinCounter</code></span>
<span class="codeline" id="line-211"><code>	speciallock            mutex    // lock for special record allocators.</code></span>
<span class="codeline" id="line-212"><code>	arenaHintAlloc         fixalloc // allocator for arenaHints</code></span>
<span class="codeline" id="line-213"><code></code></span>
<span class="codeline" id="line-214"><code>	// User arena state.</code></span>
<span class="codeline" id="line-215"><code>	//</code></span>
<span class="codeline" id="line-216"><code>	// Protected by mheap_.lock.</code></span>
<span class="codeline" id="line-217"><code>	userArena struct {</code></span>
<span class="codeline" id="line-218"><code>		// arenaHints is a list of addresses at which to attempt to</code></span>
<span class="codeline" id="line-219"><code>		// add more heap arenas for user arena chunks. This is initially</code></span>
<span class="codeline" id="line-220"><code>		// populated with a set of general hint addresses, and grown with</code></span>
<span class="codeline" id="line-221"><code>		// the bounds of actual heap arena ranges.</code></span>
<span class="codeline" id="line-222"><code>		arenaHints *arenaHint</code></span>
<span class="codeline" id="line-223"><code></code></span>
<span class="codeline" id="line-224"><code>		// quarantineList is a list of user arena spans that have been set to fault, but</code></span>
<span class="codeline" id="line-225"><code>		// are waiting for all pointers into them to go away. Sweeping handles</code></span>
<span class="codeline" id="line-226"><code>		// identifying when this is true, and moves the span to the ready list.</code></span>
<span class="codeline" id="line-227"><code>		quarantineList mSpanList</code></span>
<span class="codeline" id="line-228"><code></code></span>
<span class="codeline" id="line-229"><code>		// readyList is a list of empty user arena spans that are ready for reuse.</code></span>
<span class="codeline" id="line-230"><code>		readyList mSpanList</code></span>
<span class="codeline" id="line-231"><code>	}</code></span>
<span class="codeline" id="line-232"><code></code></span>
<span class="codeline" id="line-233"><code>	unused *specialfinalizer // never set, just here to force the specialfinalizer type into DWARF</code></span>
<span class="codeline" id="line-234"><code>}</code></span>
<span class="codeline" id="line-235"><code></code></span>
<span class="codeline" id="line-236"><code>var mheap_ mheap</code></span>
<span class="codeline" id="line-237"><code></code></span>
<span class="codeline" id="line-238"><code>// A heapArena stores metadata for a heap arena. heapArenas are stored</code></span>
<span class="codeline" id="line-239"><code>// outside of the Go heap and accessed via the mheap_.arenas index.</code></span>
<span class="codeline" id="line-240"><code>type heapArena struct {</code></span>
<span class="codeline" id="line-241"><code>	_ sys.NotInHeap</code></span>
<span class="codeline" id="line-242"><code></code></span>
<span class="codeline" id="line-243"><code>	// heapArenaPtrScalar contains pointer/scalar data about the heap for this heap arena.</code></span>
<span class="codeline" id="line-244"><code>	heapArenaPtrScalar</code></span>
<span class="codeline" id="line-245"><code></code></span>
<span class="codeline" id="line-246"><code>	// spans maps from virtual address page ID within this arena to *mspan.</code></span>
<span class="codeline" id="line-247"><code>	// For allocated spans, their pages map to the span itself.</code></span>
<span class="codeline" id="line-248"><code>	// For free spans, only the lowest and highest pages map to the span itself.</code></span>
<span class="codeline" id="line-249"><code>	// Internal pages map to an arbitrary span.</code></span>
<span class="codeline" id="line-250"><code>	// For pages that have never been allocated, spans entries are nil.</code></span>
<span class="codeline" id="line-251"><code>	//</code></span>
<span class="codeline" id="line-252"><code>	// Modifications are protected by mheap.lock. Reads can be</code></span>
<span class="codeline" id="line-253"><code>	// performed without locking, but ONLY from indexes that are</code></span>
<span class="codeline" id="line-254"><code>	// known to contain in-use or stack spans. This means there</code></span>
<span class="codeline" id="line-255"><code>	// must not be a safe-point between establishing that an</code></span>
<span class="codeline" id="line-256"><code>	// address is live and looking it up in the spans array.</code></span>
<span class="codeline" id="line-257"><code>	spans [pagesPerArena]*mspan</code></span>
<span class="codeline" id="line-258"><code></code></span>
<span class="codeline" id="line-259"><code>	// pageInUse is a bitmap that indicates which spans are in</code></span>
<span class="codeline" id="line-260"><code>	// state mSpanInUse. This bitmap is indexed by page number,</code></span>
<span class="codeline" id="line-261"><code>	// but only the bit corresponding to the first page in each</code></span>
<span class="codeline" id="line-262"><code>	// span is used.</code></span>
<span class="codeline" id="line-263"><code>	//</code></span>
<span class="codeline" id="line-264"><code>	// Reads and writes are atomic.</code></span>
<span class="codeline" id="line-265"><code>	pageInUse [pagesPerArena / 8]uint8</code></span>
<span class="codeline" id="line-266"><code></code></span>
<span class="codeline" id="line-267"><code>	// pageMarks is a bitmap that indicates which spans have any</code></span>
<span class="codeline" id="line-268"><code>	// marked objects on them. Like pageInUse, only the bit</code></span>
<span class="codeline" id="line-269"><code>	// corresponding to the first page in each span is used.</code></span>
<span class="codeline" id="line-270"><code>	//</code></span>
<span class="codeline" id="line-271"><code>	// Writes are done atomically during marking. Reads are</code></span>
<span class="codeline" id="line-272"><code>	// non-atomic and lock-free since they only occur during</code></span>
<span class="codeline" id="line-273"><code>	// sweeping (and hence never race with writes).</code></span>
<span class="codeline" id="line-274"><code>	//</code></span>
<span class="codeline" id="line-275"><code>	// This is used to quickly find whole spans that can be freed.</code></span>
<span class="codeline" id="line-276"><code>	//</code></span>
<span class="codeline" id="line-277"><code>	// TODO(austin): It would be nice if this was uint64 for</code></span>
<span class="codeline" id="line-278"><code>	// faster scanning, but we don't have 64-bit atomic bit</code></span>
<span class="codeline" id="line-279"><code>	// operations.</code></span>
<span class="codeline" id="line-280"><code>	pageMarks [pagesPerArena / 8]uint8</code></span>
<span class="codeline" id="line-281"><code></code></span>
<span class="codeline" id="line-282"><code>	// pageSpecials is a bitmap that indicates which spans have</code></span>
<span class="codeline" id="line-283"><code>	// specials (finalizers or other). Like pageInUse, only the bit</code></span>
<span class="codeline" id="line-284"><code>	// corresponding to the first page in each span is used.</code></span>
<span class="codeline" id="line-285"><code>	//</code></span>
<span class="codeline" id="line-286"><code>	// Writes are done atomically whenever a special is added to</code></span>
<span class="codeline" id="line-287"><code>	// a span and whenever the last special is removed from a span.</code></span>
<span class="codeline" id="line-288"><code>	// Reads are done atomically to find spans containing specials</code></span>
<span class="codeline" id="line-289"><code>	// during marking.</code></span>
<span class="codeline" id="line-290"><code>	pageSpecials [pagesPerArena / 8]uint8</code></span>
<span class="codeline" id="line-291"><code></code></span>
<span class="codeline" id="line-292"><code>	// checkmarks stores the debug.gccheckmark state. It is only</code></span>
<span class="codeline" id="line-293"><code>	// used if debug.gccheckmark &gt; 0.</code></span>
<span class="codeline" id="line-294"><code>	checkmarks *checkmarksMap</code></span>
<span class="codeline" id="line-295"><code></code></span>
<span class="codeline" id="line-296"><code>	// zeroedBase marks the first byte of the first page in this</code></span>
<span class="codeline" id="line-297"><code>	// arena which hasn't been used yet and is therefore already</code></span>
<span class="codeline" id="line-298"><code>	// zero. zeroedBase is relative to the arena base.</code></span>
<span class="codeline" id="line-299"><code>	// Increases monotonically until it hits heapArenaBytes.</code></span>
<span class="codeline" id="line-300"><code>	//</code></span>
<span class="codeline" id="line-301"><code>	// This field is sufficient to determine if an allocation</code></span>
<span class="codeline" id="line-302"><code>	// needs to be zeroed because the page allocator follows an</code></span>
<span class="codeline" id="line-303"><code>	// address-ordered first-fit policy.</code></span>
<span class="codeline" id="line-304"><code>	//</code></span>
<span class="codeline" id="line-305"><code>	// Read atomically and written with an atomic CAS.</code></span>
<span class="codeline" id="line-306"><code>	zeroedBase uintptr</code></span>
<span class="codeline" id="line-307"><code>}</code></span>
<span class="codeline" id="line-308"><code></code></span>
<span class="codeline" id="line-309"><code>// arenaHint is a hint for where to grow the heap arenas. See</code></span>
<span class="codeline" id="line-310"><code>// mheap_.arenaHints.</code></span>
<span class="codeline" id="line-311"><code>type arenaHint struct {</code></span>
<span class="codeline" id="line-312"><code>	_    sys.NotInHeap</code></span>
<span class="codeline" id="line-313"><code>	addr uintptr</code></span>
<span class="codeline" id="line-314"><code>	down bool</code></span>
<span class="codeline" id="line-315"><code>	next *arenaHint</code></span>
<span class="codeline" id="line-316"><code>}</code></span>
<span class="codeline" id="line-317"><code></code></span>
<span class="codeline" id="line-318"><code>// An mspan is a run of pages.</code></span>
<span class="codeline" id="line-319"><code>//</code></span>
<span class="codeline" id="line-320"><code>// When a mspan is in the heap free treap, state == mSpanFree</code></span>
<span class="codeline" id="line-321"><code>// and heapmap(s-&gt;start) == span, heapmap(s-&gt;start+s-&gt;npages-1) == span.</code></span>
<span class="codeline" id="line-322"><code>// If the mspan is in the heap scav treap, then in addition to the</code></span>
<span class="codeline" id="line-323"><code>// above scavenged == true. scavenged == false in all other cases.</code></span>
<span class="codeline" id="line-324"><code>//</code></span>
<span class="codeline" id="line-325"><code>// When a mspan is allocated, state == mSpanInUse or mSpanManual</code></span>
<span class="codeline" id="line-326"><code>// and heapmap(i) == span for all s-&gt;start &lt;= i &lt; s-&gt;start+s-&gt;npages.</code></span>
<span class="codeline" id="line-327"><code></code></span>
<span class="codeline" id="line-328"><code>// Every mspan is in one doubly-linked list, either in the mheap's</code></span>
<span class="codeline" id="line-329"><code>// busy list or one of the mcentral's span lists.</code></span>
<span class="codeline" id="line-330"><code></code></span>
<span class="codeline" id="line-331"><code>// An mspan representing actual memory has state mSpanInUse,</code></span>
<span class="codeline" id="line-332"><code>// mSpanManual, or mSpanFree. Transitions between these states are</code></span>
<span class="codeline" id="line-333"><code>// constrained as follows:</code></span>
<span class="codeline" id="line-334"><code>//</code></span>
<span class="codeline" id="line-335"><code>//   - A span may transition from free to in-use or manual during any GC</code></span>
<span class="codeline" id="line-336"><code>//     phase.</code></span>
<span class="codeline" id="line-337"><code>//</code></span>
<span class="codeline" id="line-338"><code>//   - During sweeping (gcphase == _GCoff), a span may transition from</code></span>
<span class="codeline" id="line-339"><code>//     in-use to free (as a result of sweeping) or manual to free (as a</code></span>
<span class="codeline" id="line-340"><code>//     result of stacks being freed).</code></span>
<span class="codeline" id="line-341"><code>//</code></span>
<span class="codeline" id="line-342"><code>//   - During GC (gcphase != _GCoff), a span *must not* transition from</code></span>
<span class="codeline" id="line-343"><code>//     manual or in-use to free. Because concurrent GC may read a pointer</code></span>
<span class="codeline" id="line-344"><code>//     and then look up its span, the span state must be monotonic.</code></span>
<span class="codeline" id="line-345"><code>//</code></span>
<span class="codeline" id="line-346"><code>// Setting mspan.state to mSpanInUse or mSpanManual must be done</code></span>
<span class="codeline" id="line-347"><code>// atomically and only after all other span fields are valid.</code></span>
<span class="codeline" id="line-348"><code>// Likewise, if inspecting a span is contingent on it being</code></span>
<span class="codeline" id="line-349"><code>// mSpanInUse, the state should be loaded atomically and checked</code></span>
<span class="codeline" id="line-350"><code>// before depending on other fields. This allows the garbage collector</code></span>
<span class="codeline" id="line-351"><code>// to safely deal with potentially invalid pointers, since resolving</code></span>
<span class="codeline" id="line-352"><code>// such pointers may race with a span being allocated.</code></span>
<span class="codeline" id="line-353"><code>type mSpanState uint8</code></span>
<span class="codeline" id="line-354"><code></code></span>
<span class="codeline" id="line-355"><code>const (</code></span>
<span class="codeline" id="line-356"><code>	mSpanDead   mSpanState = iota</code></span>
<span class="codeline" id="line-357"><code>	mSpanInUse             // allocated for garbage collected heap</code></span>
<span class="codeline" id="line-358"><code>	mSpanManual            // allocated for manual management (e.g., stack allocator)</code></span>
<span class="codeline" id="line-359"><code>)</code></span>
<span class="codeline" id="line-360"><code></code></span>
<span class="codeline" id="line-361"><code>// mSpanStateNames are the names of the span states, indexed by</code></span>
<span class="codeline" id="line-362"><code>// mSpanState.</code></span>
<span class="codeline" id="line-363"><code>var mSpanStateNames = []string{</code></span>
<span class="codeline" id="line-364"><code>	"mSpanDead",</code></span>
<span class="codeline" id="line-365"><code>	"mSpanInUse",</code></span>
<span class="codeline" id="line-366"><code>	"mSpanManual",</code></span>
<span class="codeline" id="line-367"><code>}</code></span>
<span class="codeline" id="line-368"><code></code></span>
<span class="codeline" id="line-369"><code>// mSpanStateBox holds an atomic.Uint8 to provide atomic operations on</code></span>
<span class="codeline" id="line-370"><code>// an mSpanState. This is a separate type to disallow accidental comparison</code></span>
<span class="codeline" id="line-371"><code>// or assignment with mSpanState.</code></span>
<span class="codeline" id="line-372"><code>type mSpanStateBox struct {</code></span>
<span class="codeline" id="line-373"><code>	s atomic.Uint8</code></span>
<span class="codeline" id="line-374"><code>}</code></span>
<span class="codeline" id="line-375"><code></code></span>
<span class="codeline" id="line-376"><code>// It is nosplit to match get, below.</code></span>
<span class="codeline" id="line-377"><code></code></span>
<span class="codeline" id="line-378"><code>//go:nosplit</code></span>
<span class="codeline" id="line-379"><code>func (b *mSpanStateBox) set(s mSpanState) {</code></span>
<span class="codeline" id="line-380"><code>	b.s.Store(uint8(s))</code></span>
<span class="codeline" id="line-381"><code>}</code></span>
<span class="codeline" id="line-382"><code></code></span>
<span class="codeline" id="line-383"><code>// It is nosplit because it's called indirectly by typedmemclr,</code></span>
<span class="codeline" id="line-384"><code>// which must not be preempted.</code></span>
<span class="codeline" id="line-385"><code></code></span>
<span class="codeline" id="line-386"><code>//go:nosplit</code></span>
<span class="codeline" id="line-387"><code>func (b *mSpanStateBox) get() mSpanState {</code></span>
<span class="codeline" id="line-388"><code>	return mSpanState(b.s.Load())</code></span>
<span class="codeline" id="line-389"><code>}</code></span>
<span class="codeline" id="line-390"><code></code></span>
<span class="codeline" id="line-391"><code>// mSpanList heads a linked list of spans.</code></span>
<span class="codeline" id="line-392"><code>type mSpanList struct {</code></span>
<span class="codeline" id="line-393"><code>	_     sys.NotInHeap</code></span>
<span class="codeline" id="line-394"><code>	first *mspan // first span in list, or nil if none</code></span>
<span class="codeline" id="line-395"><code>	last  *mspan // last span in list, or nil if none</code></span>
<span class="codeline" id="line-396"><code>}</code></span>
<span class="codeline" id="line-397"><code></code></span>
<span class="codeline" id="line-398"><code>type mspan struct {</code></span>
<span class="codeline" id="line-399"><code>	_    sys.NotInHeap</code></span>
<span class="codeline" id="line-400"><code>	next *mspan     // next span in list, or nil if none</code></span>
<span class="codeline" id="line-401"><code>	prev *mspan     // previous span in list, or nil if none</code></span>
<span class="codeline" id="line-402"><code>	list *mSpanList // For debugging.</code></span>
<span class="codeline" id="line-403"><code></code></span>
<span class="codeline" id="line-404"><code>	startAddr uintptr // address of first byte of span aka s.base()</code></span>
<span class="codeline" id="line-405"><code>	npages    uintptr // number of pages in span</code></span>
<span class="codeline" id="line-406"><code></code></span>
<span class="codeline" id="line-407"><code>	manualFreeList gclinkptr // list of free objects in mSpanManual spans</code></span>
<span class="codeline" id="line-408"><code></code></span>
<span class="codeline" id="line-409"><code>	// freeindex is the slot index between 0 and nelems at which to begin scanning</code></span>
<span class="codeline" id="line-410"><code>	// for the next free object in this span.</code></span>
<span class="codeline" id="line-411"><code>	// Each allocation scans allocBits starting at freeindex until it encounters a 0</code></span>
<span class="codeline" id="line-412"><code>	// indicating a free object. freeindex is then adjusted so that subsequent scans begin</code></span>
<span class="codeline" id="line-413"><code>	// just past the newly discovered free object.</code></span>
<span class="codeline" id="line-414"><code>	//</code></span>
<span class="codeline" id="line-415"><code>	// If freeindex == nelem, this span has no free objects.</code></span>
<span class="codeline" id="line-416"><code>	//</code></span>
<span class="codeline" id="line-417"><code>	// allocBits is a bitmap of objects in this span.</code></span>
<span class="codeline" id="line-418"><code>	// If n &gt;= freeindex and allocBits[n/8] &amp; (1&lt;&lt;(n%8)) is 0</code></span>
<span class="codeline" id="line-419"><code>	// then object n is free;</code></span>
<span class="codeline" id="line-420"><code>	// otherwise, object n is allocated. Bits starting at nelem are</code></span>
<span class="codeline" id="line-421"><code>	// undefined and should never be referenced.</code></span>
<span class="codeline" id="line-422"><code>	//</code></span>
<span class="codeline" id="line-423"><code>	// Object n starts at address n*elemsize + (start &lt;&lt; pageShift).</code></span>
<span class="codeline" id="line-424"><code>	freeindex uint16</code></span>
<span class="codeline" id="line-425"><code>	// TODO: Look up nelems from sizeclass and remove this field if it</code></span>
<span class="codeline" id="line-426"><code>	// helps performance.</code></span>
<span class="codeline" id="line-427"><code>	nelems uint16 // number of object in the span.</code></span>
<span class="codeline" id="line-428"><code>	// freeIndexForScan is like freeindex, except that freeindex is</code></span>
<span class="codeline" id="line-429"><code>	// used by the allocator whereas freeIndexForScan is used by the</code></span>
<span class="codeline" id="line-430"><code>	// GC scanner. They are two fields so that the GC sees the object</code></span>
<span class="codeline" id="line-431"><code>	// is allocated only when the object and the heap bits are</code></span>
<span class="codeline" id="line-432"><code>	// initialized (see also the assignment of freeIndexForScan in</code></span>
<span class="codeline" id="line-433"><code>	// mallocgc, and issue 54596).</code></span>
<span class="codeline" id="line-434"><code>	freeIndexForScan uint16</code></span>
<span class="codeline" id="line-435"><code></code></span>
<span class="codeline" id="line-436"><code>	// Cache of the allocBits at freeindex. allocCache is shifted</code></span>
<span class="codeline" id="line-437"><code>	// such that the lowest bit corresponds to the bit freeindex.</code></span>
<span class="codeline" id="line-438"><code>	// allocCache holds the complement of allocBits, thus allowing</code></span>
<span class="codeline" id="line-439"><code>	// ctz (count trailing zero) to use it directly.</code></span>
<span class="codeline" id="line-440"><code>	// allocCache may contain bits beyond s.nelems; the caller must ignore</code></span>
<span class="codeline" id="line-441"><code>	// these.</code></span>
<span class="codeline" id="line-442"><code>	allocCache uint64</code></span>
<span class="codeline" id="line-443"><code></code></span>
<span class="codeline" id="line-444"><code>	// allocBits and gcmarkBits hold pointers to a span's mark and</code></span>
<span class="codeline" id="line-445"><code>	// allocation bits. The pointers are 8 byte aligned.</code></span>
<span class="codeline" id="line-446"><code>	// There are three arenas where this data is held.</code></span>
<span class="codeline" id="line-447"><code>	// free: Dirty arenas that are no longer accessed</code></span>
<span class="codeline" id="line-448"><code>	//       and can be reused.</code></span>
<span class="codeline" id="line-449"><code>	// next: Holds information to be used in the next GC cycle.</code></span>
<span class="codeline" id="line-450"><code>	// current: Information being used during this GC cycle.</code></span>
<span class="codeline" id="line-451"><code>	// previous: Information being used during the last GC cycle.</code></span>
<span class="codeline" id="line-452"><code>	// A new GC cycle starts with the call to finishsweep_m.</code></span>
<span class="codeline" id="line-453"><code>	// finishsweep_m moves the previous arena to the free arena,</code></span>
<span class="codeline" id="line-454"><code>	// the current arena to the previous arena, and</code></span>
<span class="codeline" id="line-455"><code>	// the next arena to the current arena.</code></span>
<span class="codeline" id="line-456"><code>	// The next arena is populated as the spans request</code></span>
<span class="codeline" id="line-457"><code>	// memory to hold gcmarkBits for the next GC cycle as well</code></span>
<span class="codeline" id="line-458"><code>	// as allocBits for newly allocated spans.</code></span>
<span class="codeline" id="line-459"><code>	//</code></span>
<span class="codeline" id="line-460"><code>	// The pointer arithmetic is done "by hand" instead of using</code></span>
<span class="codeline" id="line-461"><code>	// arrays to avoid bounds checks along critical performance</code></span>
<span class="codeline" id="line-462"><code>	// paths.</code></span>
<span class="codeline" id="line-463"><code>	// The sweep will free the old allocBits and set allocBits to the</code></span>
<span class="codeline" id="line-464"><code>	// gcmarkBits. The gcmarkBits are replaced with a fresh zeroed</code></span>
<span class="codeline" id="line-465"><code>	// out memory.</code></span>
<span class="codeline" id="line-466"><code>	allocBits  *gcBits</code></span>
<span class="codeline" id="line-467"><code>	gcmarkBits *gcBits</code></span>
<span class="codeline" id="line-468"><code>	pinnerBits *gcBits // bitmap for pinned objects; accessed atomically</code></span>
<span class="codeline" id="line-469"><code></code></span>
<span class="codeline" id="line-470"><code>	// sweep generation:</code></span>
<span class="codeline" id="line-471"><code>	// if sweepgen == h-&gt;sweepgen - 2, the span needs sweeping</code></span>
<span class="codeline" id="line-472"><code>	// if sweepgen == h-&gt;sweepgen - 1, the span is currently being swept</code></span>
<span class="codeline" id="line-473"><code>	// if sweepgen == h-&gt;sweepgen, the span is swept and ready to use</code></span>
<span class="codeline" id="line-474"><code>	// if sweepgen == h-&gt;sweepgen + 1, the span was cached before sweep began and is still cached, and needs sweeping</code></span>
<span class="codeline" id="line-475"><code>	// if sweepgen == h-&gt;sweepgen + 3, the span was swept and then cached and is still cached</code></span>
<span class="codeline" id="line-476"><code>	// h-&gt;sweepgen is incremented by 2 after every GC</code></span>
<span class="codeline" id="line-477"><code></code></span>
<span class="codeline" id="line-478"><code>	sweepgen              uint32</code></span>
<span class="codeline" id="line-479"><code>	divMul                uint32        // for divide by elemsize</code></span>
<span class="codeline" id="line-480"><code>	allocCount            uint16        // number of allocated objects</code></span>
<span class="codeline" id="line-481"><code>	spanclass             spanClass     // size class and noscan (uint8)</code></span>
<span class="codeline" id="line-482"><code>	state                 mSpanStateBox // mSpanInUse etc; accessed atomically (get/set methods)</code></span>
<span class="codeline" id="line-483"><code>	needzero              uint8         // needs to be zeroed before allocation</code></span>
<span class="codeline" id="line-484"><code>	isUserArenaChunk      bool          // whether or not this span represents a user arena</code></span>
<span class="codeline" id="line-485"><code>	allocCountBeforeCache uint16        // a copy of allocCount that is stored just before this span is cached</code></span>
<span class="codeline" id="line-486"><code>	elemsize              uintptr       // computed from sizeclass or from npages</code></span>
<span class="codeline" id="line-487"><code>	limit                 uintptr       // end of data in span</code></span>
<span class="codeline" id="line-488"><code>	speciallock           mutex         // guards specials list and changes to pinnerBits</code></span>
<span class="codeline" id="line-489"><code>	specials              *special      // linked list of special records sorted by offset.</code></span>
<span class="codeline" id="line-490"><code>	userArenaChunkFree    addrRange     // interval for managing chunk allocation</code></span>
<span class="codeline" id="line-491"><code>	largeType             *_type        // malloc header for large objects.</code></span>
<span class="codeline" id="line-492"><code>}</code></span>
<span class="codeline" id="line-493"><code></code></span>
<span class="codeline" id="line-494"><code>func (s *mspan) base() uintptr {</code></span>
<span class="codeline" id="line-495"><code>	return s.startAddr</code></span>
<span class="codeline" id="line-496"><code>}</code></span>
<span class="codeline" id="line-497"><code></code></span>
<span class="codeline" id="line-498"><code>func (s *mspan) layout() (size, n, total uintptr) {</code></span>
<span class="codeline" id="line-499"><code>	total = s.npages &lt;&lt; _PageShift</code></span>
<span class="codeline" id="line-500"><code>	size = s.elemsize</code></span>
<span class="codeline" id="line-501"><code>	if size &gt; 0 {</code></span>
<span class="codeline" id="line-502"><code>		n = total / size</code></span>
<span class="codeline" id="line-503"><code>	}</code></span>
<span class="codeline" id="line-504"><code>	return</code></span>
<span class="codeline" id="line-505"><code>}</code></span>
<span class="codeline" id="line-506"><code></code></span>
<span class="codeline" id="line-507"><code>// recordspan adds a newly allocated span to h.allspans.</code></span>
<span class="codeline" id="line-508"><code>//</code></span>
<span class="codeline" id="line-509"><code>// This only happens the first time a span is allocated from</code></span>
<span class="codeline" id="line-510"><code>// mheap.spanalloc (it is not called when a span is reused).</code></span>
<span class="codeline" id="line-511"><code>//</code></span>
<span class="codeline" id="line-512"><code>// Write barriers are disallowed here because it can be called from</code></span>
<span class="codeline" id="line-513"><code>// gcWork when allocating new workbufs. However, because it's an</code></span>
<span class="codeline" id="line-514"><code>// indirect call from the fixalloc initializer, the compiler can't see</code></span>
<span class="codeline" id="line-515"><code>// this.</code></span>
<span class="codeline" id="line-516"><code>//</code></span>
<span class="codeline" id="line-517"><code>// The heap lock must be held.</code></span>
<span class="codeline" id="line-518"><code>//</code></span>
<span class="codeline" id="line-519"><code>//go:nowritebarrierrec</code></span>
<span class="codeline" id="line-520"><code>func recordspan(vh unsafe.Pointer, p unsafe.Pointer) {</code></span>
<span class="codeline" id="line-521"><code>	h := (*mheap)(vh)</code></span>
<span class="codeline" id="line-522"><code>	s := (*mspan)(p)</code></span>
<span class="codeline" id="line-523"><code></code></span>
<span class="codeline" id="line-524"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-525"><code></code></span>
<span class="codeline" id="line-526"><code>	if len(h.allspans) &gt;= cap(h.allspans) {</code></span>
<span class="codeline" id="line-527"><code>		n := 64 * 1024 / goarch.PtrSize</code></span>
<span class="codeline" id="line-528"><code>		if n &lt; cap(h.allspans)*3/2 {</code></span>
<span class="codeline" id="line-529"><code>			n = cap(h.allspans) * 3 / 2</code></span>
<span class="codeline" id="line-530"><code>		}</code></span>
<span class="codeline" id="line-531"><code>		var new []*mspan</code></span>
<span class="codeline" id="line-532"><code>		sp := (*slice)(unsafe.Pointer(&amp;new))</code></span>
<span class="codeline" id="line-533"><code>		sp.array = sysAlloc(uintptr(n)*goarch.PtrSize, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-534"><code>		if sp.array == nil {</code></span>
<span class="codeline" id="line-535"><code>			throw("runtime: cannot allocate memory")</code></span>
<span class="codeline" id="line-536"><code>		}</code></span>
<span class="codeline" id="line-537"><code>		sp.len = len(h.allspans)</code></span>
<span class="codeline" id="line-538"><code>		sp.cap = n</code></span>
<span class="codeline" id="line-539"><code>		if len(h.allspans) &gt; 0 {</code></span>
<span class="codeline" id="line-540"><code>			copy(new, h.allspans)</code></span>
<span class="codeline" id="line-541"><code>		}</code></span>
<span class="codeline" id="line-542"><code>		oldAllspans := h.allspans</code></span>
<span class="codeline" id="line-543"><code>		*(*notInHeapSlice)(unsafe.Pointer(&amp;h.allspans)) = *(*notInHeapSlice)(unsafe.Pointer(&amp;new))</code></span>
<span class="codeline" id="line-544"><code>		if len(oldAllspans) != 0 {</code></span>
<span class="codeline" id="line-545"><code>			sysFree(unsafe.Pointer(&amp;oldAllspans[0]), uintptr(cap(oldAllspans))*unsafe.Sizeof(oldAllspans[0]), &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-546"><code>		}</code></span>
<span class="codeline" id="line-547"><code>	}</code></span>
<span class="codeline" id="line-548"><code>	h.allspans = h.allspans[:len(h.allspans)+1]</code></span>
<span class="codeline" id="line-549"><code>	h.allspans[len(h.allspans)-1] = s</code></span>
<span class="codeline" id="line-550"><code>}</code></span>
<span class="codeline" id="line-551"><code></code></span>
<span class="codeline" id="line-552"><code>// A spanClass represents the size class and noscan-ness of a span.</code></span>
<span class="codeline" id="line-553"><code>//</code></span>
<span class="codeline" id="line-554"><code>// Each size class has a noscan spanClass and a scan spanClass. The</code></span>
<span class="codeline" id="line-555"><code>// noscan spanClass contains only noscan objects, which do not contain</code></span>
<span class="codeline" id="line-556"><code>// pointers and thus do not need to be scanned by the garbage</code></span>
<span class="codeline" id="line-557"><code>// collector.</code></span>
<span class="codeline" id="line-558"><code>type spanClass uint8</code></span>
<span class="codeline" id="line-559"><code></code></span>
<span class="codeline" id="line-560"><code>const (</code></span>
<span class="codeline" id="line-561"><code>	numSpanClasses = _NumSizeClasses &lt;&lt; 1</code></span>
<span class="codeline" id="line-562"><code>	tinySpanClass  = spanClass(tinySizeClass&lt;&lt;1 | 1)</code></span>
<span class="codeline" id="line-563"><code>)</code></span>
<span class="codeline" id="line-564"><code></code></span>
<span class="codeline" id="line-565"><code>func makeSpanClass(sizeclass uint8, noscan bool) spanClass {</code></span>
<span class="codeline" id="line-566"><code>	return spanClass(sizeclass&lt;&lt;1) | spanClass(bool2int(noscan))</code></span>
<span class="codeline" id="line-567"><code>}</code></span>
<span class="codeline" id="line-568"><code></code></span>
<span class="codeline" id="line-569"><code>//go:nosplit</code></span>
<span class="codeline" id="line-570"><code>func (sc spanClass) sizeclass() int8 {</code></span>
<span class="codeline" id="line-571"><code>	return int8(sc &gt;&gt; 1)</code></span>
<span class="codeline" id="line-572"><code>}</code></span>
<span class="codeline" id="line-573"><code></code></span>
<span class="codeline" id="line-574"><code>//go:nosplit</code></span>
<span class="codeline" id="line-575"><code>func (sc spanClass) noscan() bool {</code></span>
<span class="codeline" id="line-576"><code>	return sc&amp;1 != 0</code></span>
<span class="codeline" id="line-577"><code>}</code></span>
<span class="codeline" id="line-578"><code></code></span>
<span class="codeline" id="line-579"><code>// arenaIndex returns the index into mheap_.arenas of the arena</code></span>
<span class="codeline" id="line-580"><code>// containing metadata for p. This index combines of an index into the</code></span>
<span class="codeline" id="line-581"><code>// L1 map and an index into the L2 map and should be used as</code></span>
<span class="codeline" id="line-582"><code>// mheap_.arenas[ai.l1()][ai.l2()].</code></span>
<span class="codeline" id="line-583"><code>//</code></span>
<span class="codeline" id="line-584"><code>// If p is outside the range of valid heap addresses, either l1() or</code></span>
<span class="codeline" id="line-585"><code>// l2() will be out of bounds.</code></span>
<span class="codeline" id="line-586"><code>//</code></span>
<span class="codeline" id="line-587"><code>// It is nosplit because it's called by spanOf and several other</code></span>
<span class="codeline" id="line-588"><code>// nosplit functions.</code></span>
<span class="codeline" id="line-589"><code>//</code></span>
<span class="codeline" id="line-590"><code>//go:nosplit</code></span>
<span class="codeline" id="line-591"><code>func arenaIndex(p uintptr) arenaIdx {</code></span>
<span class="codeline" id="line-592"><code>	return arenaIdx((p - arenaBaseOffset) / heapArenaBytes)</code></span>
<span class="codeline" id="line-593"><code>}</code></span>
<span class="codeline" id="line-594"><code></code></span>
<span class="codeline" id="line-595"><code>// arenaBase returns the low address of the region covered by heap</code></span>
<span class="codeline" id="line-596"><code>// arena i.</code></span>
<span class="codeline" id="line-597"><code>func arenaBase(i arenaIdx) uintptr {</code></span>
<span class="codeline" id="line-598"><code>	return uintptr(i)*heapArenaBytes + arenaBaseOffset</code></span>
<span class="codeline" id="line-599"><code>}</code></span>
<span class="codeline" id="line-600"><code></code></span>
<span class="codeline" id="line-601"><code>type arenaIdx uint</code></span>
<span class="codeline" id="line-602"><code></code></span>
<span class="codeline" id="line-603"><code>// l1 returns the "l1" portion of an arenaIdx.</code></span>
<span class="codeline" id="line-604"><code>//</code></span>
<span class="codeline" id="line-605"><code>// Marked nosplit because it's called by spanOf and other nosplit</code></span>
<span class="codeline" id="line-606"><code>// functions.</code></span>
<span class="codeline" id="line-607"><code>//</code></span>
<span class="codeline" id="line-608"><code>//go:nosplit</code></span>
<span class="codeline" id="line-609"><code>func (i arenaIdx) l1() uint {</code></span>
<span class="codeline" id="line-610"><code>	if arenaL1Bits == 0 {</code></span>
<span class="codeline" id="line-611"><code>		// Let the compiler optimize this away if there's no</code></span>
<span class="codeline" id="line-612"><code>		// L1 map.</code></span>
<span class="codeline" id="line-613"><code>		return 0</code></span>
<span class="codeline" id="line-614"><code>	} else {</code></span>
<span class="codeline" id="line-615"><code>		return uint(i) &gt;&gt; arenaL1Shift</code></span>
<span class="codeline" id="line-616"><code>	}</code></span>
<span class="codeline" id="line-617"><code>}</code></span>
<span class="codeline" id="line-618"><code></code></span>
<span class="codeline" id="line-619"><code>// l2 returns the "l2" portion of an arenaIdx.</code></span>
<span class="codeline" id="line-620"><code>//</code></span>
<span class="codeline" id="line-621"><code>// Marked nosplit because it's called by spanOf and other nosplit funcs.</code></span>
<span class="codeline" id="line-622"><code>// functions.</code></span>
<span class="codeline" id="line-623"><code>//</code></span>
<span class="codeline" id="line-624"><code>//go:nosplit</code></span>
<span class="codeline" id="line-625"><code>func (i arenaIdx) l2() uint {</code></span>
<span class="codeline" id="line-626"><code>	if arenaL1Bits == 0 {</code></span>
<span class="codeline" id="line-627"><code>		return uint(i)</code></span>
<span class="codeline" id="line-628"><code>	} else {</code></span>
<span class="codeline" id="line-629"><code>		return uint(i) &amp; (1&lt;&lt;arenaL2Bits - 1)</code></span>
<span class="codeline" id="line-630"><code>	}</code></span>
<span class="codeline" id="line-631"><code>}</code></span>
<span class="codeline" id="line-632"><code></code></span>
<span class="codeline" id="line-633"><code>// inheap reports whether b is a pointer into a (potentially dead) heap object.</code></span>
<span class="codeline" id="line-634"><code>// It returns false for pointers into mSpanManual spans.</code></span>
<span class="codeline" id="line-635"><code>// Non-preemptible because it is used by write barriers.</code></span>
<span class="codeline" id="line-636"><code>//</code></span>
<span class="codeline" id="line-637"><code>//go:nowritebarrier</code></span>
<span class="codeline" id="line-638"><code>//go:nosplit</code></span>
<span class="codeline" id="line-639"><code>func inheap(b uintptr) bool {</code></span>
<span class="codeline" id="line-640"><code>	return spanOfHeap(b) != nil</code></span>
<span class="codeline" id="line-641"><code>}</code></span>
<span class="codeline" id="line-642"><code></code></span>
<span class="codeline" id="line-643"><code>// inHeapOrStack is a variant of inheap that returns true for pointers</code></span>
<span class="codeline" id="line-644"><code>// into any allocated heap span.</code></span>
<span class="codeline" id="line-645"><code>//</code></span>
<span class="codeline" id="line-646"><code>//go:nowritebarrier</code></span>
<span class="codeline" id="line-647"><code>//go:nosplit</code></span>
<span class="codeline" id="line-648"><code>func inHeapOrStack(b uintptr) bool {</code></span>
<span class="codeline" id="line-649"><code>	s := spanOf(b)</code></span>
<span class="codeline" id="line-650"><code>	if s == nil || b &lt; s.base() {</code></span>
<span class="codeline" id="line-651"><code>		return false</code></span>
<span class="codeline" id="line-652"><code>	}</code></span>
<span class="codeline" id="line-653"><code>	switch s.state.get() {</code></span>
<span class="codeline" id="line-654"><code>	case mSpanInUse, mSpanManual:</code></span>
<span class="codeline" id="line-655"><code>		return b &lt; s.limit</code></span>
<span class="codeline" id="line-656"><code>	default:</code></span>
<span class="codeline" id="line-657"><code>		return false</code></span>
<span class="codeline" id="line-658"><code>	}</code></span>
<span class="codeline" id="line-659"><code>}</code></span>
<span class="codeline" id="line-660"><code></code></span>
<span class="codeline" id="line-661"><code>// spanOf returns the span of p. If p does not point into the heap</code></span>
<span class="codeline" id="line-662"><code>// arena or no span has ever contained p, spanOf returns nil.</code></span>
<span class="codeline" id="line-663"><code>//</code></span>
<span class="codeline" id="line-664"><code>// If p does not point to allocated memory, this may return a non-nil</code></span>
<span class="codeline" id="line-665"><code>// span that does *not* contain p. If this is a possibility, the</code></span>
<span class="codeline" id="line-666"><code>// caller should either call spanOfHeap or check the span bounds</code></span>
<span class="codeline" id="line-667"><code>// explicitly.</code></span>
<span class="codeline" id="line-668"><code>//</code></span>
<span class="codeline" id="line-669"><code>// Must be nosplit because it has callers that are nosplit.</code></span>
<span class="codeline" id="line-670"><code>//</code></span>
<span class="codeline" id="line-671"><code>//go:nosplit</code></span>
<span class="codeline" id="line-672"><code>func spanOf(p uintptr) *mspan {</code></span>
<span class="codeline" id="line-673"><code>	// This function looks big, but we use a lot of constant</code></span>
<span class="codeline" id="line-674"><code>	// folding around arenaL1Bits to get it under the inlining</code></span>
<span class="codeline" id="line-675"><code>	// budget. Also, many of the checks here are safety checks</code></span>
<span class="codeline" id="line-676"><code>	// that Go needs to do anyway, so the generated code is quite</code></span>
<span class="codeline" id="line-677"><code>	// short.</code></span>
<span class="codeline" id="line-678"><code>	ri := arenaIndex(p)</code></span>
<span class="codeline" id="line-679"><code>	if arenaL1Bits == 0 {</code></span>
<span class="codeline" id="line-680"><code>		// If there's no L1, then ri.l1() can't be out of bounds but ri.l2() can.</code></span>
<span class="codeline" id="line-681"><code>		if ri.l2() &gt;= uint(len(mheap_.arenas[0])) {</code></span>
<span class="codeline" id="line-682"><code>			return nil</code></span>
<span class="codeline" id="line-683"><code>		}</code></span>
<span class="codeline" id="line-684"><code>	} else {</code></span>
<span class="codeline" id="line-685"><code>		// If there's an L1, then ri.l1() can be out of bounds but ri.l2() can't.</code></span>
<span class="codeline" id="line-686"><code>		if ri.l1() &gt;= uint(len(mheap_.arenas)) {</code></span>
<span class="codeline" id="line-687"><code>			return nil</code></span>
<span class="codeline" id="line-688"><code>		}</code></span>
<span class="codeline" id="line-689"><code>	}</code></span>
<span class="codeline" id="line-690"><code>	l2 := mheap_.arenas[ri.l1()]</code></span>
<span class="codeline" id="line-691"><code>	if arenaL1Bits != 0 &amp;&amp; l2 == nil { // Should never happen if there's no L1.</code></span>
<span class="codeline" id="line-692"><code>		return nil</code></span>
<span class="codeline" id="line-693"><code>	}</code></span>
<span class="codeline" id="line-694"><code>	ha := l2[ri.l2()]</code></span>
<span class="codeline" id="line-695"><code>	if ha == nil {</code></span>
<span class="codeline" id="line-696"><code>		return nil</code></span>
<span class="codeline" id="line-697"><code>	}</code></span>
<span class="codeline" id="line-698"><code>	return ha.spans[(p/pageSize)%pagesPerArena]</code></span>
<span class="codeline" id="line-699"><code>}</code></span>
<span class="codeline" id="line-700"><code></code></span>
<span class="codeline" id="line-701"><code>// spanOfUnchecked is equivalent to spanOf, but the caller must ensure</code></span>
<span class="codeline" id="line-702"><code>// that p points into an allocated heap arena.</code></span>
<span class="codeline" id="line-703"><code>//</code></span>
<span class="codeline" id="line-704"><code>// Must be nosplit because it has callers that are nosplit.</code></span>
<span class="codeline" id="line-705"><code>//</code></span>
<span class="codeline" id="line-706"><code>//go:nosplit</code></span>
<span class="codeline" id="line-707"><code>func spanOfUnchecked(p uintptr) *mspan {</code></span>
<span class="codeline" id="line-708"><code>	ai := arenaIndex(p)</code></span>
<span class="codeline" id="line-709"><code>	return mheap_.arenas[ai.l1()][ai.l2()].spans[(p/pageSize)%pagesPerArena]</code></span>
<span class="codeline" id="line-710"><code>}</code></span>
<span class="codeline" id="line-711"><code></code></span>
<span class="codeline" id="line-712"><code>// spanOfHeap is like spanOf, but returns nil if p does not point to a</code></span>
<span class="codeline" id="line-713"><code>// heap object.</code></span>
<span class="codeline" id="line-714"><code>//</code></span>
<span class="codeline" id="line-715"><code>// Must be nosplit because it has callers that are nosplit.</code></span>
<span class="codeline" id="line-716"><code>//</code></span>
<span class="codeline" id="line-717"><code>//go:nosplit</code></span>
<span class="codeline" id="line-718"><code>func spanOfHeap(p uintptr) *mspan {</code></span>
<span class="codeline" id="line-719"><code>	s := spanOf(p)</code></span>
<span class="codeline" id="line-720"><code>	// s is nil if it's never been allocated. Otherwise, we check</code></span>
<span class="codeline" id="line-721"><code>	// its state first because we don't trust this pointer, so we</code></span>
<span class="codeline" id="line-722"><code>	// have to synchronize with span initialization. Then, it's</code></span>
<span class="codeline" id="line-723"><code>	// still possible we picked up a stale span pointer, so we</code></span>
<span class="codeline" id="line-724"><code>	// have to check the span's bounds.</code></span>
<span class="codeline" id="line-725"><code>	if s == nil || s.state.get() != mSpanInUse || p &lt; s.base() || p &gt;= s.limit {</code></span>
<span class="codeline" id="line-726"><code>		return nil</code></span>
<span class="codeline" id="line-727"><code>	}</code></span>
<span class="codeline" id="line-728"><code>	return s</code></span>
<span class="codeline" id="line-729"><code>}</code></span>
<span class="codeline" id="line-730"><code></code></span>
<span class="codeline" id="line-731"><code>// pageIndexOf returns the arena, page index, and page mask for pointer p.</code></span>
<span class="codeline" id="line-732"><code>// The caller must ensure p is in the heap.</code></span>
<span class="codeline" id="line-733"><code>func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8) {</code></span>
<span class="codeline" id="line-734"><code>	ai := arenaIndex(p)</code></span>
<span class="codeline" id="line-735"><code>	arena = mheap_.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-736"><code>	pageIdx = ((p / pageSize) / 8) % uintptr(len(arena.pageInUse))</code></span>
<span class="codeline" id="line-737"><code>	pageMask = byte(1 &lt;&lt; ((p / pageSize) % 8))</code></span>
<span class="codeline" id="line-738"><code>	return</code></span>
<span class="codeline" id="line-739"><code>}</code></span>
<span class="codeline" id="line-740"><code></code></span>
<span class="codeline" id="line-741"><code>// Initialize the heap.</code></span>
<span class="codeline" id="line-742"><code>func (h *mheap) init() {</code></span>
<span class="codeline" id="line-743"><code>	lockInit(&amp;h.lock, lockRankMheap)</code></span>
<span class="codeline" id="line-744"><code>	lockInit(&amp;h.speciallock, lockRankMheapSpecial)</code></span>
<span class="codeline" id="line-745"><code></code></span>
<span class="codeline" id="line-746"><code>	h.spanalloc.init(unsafe.Sizeof(mspan{}), recordspan, unsafe.Pointer(h), &amp;memstats.mspan_sys)</code></span>
<span class="codeline" id="line-747"><code>	h.cachealloc.init(unsafe.Sizeof(mcache{}), nil, nil, &amp;memstats.mcache_sys)</code></span>
<span class="codeline" id="line-748"><code>	h.specialfinalizeralloc.init(unsafe.Sizeof(specialfinalizer{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-749"><code>	h.specialprofilealloc.init(unsafe.Sizeof(specialprofile{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-750"><code>	h.specialReachableAlloc.init(unsafe.Sizeof(specialReachable{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-751"><code>	h.specialPinCounterAlloc.init(unsafe.Sizeof(specialPinCounter{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-752"><code>	h.arenaHintAlloc.init(unsafe.Sizeof(arenaHint{}), nil, nil, &amp;memstats.other_sys)</code></span>
<span class="codeline" id="line-753"><code></code></span>
<span class="codeline" id="line-754"><code>	// Don't zero mspan allocations. Background sweeping can</code></span>
<span class="codeline" id="line-755"><code>	// inspect a span concurrently with allocating it, so it's</code></span>
<span class="codeline" id="line-756"><code>	// important that the span's sweepgen survive across freeing</code></span>
<span class="codeline" id="line-757"><code>	// and re-allocating a span to prevent background sweeping</code></span>
<span class="codeline" id="line-758"><code>	// from improperly cas'ing it from 0.</code></span>
<span class="codeline" id="line-759"><code>	//</code></span>
<span class="codeline" id="line-760"><code>	// This is safe because mspan contains no heap pointers.</code></span>
<span class="codeline" id="line-761"><code>	h.spanalloc.zero = false</code></span>
<span class="codeline" id="line-762"><code></code></span>
<span class="codeline" id="line-763"><code>	// h-&gt;mapcache needs no init</code></span>
<span class="codeline" id="line-764"><code></code></span>
<span class="codeline" id="line-765"><code>	for i := range h.central {</code></span>
<span class="codeline" id="line-766"><code>		h.central[i].mcentral.init(spanClass(i))</code></span>
<span class="codeline" id="line-767"><code>	}</code></span>
<span class="codeline" id="line-768"><code></code></span>
<span class="codeline" id="line-769"><code>	h.pages.init(&amp;h.lock, &amp;memstats.gcMiscSys, false)</code></span>
<span class="codeline" id="line-770"><code>}</code></span>
<span class="codeline" id="line-771"><code></code></span>
<span class="codeline" id="line-772"><code>// reclaim sweeps and reclaims at least npage pages into the heap.</code></span>
<span class="codeline" id="line-773"><code>// It is called before allocating npage pages to keep growth in check.</code></span>
<span class="codeline" id="line-774"><code>//</code></span>
<span class="codeline" id="line-775"><code>// reclaim implements the page-reclaimer half of the sweeper.</code></span>
<span class="codeline" id="line-776"><code>//</code></span>
<span class="codeline" id="line-777"><code>// h.lock must NOT be held.</code></span>
<span class="codeline" id="line-778"><code>func (h *mheap) reclaim(npage uintptr) {</code></span>
<span class="codeline" id="line-779"><code>	// TODO(austin): Half of the time spent freeing spans is in</code></span>
<span class="codeline" id="line-780"><code>	// locking/unlocking the heap (even with low contention). We</code></span>
<span class="codeline" id="line-781"><code>	// could make the slow path here several times faster by</code></span>
<span class="codeline" id="line-782"><code>	// batching heap frees.</code></span>
<span class="codeline" id="line-783"><code></code></span>
<span class="codeline" id="line-784"><code>	// Bail early if there's no more reclaim work.</code></span>
<span class="codeline" id="line-785"><code>	if h.reclaimIndex.Load() &gt;= 1&lt;&lt;63 {</code></span>
<span class="codeline" id="line-786"><code>		return</code></span>
<span class="codeline" id="line-787"><code>	}</code></span>
<span class="codeline" id="line-788"><code></code></span>
<span class="codeline" id="line-789"><code>	// Disable preemption so the GC can't start while we're</code></span>
<span class="codeline" id="line-790"><code>	// sweeping, so we can read h.sweepArenas, and so</code></span>
<span class="codeline" id="line-791"><code>	// traceGCSweepStart/Done pair on the P.</code></span>
<span class="codeline" id="line-792"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-793"><code></code></span>
<span class="codeline" id="line-794"><code>	trace := traceAcquire()</code></span>
<span class="codeline" id="line-795"><code>	if trace.ok() {</code></span>
<span class="codeline" id="line-796"><code>		trace.GCSweepStart()</code></span>
<span class="codeline" id="line-797"><code>		traceRelease(trace)</code></span>
<span class="codeline" id="line-798"><code>	}</code></span>
<span class="codeline" id="line-799"><code></code></span>
<span class="codeline" id="line-800"><code>	arenas := h.sweepArenas</code></span>
<span class="codeline" id="line-801"><code>	locked := false</code></span>
<span class="codeline" id="line-802"><code>	for npage &gt; 0 {</code></span>
<span class="codeline" id="line-803"><code>		// Pull from accumulated credit first.</code></span>
<span class="codeline" id="line-804"><code>		if credit := h.reclaimCredit.Load(); credit &gt; 0 {</code></span>
<span class="codeline" id="line-805"><code>			take := credit</code></span>
<span class="codeline" id="line-806"><code>			if take &gt; npage {</code></span>
<span class="codeline" id="line-807"><code>				// Take only what we need.</code></span>
<span class="codeline" id="line-808"><code>				take = npage</code></span>
<span class="codeline" id="line-809"><code>			}</code></span>
<span class="codeline" id="line-810"><code>			if h.reclaimCredit.CompareAndSwap(credit, credit-take) {</code></span>
<span class="codeline" id="line-811"><code>				npage -= take</code></span>
<span class="codeline" id="line-812"><code>			}</code></span>
<span class="codeline" id="line-813"><code>			continue</code></span>
<span class="codeline" id="line-814"><code>		}</code></span>
<span class="codeline" id="line-815"><code></code></span>
<span class="codeline" id="line-816"><code>		// Claim a chunk of work.</code></span>
<span class="codeline" id="line-817"><code>		idx := uintptr(h.reclaimIndex.Add(pagesPerReclaimerChunk) - pagesPerReclaimerChunk)</code></span>
<span class="codeline" id="line-818"><code>		if idx/pagesPerArena &gt;= uintptr(len(arenas)) {</code></span>
<span class="codeline" id="line-819"><code>			// Page reclaiming is done.</code></span>
<span class="codeline" id="line-820"><code>			h.reclaimIndex.Store(1 &lt;&lt; 63)</code></span>
<span class="codeline" id="line-821"><code>			break</code></span>
<span class="codeline" id="line-822"><code>		}</code></span>
<span class="codeline" id="line-823"><code></code></span>
<span class="codeline" id="line-824"><code>		if !locked {</code></span>
<span class="codeline" id="line-825"><code>			// Lock the heap for reclaimChunk.</code></span>
<span class="codeline" id="line-826"><code>			lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-827"><code>			locked = true</code></span>
<span class="codeline" id="line-828"><code>		}</code></span>
<span class="codeline" id="line-829"><code></code></span>
<span class="codeline" id="line-830"><code>		// Scan this chunk.</code></span>
<span class="codeline" id="line-831"><code>		nfound := h.reclaimChunk(arenas, idx, pagesPerReclaimerChunk)</code></span>
<span class="codeline" id="line-832"><code>		if nfound &lt;= npage {</code></span>
<span class="codeline" id="line-833"><code>			npage -= nfound</code></span>
<span class="codeline" id="line-834"><code>		} else {</code></span>
<span class="codeline" id="line-835"><code>			// Put spare pages toward global credit.</code></span>
<span class="codeline" id="line-836"><code>			h.reclaimCredit.Add(nfound - npage)</code></span>
<span class="codeline" id="line-837"><code>			npage = 0</code></span>
<span class="codeline" id="line-838"><code>		}</code></span>
<span class="codeline" id="line-839"><code>	}</code></span>
<span class="codeline" id="line-840"><code>	if locked {</code></span>
<span class="codeline" id="line-841"><code>		unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-842"><code>	}</code></span>
<span class="codeline" id="line-843"><code></code></span>
<span class="codeline" id="line-844"><code>	trace = traceAcquire()</code></span>
<span class="codeline" id="line-845"><code>	if trace.ok() {</code></span>
<span class="codeline" id="line-846"><code>		trace.GCSweepDone()</code></span>
<span class="codeline" id="line-847"><code>		traceRelease(trace)</code></span>
<span class="codeline" id="line-848"><code>	}</code></span>
<span class="codeline" id="line-849"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-850"><code>}</code></span>
<span class="codeline" id="line-851"><code></code></span>
<span class="codeline" id="line-852"><code>// reclaimChunk sweeps unmarked spans that start at page indexes [pageIdx, pageIdx+n).</code></span>
<span class="codeline" id="line-853"><code>// It returns the number of pages returned to the heap.</code></span>
<span class="codeline" id="line-854"><code>//</code></span>
<span class="codeline" id="line-855"><code>// h.lock must be held and the caller must be non-preemptible. Note: h.lock may be</code></span>
<span class="codeline" id="line-856"><code>// temporarily unlocked and re-locked in order to do sweeping or if tracing is</code></span>
<span class="codeline" id="line-857"><code>// enabled.</code></span>
<span class="codeline" id="line-858"><code>func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr {</code></span>
<span class="codeline" id="line-859"><code>	// The heap lock must be held because this accesses the</code></span>
<span class="codeline" id="line-860"><code>	// heapArena.spans arrays using potentially non-live pointers.</code></span>
<span class="codeline" id="line-861"><code>	// In particular, if a span were freed and merged concurrently</code></span>
<span class="codeline" id="line-862"><code>	// with this probing heapArena.spans, it would be possible to</code></span>
<span class="codeline" id="line-863"><code>	// observe arbitrary, stale span pointers.</code></span>
<span class="codeline" id="line-864"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-865"><code></code></span>
<span class="codeline" id="line-866"><code>	n0 := n</code></span>
<span class="codeline" id="line-867"><code>	var nFreed uintptr</code></span>
<span class="codeline" id="line-868"><code>	sl := sweep.active.begin()</code></span>
<span class="codeline" id="line-869"><code>	if !sl.valid {</code></span>
<span class="codeline" id="line-870"><code>		return 0</code></span>
<span class="codeline" id="line-871"><code>	}</code></span>
<span class="codeline" id="line-872"><code>	for n &gt; 0 {</code></span>
<span class="codeline" id="line-873"><code>		ai := arenas[pageIdx/pagesPerArena]</code></span>
<span class="codeline" id="line-874"><code>		ha := h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-875"><code></code></span>
<span class="codeline" id="line-876"><code>		// Get a chunk of the bitmap to work on.</code></span>
<span class="codeline" id="line-877"><code>		arenaPage := uint(pageIdx % pagesPerArena)</code></span>
<span class="codeline" id="line-878"><code>		inUse := ha.pageInUse[arenaPage/8:]</code></span>
<span class="codeline" id="line-879"><code>		marked := ha.pageMarks[arenaPage/8:]</code></span>
<span class="codeline" id="line-880"><code>		if uintptr(len(inUse)) &gt; n/8 {</code></span>
<span class="codeline" id="line-881"><code>			inUse = inUse[:n/8]</code></span>
<span class="codeline" id="line-882"><code>			marked = marked[:n/8]</code></span>
<span class="codeline" id="line-883"><code>		}</code></span>
<span class="codeline" id="line-884"><code></code></span>
<span class="codeline" id="line-885"><code>		// Scan this bitmap chunk for spans that are in-use</code></span>
<span class="codeline" id="line-886"><code>		// but have no marked objects on them.</code></span>
<span class="codeline" id="line-887"><code>		for i := range inUse {</code></span>
<span class="codeline" id="line-888"><code>			inUseUnmarked := atomic.Load8(&amp;inUse[i]) &amp;^ marked[i]</code></span>
<span class="codeline" id="line-889"><code>			if inUseUnmarked == 0 {</code></span>
<span class="codeline" id="line-890"><code>				continue</code></span>
<span class="codeline" id="line-891"><code>			}</code></span>
<span class="codeline" id="line-892"><code></code></span>
<span class="codeline" id="line-893"><code>			for j := uint(0); j &lt; 8; j++ {</code></span>
<span class="codeline" id="line-894"><code>				if inUseUnmarked&amp;(1&lt;&lt;j) != 0 {</code></span>
<span class="codeline" id="line-895"><code>					s := ha.spans[arenaPage+uint(i)*8+j]</code></span>
<span class="codeline" id="line-896"><code>					if s, ok := sl.tryAcquire(s); ok {</code></span>
<span class="codeline" id="line-897"><code>						npages := s.npages</code></span>
<span class="codeline" id="line-898"><code>						unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-899"><code>						if s.sweep(false) {</code></span>
<span class="codeline" id="line-900"><code>							nFreed += npages</code></span>
<span class="codeline" id="line-901"><code>						}</code></span>
<span class="codeline" id="line-902"><code>						lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-903"><code>						// Reload inUse. It's possible nearby</code></span>
<span class="codeline" id="line-904"><code>						// spans were freed when we dropped the</code></span>
<span class="codeline" id="line-905"><code>						// lock and we don't want to get stale</code></span>
<span class="codeline" id="line-906"><code>						// pointers from the spans array.</code></span>
<span class="codeline" id="line-907"><code>						inUseUnmarked = atomic.Load8(&amp;inUse[i]) &amp;^ marked[i]</code></span>
<span class="codeline" id="line-908"><code>					}</code></span>
<span class="codeline" id="line-909"><code>				}</code></span>
<span class="codeline" id="line-910"><code>			}</code></span>
<span class="codeline" id="line-911"><code>		}</code></span>
<span class="codeline" id="line-912"><code></code></span>
<span class="codeline" id="line-913"><code>		// Advance.</code></span>
<span class="codeline" id="line-914"><code>		pageIdx += uintptr(len(inUse) * 8)</code></span>
<span class="codeline" id="line-915"><code>		n -= uintptr(len(inUse) * 8)</code></span>
<span class="codeline" id="line-916"><code>	}</code></span>
<span class="codeline" id="line-917"><code>	sweep.active.end(sl)</code></span>
<span class="codeline" id="line-918"><code>	trace := traceAcquire()</code></span>
<span class="codeline" id="line-919"><code>	if trace.ok() {</code></span>
<span class="codeline" id="line-920"><code>		unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-921"><code>		// Account for pages scanned but not reclaimed.</code></span>
<span class="codeline" id="line-922"><code>		trace.GCSweepSpan((n0 - nFreed) * pageSize)</code></span>
<span class="codeline" id="line-923"><code>		traceRelease(trace)</code></span>
<span class="codeline" id="line-924"><code>		lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-925"><code>	}</code></span>
<span class="codeline" id="line-926"><code></code></span>
<span class="codeline" id="line-927"><code>	assertLockHeld(&amp;h.lock) // Must be locked on return.</code></span>
<span class="codeline" id="line-928"><code>	return nFreed</code></span>
<span class="codeline" id="line-929"><code>}</code></span>
<span class="codeline" id="line-930"><code></code></span>
<span class="codeline" id="line-931"><code>// spanAllocType represents the type of allocation to make, or</code></span>
<span class="codeline" id="line-932"><code>// the type of allocation to be freed.</code></span>
<span class="codeline" id="line-933"><code>type spanAllocType uint8</code></span>
<span class="codeline" id="line-934"><code></code></span>
<span class="codeline" id="line-935"><code>const (</code></span>
<span class="codeline" id="line-936"><code>	spanAllocHeap          spanAllocType = iota // heap span</code></span>
<span class="codeline" id="line-937"><code>	spanAllocStack                              // stack span</code></span>
<span class="codeline" id="line-938"><code>	spanAllocPtrScalarBits                      // unrolled GC prog bitmap span</code></span>
<span class="codeline" id="line-939"><code>	spanAllocWorkBuf                            // work buf span</code></span>
<span class="codeline" id="line-940"><code>)</code></span>
<span class="codeline" id="line-941"><code></code></span>
<span class="codeline" id="line-942"><code>// manual returns true if the span allocation is manually managed.</code></span>
<span class="codeline" id="line-943"><code>func (s spanAllocType) manual() bool {</code></span>
<span class="codeline" id="line-944"><code>	return s != spanAllocHeap</code></span>
<span class="codeline" id="line-945"><code>}</code></span>
<span class="codeline" id="line-946"><code></code></span>
<span class="codeline" id="line-947"><code>// alloc allocates a new span of npage pages from the GC'd heap.</code></span>
<span class="codeline" id="line-948"><code>//</code></span>
<span class="codeline" id="line-949"><code>// spanclass indicates the span's size class and scannability.</code></span>
<span class="codeline" id="line-950"><code>//</code></span>
<span class="codeline" id="line-951"><code>// Returns a span that has been fully initialized. span.needzero indicates</code></span>
<span class="codeline" id="line-952"><code>// whether the span has been zeroed. Note that it may not be.</code></span>
<span class="codeline" id="line-953"><code>func (h *mheap) alloc(npages uintptr, spanclass spanClass) *mspan {</code></span>
<span class="codeline" id="line-954"><code>	// Don't do any operations that lock the heap on the G stack.</code></span>
<span class="codeline" id="line-955"><code>	// It might trigger stack growth, and the stack growth code needs</code></span>
<span class="codeline" id="line-956"><code>	// to be able to allocate heap.</code></span>
<span class="codeline" id="line-957"><code>	var s *mspan</code></span>
<span class="codeline" id="line-958"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-959"><code>		// To prevent excessive heap growth, before allocating n pages</code></span>
<span class="codeline" id="line-960"><code>		// we need to sweep and reclaim at least n pages.</code></span>
<span class="codeline" id="line-961"><code>		if !isSweepDone() {</code></span>
<span class="codeline" id="line-962"><code>			h.reclaim(npages)</code></span>
<span class="codeline" id="line-963"><code>		}</code></span>
<span class="codeline" id="line-964"><code>		s = h.allocSpan(npages, spanAllocHeap, spanclass)</code></span>
<span class="codeline" id="line-965"><code>	})</code></span>
<span class="codeline" id="line-966"><code>	return s</code></span>
<span class="codeline" id="line-967"><code>}</code></span>
<span class="codeline" id="line-968"><code></code></span>
<span class="codeline" id="line-969"><code>// allocManual allocates a manually-managed span of npage pages.</code></span>
<span class="codeline" id="line-970"><code>// allocManual returns nil if allocation fails.</code></span>
<span class="codeline" id="line-971"><code>//</code></span>
<span class="codeline" id="line-972"><code>// allocManual adds the bytes used to *stat, which should be a</code></span>
<span class="codeline" id="line-973"><code>// memstats in-use field. Unlike allocations in the GC'd heap, the</code></span>
<span class="codeline" id="line-974"><code>// allocation does *not* count toward heapInUse.</code></span>
<span class="codeline" id="line-975"><code>//</code></span>
<span class="codeline" id="line-976"><code>// The memory backing the returned span may not be zeroed if</code></span>
<span class="codeline" id="line-977"><code>// span.needzero is set.</code></span>
<span class="codeline" id="line-978"><code>//</code></span>
<span class="codeline" id="line-979"><code>// allocManual must be called on the system stack because it may</code></span>
<span class="codeline" id="line-980"><code>// acquire the heap lock via allocSpan. See mheap for details.</code></span>
<span class="codeline" id="line-981"><code>//</code></span>
<span class="codeline" id="line-982"><code>// If new code is written to call allocManual, do NOT use an</code></span>
<span class="codeline" id="line-983"><code>// existing spanAllocType value and instead declare a new one.</code></span>
<span class="codeline" id="line-984"><code>//</code></span>
<span class="codeline" id="line-985"><code>//go:systemstack</code></span>
<span class="codeline" id="line-986"><code>func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan {</code></span>
<span class="codeline" id="line-987"><code>	if !typ.manual() {</code></span>
<span class="codeline" id="line-988"><code>		throw("manual span allocation called with non-manually-managed type")</code></span>
<span class="codeline" id="line-989"><code>	}</code></span>
<span class="codeline" id="line-990"><code>	return h.allocSpan(npages, typ, 0)</code></span>
<span class="codeline" id="line-991"><code>}</code></span>
<span class="codeline" id="line-992"><code></code></span>
<span class="codeline" id="line-993"><code>// setSpans modifies the span map so [spanOf(base), spanOf(base+npage*pageSize))</code></span>
<span class="codeline" id="line-994"><code>// is s.</code></span>
<span class="codeline" id="line-995"><code>func (h *mheap) setSpans(base, npage uintptr, s *mspan) {</code></span>
<span class="codeline" id="line-996"><code>	p := base / pageSize</code></span>
<span class="codeline" id="line-997"><code>	ai := arenaIndex(base)</code></span>
<span class="codeline" id="line-998"><code>	ha := h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-999"><code>	for n := uintptr(0); n &lt; npage; n++ {</code></span>
<span class="codeline" id="line-1000"><code>		i := (p + n) % pagesPerArena</code></span>
<span class="codeline" id="line-1001"><code>		if i == 0 {</code></span>
<span class="codeline" id="line-1002"><code>			ai = arenaIndex(base + n*pageSize)</code></span>
<span class="codeline" id="line-1003"><code>			ha = h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-1004"><code>		}</code></span>
<span class="codeline" id="line-1005"><code>		ha.spans[i] = s</code></span>
<span class="codeline" id="line-1006"><code>	}</code></span>
<span class="codeline" id="line-1007"><code>}</code></span>
<span class="codeline" id="line-1008"><code></code></span>
<span class="codeline" id="line-1009"><code>// allocNeedsZero checks if the region of address space [base, base+npage*pageSize),</code></span>
<span class="codeline" id="line-1010"><code>// assumed to be allocated, needs to be zeroed, updating heap arena metadata for</code></span>
<span class="codeline" id="line-1011"><code>// future allocations.</code></span>
<span class="codeline" id="line-1012"><code>//</code></span>
<span class="codeline" id="line-1013"><code>// This must be called each time pages are allocated from the heap, even if the page</code></span>
<span class="codeline" id="line-1014"><code>// allocator can otherwise prove the memory it's allocating is already zero because</code></span>
<span class="codeline" id="line-1015"><code>// they're fresh from the operating system. It updates heapArena metadata that is</code></span>
<span class="codeline" id="line-1016"><code>// critical for future page allocations.</code></span>
<span class="codeline" id="line-1017"><code>//</code></span>
<span class="codeline" id="line-1018"><code>// There are no locking constraints on this method.</code></span>
<span class="codeline" id="line-1019"><code>func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool) {</code></span>
<span class="codeline" id="line-1020"><code>	for npage &gt; 0 {</code></span>
<span class="codeline" id="line-1021"><code>		ai := arenaIndex(base)</code></span>
<span class="codeline" id="line-1022"><code>		ha := h.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-1023"><code></code></span>
<span class="codeline" id="line-1024"><code>		zeroedBase := atomic.Loaduintptr(&amp;ha.zeroedBase)</code></span>
<span class="codeline" id="line-1025"><code>		arenaBase := base % heapArenaBytes</code></span>
<span class="codeline" id="line-1026"><code>		if arenaBase &lt; zeroedBase {</code></span>
<span class="codeline" id="line-1027"><code>			// We extended into the non-zeroed part of the</code></span>
<span class="codeline" id="line-1028"><code>			// arena, so this region needs to be zeroed before use.</code></span>
<span class="codeline" id="line-1029"><code>			//</code></span>
<span class="codeline" id="line-1030"><code>			// zeroedBase is monotonically increasing, so if we see this now then</code></span>
<span class="codeline" id="line-1031"><code>			// we can be sure we need to zero this memory region.</code></span>
<span class="codeline" id="line-1032"><code>			//</code></span>
<span class="codeline" id="line-1033"><code>			// We still need to update zeroedBase for this arena, and</code></span>
<span class="codeline" id="line-1034"><code>			// potentially more arenas.</code></span>
<span class="codeline" id="line-1035"><code>			needZero = true</code></span>
<span class="codeline" id="line-1036"><code>		}</code></span>
<span class="codeline" id="line-1037"><code>		// We may observe arenaBase &gt; zeroedBase if we're racing with one or more</code></span>
<span class="codeline" id="line-1038"><code>		// allocations which are acquiring memory directly before us in the address</code></span>
<span class="codeline" id="line-1039"><code>		// space. But, because we know no one else is acquiring *this* memory, it's</code></span>
<span class="codeline" id="line-1040"><code>		// still safe to not zero.</code></span>
<span class="codeline" id="line-1041"><code></code></span>
<span class="codeline" id="line-1042"><code>		// Compute how far into the arena we extend into, capped</code></span>
<span class="codeline" id="line-1043"><code>		// at heapArenaBytes.</code></span>
<span class="codeline" id="line-1044"><code>		arenaLimit := arenaBase + npage*pageSize</code></span>
<span class="codeline" id="line-1045"><code>		if arenaLimit &gt; heapArenaBytes {</code></span>
<span class="codeline" id="line-1046"><code>			arenaLimit = heapArenaBytes</code></span>
<span class="codeline" id="line-1047"><code>		}</code></span>
<span class="codeline" id="line-1048"><code>		// Increase ha.zeroedBase so it's &gt;= arenaLimit.</code></span>
<span class="codeline" id="line-1049"><code>		// We may be racing with other updates.</code></span>
<span class="codeline" id="line-1050"><code>		for arenaLimit &gt; zeroedBase {</code></span>
<span class="codeline" id="line-1051"><code>			if atomic.Casuintptr(&amp;ha.zeroedBase, zeroedBase, arenaLimit) {</code></span>
<span class="codeline" id="line-1052"><code>				break</code></span>
<span class="codeline" id="line-1053"><code>			}</code></span>
<span class="codeline" id="line-1054"><code>			zeroedBase = atomic.Loaduintptr(&amp;ha.zeroedBase)</code></span>
<span class="codeline" id="line-1055"><code>			// Double check basic conditions of zeroedBase.</code></span>
<span class="codeline" id="line-1056"><code>			if zeroedBase &lt;= arenaLimit &amp;&amp; zeroedBase &gt; arenaBase {</code></span>
<span class="codeline" id="line-1057"><code>				// The zeroedBase moved into the space we were trying to</code></span>
<span class="codeline" id="line-1058"><code>				// claim. That's very bad, and indicates someone allocated</code></span>
<span class="codeline" id="line-1059"><code>				// the same region we did.</code></span>
<span class="codeline" id="line-1060"><code>				throw("potentially overlapping in-use allocations detected")</code></span>
<span class="codeline" id="line-1061"><code>			}</code></span>
<span class="codeline" id="line-1062"><code>		}</code></span>
<span class="codeline" id="line-1063"><code></code></span>
<span class="codeline" id="line-1064"><code>		// Move base forward and subtract from npage to move into</code></span>
<span class="codeline" id="line-1065"><code>		// the next arena, or finish.</code></span>
<span class="codeline" id="line-1066"><code>		base += arenaLimit - arenaBase</code></span>
<span class="codeline" id="line-1067"><code>		npage -= (arenaLimit - arenaBase) / pageSize</code></span>
<span class="codeline" id="line-1068"><code>	}</code></span>
<span class="codeline" id="line-1069"><code>	return</code></span>
<span class="codeline" id="line-1070"><code>}</code></span>
<span class="codeline" id="line-1071"><code></code></span>
<span class="codeline" id="line-1072"><code>// tryAllocMSpan attempts to allocate an mspan object from</code></span>
<span class="codeline" id="line-1073"><code>// the P-local cache, but may fail.</code></span>
<span class="codeline" id="line-1074"><code>//</code></span>
<span class="codeline" id="line-1075"><code>// h.lock need not be held.</code></span>
<span class="codeline" id="line-1076"><code>//</code></span>
<span class="codeline" id="line-1077"><code>// This caller must ensure that its P won't change underneath</code></span>
<span class="codeline" id="line-1078"><code>// it during this function. Currently to ensure that we enforce</code></span>
<span class="codeline" id="line-1079"><code>// that the function is run on the system stack, because that's</code></span>
<span class="codeline" id="line-1080"><code>// the only place it is used now. In the future, this requirement</code></span>
<span class="codeline" id="line-1081"><code>// may be relaxed if its use is necessary elsewhere.</code></span>
<span class="codeline" id="line-1082"><code>//</code></span>
<span class="codeline" id="line-1083"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1084"><code>func (h *mheap) tryAllocMSpan() *mspan {</code></span>
<span class="codeline" id="line-1085"><code>	pp := getg().m.p.ptr()</code></span>
<span class="codeline" id="line-1086"><code>	// If we don't have a p or the cache is empty, we can't do</code></span>
<span class="codeline" id="line-1087"><code>	// anything here.</code></span>
<span class="codeline" id="line-1088"><code>	if pp == nil || pp.mspancache.len == 0 {</code></span>
<span class="codeline" id="line-1089"><code>		return nil</code></span>
<span class="codeline" id="line-1090"><code>	}</code></span>
<span class="codeline" id="line-1091"><code>	// Pull off the last entry in the cache.</code></span>
<span class="codeline" id="line-1092"><code>	s := pp.mspancache.buf[pp.mspancache.len-1]</code></span>
<span class="codeline" id="line-1093"><code>	pp.mspancache.len--</code></span>
<span class="codeline" id="line-1094"><code>	return s</code></span>
<span class="codeline" id="line-1095"><code>}</code></span>
<span class="codeline" id="line-1096"><code></code></span>
<span class="codeline" id="line-1097"><code>// allocMSpanLocked allocates an mspan object.</code></span>
<span class="codeline" id="line-1098"><code>//</code></span>
<span class="codeline" id="line-1099"><code>// h.lock must be held.</code></span>
<span class="codeline" id="line-1100"><code>//</code></span>
<span class="codeline" id="line-1101"><code>// allocMSpanLocked must be called on the system stack because</code></span>
<span class="codeline" id="line-1102"><code>// its caller holds the heap lock. See mheap for details.</code></span>
<span class="codeline" id="line-1103"><code>// Running on the system stack also ensures that we won't</code></span>
<span class="codeline" id="line-1104"><code>// switch Ps during this function. See tryAllocMSpan for details.</code></span>
<span class="codeline" id="line-1105"><code>//</code></span>
<span class="codeline" id="line-1106"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1107"><code>func (h *mheap) allocMSpanLocked() *mspan {</code></span>
<span class="codeline" id="line-1108"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1109"><code></code></span>
<span class="codeline" id="line-1110"><code>	pp := getg().m.p.ptr()</code></span>
<span class="codeline" id="line-1111"><code>	if pp == nil {</code></span>
<span class="codeline" id="line-1112"><code>		// We don't have a p so just do the normal thing.</code></span>
<span class="codeline" id="line-1113"><code>		return (*mspan)(h.spanalloc.alloc())</code></span>
<span class="codeline" id="line-1114"><code>	}</code></span>
<span class="codeline" id="line-1115"><code>	// Refill the cache if necessary.</code></span>
<span class="codeline" id="line-1116"><code>	if pp.mspancache.len == 0 {</code></span>
<span class="codeline" id="line-1117"><code>		const refillCount = len(pp.mspancache.buf) / 2</code></span>
<span class="codeline" id="line-1118"><code>		for i := 0; i &lt; refillCount; i++ {</code></span>
<span class="codeline" id="line-1119"><code>			pp.mspancache.buf[i] = (*mspan)(h.spanalloc.alloc())</code></span>
<span class="codeline" id="line-1120"><code>		}</code></span>
<span class="codeline" id="line-1121"><code>		pp.mspancache.len = refillCount</code></span>
<span class="codeline" id="line-1122"><code>	}</code></span>
<span class="codeline" id="line-1123"><code>	// Pull off the last entry in the cache.</code></span>
<span class="codeline" id="line-1124"><code>	s := pp.mspancache.buf[pp.mspancache.len-1]</code></span>
<span class="codeline" id="line-1125"><code>	pp.mspancache.len--</code></span>
<span class="codeline" id="line-1126"><code>	return s</code></span>
<span class="codeline" id="line-1127"><code>}</code></span>
<span class="codeline" id="line-1128"><code></code></span>
<span class="codeline" id="line-1129"><code>// freeMSpanLocked free an mspan object.</code></span>
<span class="codeline" id="line-1130"><code>//</code></span>
<span class="codeline" id="line-1131"><code>// h.lock must be held.</code></span>
<span class="codeline" id="line-1132"><code>//</code></span>
<span class="codeline" id="line-1133"><code>// freeMSpanLocked must be called on the system stack because</code></span>
<span class="codeline" id="line-1134"><code>// its caller holds the heap lock. See mheap for details.</code></span>
<span class="codeline" id="line-1135"><code>// Running on the system stack also ensures that we won't</code></span>
<span class="codeline" id="line-1136"><code>// switch Ps during this function. See tryAllocMSpan for details.</code></span>
<span class="codeline" id="line-1137"><code>//</code></span>
<span class="codeline" id="line-1138"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1139"><code>func (h *mheap) freeMSpanLocked(s *mspan) {</code></span>
<span class="codeline" id="line-1140"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1141"><code></code></span>
<span class="codeline" id="line-1142"><code>	pp := getg().m.p.ptr()</code></span>
<span class="codeline" id="line-1143"><code>	// First try to free the mspan directly to the cache.</code></span>
<span class="codeline" id="line-1144"><code>	if pp != nil &amp;&amp; pp.mspancache.len &lt; len(pp.mspancache.buf) {</code></span>
<span class="codeline" id="line-1145"><code>		pp.mspancache.buf[pp.mspancache.len] = s</code></span>
<span class="codeline" id="line-1146"><code>		pp.mspancache.len++</code></span>
<span class="codeline" id="line-1147"><code>		return</code></span>
<span class="codeline" id="line-1148"><code>	}</code></span>
<span class="codeline" id="line-1149"><code>	// Failing that (or if we don't have a p), just free it to</code></span>
<span class="codeline" id="line-1150"><code>	// the heap.</code></span>
<span class="codeline" id="line-1151"><code>	h.spanalloc.free(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1152"><code>}</code></span>
<span class="codeline" id="line-1153"><code></code></span>
<span class="codeline" id="line-1154"><code>// allocSpan allocates an mspan which owns npages worth of memory.</code></span>
<span class="codeline" id="line-1155"><code>//</code></span>
<span class="codeline" id="line-1156"><code>// If typ.manual() == false, allocSpan allocates a heap span of class spanclass</code></span>
<span class="codeline" id="line-1157"><code>// and updates heap accounting. If manual == true, allocSpan allocates a</code></span>
<span class="codeline" id="line-1158"><code>// manually-managed span (spanclass is ignored), and the caller is</code></span>
<span class="codeline" id="line-1159"><code>// responsible for any accounting related to its use of the span. Either</code></span>
<span class="codeline" id="line-1160"><code>// way, allocSpan will atomically add the bytes in the newly allocated</code></span>
<span class="codeline" id="line-1161"><code>// span to *sysStat.</code></span>
<span class="codeline" id="line-1162"><code>//</code></span>
<span class="codeline" id="line-1163"><code>// The returned span is fully initialized.</code></span>
<span class="codeline" id="line-1164"><code>//</code></span>
<span class="codeline" id="line-1165"><code>// h.lock must not be held.</code></span>
<span class="codeline" id="line-1166"><code>//</code></span>
<span class="codeline" id="line-1167"><code>// allocSpan must be called on the system stack both because it acquires</code></span>
<span class="codeline" id="line-1168"><code>// the heap lock and because it must block GC transitions.</code></span>
<span class="codeline" id="line-1169"><code>//</code></span>
<span class="codeline" id="line-1170"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1171"><code>func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan) {</code></span>
<span class="codeline" id="line-1172"><code>	// Function-global state.</code></span>
<span class="codeline" id="line-1173"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1174"><code>	base, scav := uintptr(0), uintptr(0)</code></span>
<span class="codeline" id="line-1175"><code>	growth := uintptr(0)</code></span>
<span class="codeline" id="line-1176"><code></code></span>
<span class="codeline" id="line-1177"><code>	// On some platforms we need to provide physical page aligned stack</code></span>
<span class="codeline" id="line-1178"><code>	// allocations. Where the page size is less than the physical page</code></span>
<span class="codeline" id="line-1179"><code>	// size, we already manage to do this by default.</code></span>
<span class="codeline" id="line-1180"><code>	needPhysPageAlign := physPageAlignedStacks &amp;&amp; typ == spanAllocStack &amp;&amp; pageSize &lt; physPageSize</code></span>
<span class="codeline" id="line-1181"><code></code></span>
<span class="codeline" id="line-1182"><code>	// If the allocation is small enough, try the page cache!</code></span>
<span class="codeline" id="line-1183"><code>	// The page cache does not support aligned allocations, so we cannot use</code></span>
<span class="codeline" id="line-1184"><code>	// it if we need to provide a physical page aligned stack allocation.</code></span>
<span class="codeline" id="line-1185"><code>	pp := gp.m.p.ptr()</code></span>
<span class="codeline" id="line-1186"><code>	if !needPhysPageAlign &amp;&amp; pp != nil &amp;&amp; npages &lt; pageCachePages/4 {</code></span>
<span class="codeline" id="line-1187"><code>		c := &amp;pp.pcache</code></span>
<span class="codeline" id="line-1188"><code></code></span>
<span class="codeline" id="line-1189"><code>		// If the cache is empty, refill it.</code></span>
<span class="codeline" id="line-1190"><code>		if c.empty() {</code></span>
<span class="codeline" id="line-1191"><code>			lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1192"><code>			*c = h.pages.allocToCache()</code></span>
<span class="codeline" id="line-1193"><code>			unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1194"><code>		}</code></span>
<span class="codeline" id="line-1195"><code></code></span>
<span class="codeline" id="line-1196"><code>		// Try to allocate from the cache.</code></span>
<span class="codeline" id="line-1197"><code>		base, scav = c.alloc(npages)</code></span>
<span class="codeline" id="line-1198"><code>		if base != 0 {</code></span>
<span class="codeline" id="line-1199"><code>			s = h.tryAllocMSpan()</code></span>
<span class="codeline" id="line-1200"><code>			if s != nil {</code></span>
<span class="codeline" id="line-1201"><code>				goto HaveSpan</code></span>
<span class="codeline" id="line-1202"><code>			}</code></span>
<span class="codeline" id="line-1203"><code>			// We have a base but no mspan, so we need</code></span>
<span class="codeline" id="line-1204"><code>			// to lock the heap.</code></span>
<span class="codeline" id="line-1205"><code>		}</code></span>
<span class="codeline" id="line-1206"><code>	}</code></span>
<span class="codeline" id="line-1207"><code></code></span>
<span class="codeline" id="line-1208"><code>	// For one reason or another, we couldn't get the</code></span>
<span class="codeline" id="line-1209"><code>	// whole job done without the heap lock.</code></span>
<span class="codeline" id="line-1210"><code>	lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1211"><code></code></span>
<span class="codeline" id="line-1212"><code>	if needPhysPageAlign {</code></span>
<span class="codeline" id="line-1213"><code>		// Overallocate by a physical page to allow for later alignment.</code></span>
<span class="codeline" id="line-1214"><code>		extraPages := physPageSize / pageSize</code></span>
<span class="codeline" id="line-1215"><code></code></span>
<span class="codeline" id="line-1216"><code>		// Find a big enough region first, but then only allocate the</code></span>
<span class="codeline" id="line-1217"><code>		// aligned portion. We can't just allocate and then free the</code></span>
<span class="codeline" id="line-1218"><code>		// edges because we need to account for scavenged memory, and</code></span>
<span class="codeline" id="line-1219"><code>		// that's difficult with alloc.</code></span>
<span class="codeline" id="line-1220"><code>		//</code></span>
<span class="codeline" id="line-1221"><code>		// Note that we skip updates to searchAddr here. It's OK if</code></span>
<span class="codeline" id="line-1222"><code>		// it's stale and higher than normal; it'll operate correctly,</code></span>
<span class="codeline" id="line-1223"><code>		// just come with a performance cost.</code></span>
<span class="codeline" id="line-1224"><code>		base, _ = h.pages.find(npages + extraPages)</code></span>
<span class="codeline" id="line-1225"><code>		if base == 0 {</code></span>
<span class="codeline" id="line-1226"><code>			var ok bool</code></span>
<span class="codeline" id="line-1227"><code>			growth, ok = h.grow(npages + extraPages)</code></span>
<span class="codeline" id="line-1228"><code>			if !ok {</code></span>
<span class="codeline" id="line-1229"><code>				unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1230"><code>				return nil</code></span>
<span class="codeline" id="line-1231"><code>			}</code></span>
<span class="codeline" id="line-1232"><code>			base, _ = h.pages.find(npages + extraPages)</code></span>
<span class="codeline" id="line-1233"><code>			if base == 0 {</code></span>
<span class="codeline" id="line-1234"><code>				throw("grew heap, but no adequate free space found")</code></span>
<span class="codeline" id="line-1235"><code>			}</code></span>
<span class="codeline" id="line-1236"><code>		}</code></span>
<span class="codeline" id="line-1237"><code>		base = alignUp(base, physPageSize)</code></span>
<span class="codeline" id="line-1238"><code>		scav = h.pages.allocRange(base, npages)</code></span>
<span class="codeline" id="line-1239"><code>	}</code></span>
<span class="codeline" id="line-1240"><code></code></span>
<span class="codeline" id="line-1241"><code>	if base == 0 {</code></span>
<span class="codeline" id="line-1242"><code>		// Try to acquire a base address.</code></span>
<span class="codeline" id="line-1243"><code>		base, scav = h.pages.alloc(npages)</code></span>
<span class="codeline" id="line-1244"><code>		if base == 0 {</code></span>
<span class="codeline" id="line-1245"><code>			var ok bool</code></span>
<span class="codeline" id="line-1246"><code>			growth, ok = h.grow(npages)</code></span>
<span class="codeline" id="line-1247"><code>			if !ok {</code></span>
<span class="codeline" id="line-1248"><code>				unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1249"><code>				return nil</code></span>
<span class="codeline" id="line-1250"><code>			}</code></span>
<span class="codeline" id="line-1251"><code>			base, scav = h.pages.alloc(npages)</code></span>
<span class="codeline" id="line-1252"><code>			if base == 0 {</code></span>
<span class="codeline" id="line-1253"><code>				throw("grew heap, but no adequate free space found")</code></span>
<span class="codeline" id="line-1254"><code>			}</code></span>
<span class="codeline" id="line-1255"><code>		}</code></span>
<span class="codeline" id="line-1256"><code>	}</code></span>
<span class="codeline" id="line-1257"><code>	if s == nil {</code></span>
<span class="codeline" id="line-1258"><code>		// We failed to get an mspan earlier, so grab</code></span>
<span class="codeline" id="line-1259"><code>		// one now that we have the heap lock.</code></span>
<span class="codeline" id="line-1260"><code>		s = h.allocMSpanLocked()</code></span>
<span class="codeline" id="line-1261"><code>	}</code></span>
<span class="codeline" id="line-1262"><code>	unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1263"><code></code></span>
<span class="codeline" id="line-1264"><code>HaveSpan:</code></span>
<span class="codeline" id="line-1265"><code>	// Decide if we need to scavenge in response to what we just allocated.</code></span>
<span class="codeline" id="line-1266"><code>	// Specifically, we track the maximum amount of memory to scavenge of all</code></span>
<span class="codeline" id="line-1267"><code>	// the alternatives below, assuming that the maximum satisfies *all*</code></span>
<span class="codeline" id="line-1268"><code>	// conditions we check (e.g. if we need to scavenge X to satisfy the</code></span>
<span class="codeline" id="line-1269"><code>	// memory limit and Y to satisfy heap-growth scavenging, and Y &gt; X, then</code></span>
<span class="codeline" id="line-1270"><code>	// it's fine to pick Y, because the memory limit is still satisfied).</code></span>
<span class="codeline" id="line-1271"><code>	//</code></span>
<span class="codeline" id="line-1272"><code>	// It's fine to do this after allocating because we expect any scavenged</code></span>
<span class="codeline" id="line-1273"><code>	// pages not to get touched until we return. Simultaneously, it's important</code></span>
<span class="codeline" id="line-1274"><code>	// to do this before calling sysUsed because that may commit address space.</code></span>
<span class="codeline" id="line-1275"><code>	bytesToScavenge := uintptr(0)</code></span>
<span class="codeline" id="line-1276"><code>	forceScavenge := false</code></span>
<span class="codeline" id="line-1277"><code>	if limit := gcController.memoryLimit.Load(); !gcCPULimiter.limiting() {</code></span>
<span class="codeline" id="line-1278"><code>		// Assist with scavenging to maintain the memory limit by the amount</code></span>
<span class="codeline" id="line-1279"><code>		// that we expect to page in.</code></span>
<span class="codeline" id="line-1280"><code>		inuse := gcController.mappedReady.Load()</code></span>
<span class="codeline" id="line-1281"><code>		// Be careful about overflow, especially with uintptrs. Even on 32-bit platforms</code></span>
<span class="codeline" id="line-1282"><code>		// someone can set a really big memory limit that isn't maxInt64.</code></span>
<span class="codeline" id="line-1283"><code>		if uint64(scav)+inuse &gt; uint64(limit) {</code></span>
<span class="codeline" id="line-1284"><code>			bytesToScavenge = uintptr(uint64(scav) + inuse - uint64(limit))</code></span>
<span class="codeline" id="line-1285"><code>			forceScavenge = true</code></span>
<span class="codeline" id="line-1286"><code>		}</code></span>
<span class="codeline" id="line-1287"><code>	}</code></span>
<span class="codeline" id="line-1288"><code>	if goal := scavenge.gcPercentGoal.Load(); goal != ^uint64(0) &amp;&amp; growth &gt; 0 {</code></span>
<span class="codeline" id="line-1289"><code>		// We just caused a heap growth, so scavenge down what will soon be used.</code></span>
<span class="codeline" id="line-1290"><code>		// By scavenging inline we deal with the failure to allocate out of</code></span>
<span class="codeline" id="line-1291"><code>		// memory fragments by scavenging the memory fragments that are least</code></span>
<span class="codeline" id="line-1292"><code>		// likely to be re-used.</code></span>
<span class="codeline" id="line-1293"><code>		//</code></span>
<span class="codeline" id="line-1294"><code>		// Only bother with this because we're not using a memory limit. We don't</code></span>
<span class="codeline" id="line-1295"><code>		// care about heap growths as long as we're under the memory limit, and the</code></span>
<span class="codeline" id="line-1296"><code>		// previous check for scaving already handles that.</code></span>
<span class="codeline" id="line-1297"><code>		if retained := heapRetained(); retained+uint64(growth) &gt; goal {</code></span>
<span class="codeline" id="line-1298"><code>			// The scavenging algorithm requires the heap lock to be dropped so it</code></span>
<span class="codeline" id="line-1299"><code>			// can acquire it only sparingly. This is a potentially expensive operation</code></span>
<span class="codeline" id="line-1300"><code>			// so it frees up other goroutines to allocate in the meanwhile. In fact,</code></span>
<span class="codeline" id="line-1301"><code>			// they can make use of the growth we just created.</code></span>
<span class="codeline" id="line-1302"><code>			todo := growth</code></span>
<span class="codeline" id="line-1303"><code>			if overage := uintptr(retained + uint64(growth) - goal); todo &gt; overage {</code></span>
<span class="codeline" id="line-1304"><code>				todo = overage</code></span>
<span class="codeline" id="line-1305"><code>			}</code></span>
<span class="codeline" id="line-1306"><code>			if todo &gt; bytesToScavenge {</code></span>
<span class="codeline" id="line-1307"><code>				bytesToScavenge = todo</code></span>
<span class="codeline" id="line-1308"><code>			}</code></span>
<span class="codeline" id="line-1309"><code>		}</code></span>
<span class="codeline" id="line-1310"><code>	}</code></span>
<span class="codeline" id="line-1311"><code>	// There are a few very limited circumstances where we won't have a P here.</code></span>
<span class="codeline" id="line-1312"><code>	// It's OK to simply skip scavenging in these cases. Something else will notice</code></span>
<span class="codeline" id="line-1313"><code>	// and pick up the tab.</code></span>
<span class="codeline" id="line-1314"><code>	var now int64</code></span>
<span class="codeline" id="line-1315"><code>	if pp != nil &amp;&amp; bytesToScavenge &gt; 0 {</code></span>
<span class="codeline" id="line-1316"><code>		// Measure how long we spent scavenging and add that measurement to the assist</code></span>
<span class="codeline" id="line-1317"><code>		// time so we can track it for the GC CPU limiter.</code></span>
<span class="codeline" id="line-1318"><code>		//</code></span>
<span class="codeline" id="line-1319"><code>		// Limiter event tracking might be disabled if we end up here</code></span>
<span class="codeline" id="line-1320"><code>		// while on a mark worker.</code></span>
<span class="codeline" id="line-1321"><code>		start := nanotime()</code></span>
<span class="codeline" id="line-1322"><code>		track := pp.limiterEvent.start(limiterEventScavengeAssist, start)</code></span>
<span class="codeline" id="line-1323"><code></code></span>
<span class="codeline" id="line-1324"><code>		// Scavenge, but back out if the limiter turns on.</code></span>
<span class="codeline" id="line-1325"><code>		released := h.pages.scavenge(bytesToScavenge, func() bool {</code></span>
<span class="codeline" id="line-1326"><code>			return gcCPULimiter.limiting()</code></span>
<span class="codeline" id="line-1327"><code>		}, forceScavenge)</code></span>
<span class="codeline" id="line-1328"><code></code></span>
<span class="codeline" id="line-1329"><code>		mheap_.pages.scav.releasedEager.Add(released)</code></span>
<span class="codeline" id="line-1330"><code></code></span>
<span class="codeline" id="line-1331"><code>		// Finish up accounting.</code></span>
<span class="codeline" id="line-1332"><code>		now = nanotime()</code></span>
<span class="codeline" id="line-1333"><code>		if track {</code></span>
<span class="codeline" id="line-1334"><code>			pp.limiterEvent.stop(limiterEventScavengeAssist, now)</code></span>
<span class="codeline" id="line-1335"><code>		}</code></span>
<span class="codeline" id="line-1336"><code>		scavenge.assistTime.Add(now - start)</code></span>
<span class="codeline" id="line-1337"><code>	}</code></span>
<span class="codeline" id="line-1338"><code></code></span>
<span class="codeline" id="line-1339"><code>	// Initialize the span.</code></span>
<span class="codeline" id="line-1340"><code>	h.initSpan(s, typ, spanclass, base, npages)</code></span>
<span class="codeline" id="line-1341"><code></code></span>
<span class="codeline" id="line-1342"><code>	// Commit and account for any scavenged memory that the span now owns.</code></span>
<span class="codeline" id="line-1343"><code>	nbytes := npages * pageSize</code></span>
<span class="codeline" id="line-1344"><code>	if scav != 0 {</code></span>
<span class="codeline" id="line-1345"><code>		// sysUsed all the pages that are actually available</code></span>
<span class="codeline" id="line-1346"><code>		// in the span since some of them might be scavenged.</code></span>
<span class="codeline" id="line-1347"><code>		sysUsed(unsafe.Pointer(base), nbytes, scav)</code></span>
<span class="codeline" id="line-1348"><code>		gcController.heapReleased.add(-int64(scav))</code></span>
<span class="codeline" id="line-1349"><code>	}</code></span>
<span class="codeline" id="line-1350"><code>	// Update stats.</code></span>
<span class="codeline" id="line-1351"><code>	gcController.heapFree.add(-int64(nbytes - scav))</code></span>
<span class="codeline" id="line-1352"><code>	if typ == spanAllocHeap {</code></span>
<span class="codeline" id="line-1353"><code>		gcController.heapInUse.add(int64(nbytes))</code></span>
<span class="codeline" id="line-1354"><code>	}</code></span>
<span class="codeline" id="line-1355"><code>	// Update consistent stats.</code></span>
<span class="codeline" id="line-1356"><code>	stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-1357"><code>	atomic.Xaddint64(&amp;stats.committed, int64(scav))</code></span>
<span class="codeline" id="line-1358"><code>	atomic.Xaddint64(&amp;stats.released, -int64(scav))</code></span>
<span class="codeline" id="line-1359"><code>	switch typ {</code></span>
<span class="codeline" id="line-1360"><code>	case spanAllocHeap:</code></span>
<span class="codeline" id="line-1361"><code>		atomic.Xaddint64(&amp;stats.inHeap, int64(nbytes))</code></span>
<span class="codeline" id="line-1362"><code>	case spanAllocStack:</code></span>
<span class="codeline" id="line-1363"><code>		atomic.Xaddint64(&amp;stats.inStacks, int64(nbytes))</code></span>
<span class="codeline" id="line-1364"><code>	case spanAllocPtrScalarBits:</code></span>
<span class="codeline" id="line-1365"><code>		atomic.Xaddint64(&amp;stats.inPtrScalarBits, int64(nbytes))</code></span>
<span class="codeline" id="line-1366"><code>	case spanAllocWorkBuf:</code></span>
<span class="codeline" id="line-1367"><code>		atomic.Xaddint64(&amp;stats.inWorkBufs, int64(nbytes))</code></span>
<span class="codeline" id="line-1368"><code>	}</code></span>
<span class="codeline" id="line-1369"><code>	memstats.heapStats.release()</code></span>
<span class="codeline" id="line-1370"><code></code></span>
<span class="codeline" id="line-1371"><code>	pageTraceAlloc(pp, now, base, npages)</code></span>
<span class="codeline" id="line-1372"><code>	return s</code></span>
<span class="codeline" id="line-1373"><code>}</code></span>
<span class="codeline" id="line-1374"><code></code></span>
<span class="codeline" id="line-1375"><code>// initSpan initializes a blank span s which will represent the range</code></span>
<span class="codeline" id="line-1376"><code>// [base, base+npages*pageSize). typ is the type of span being allocated.</code></span>
<span class="codeline" id="line-1377"><code>func (h *mheap) initSpan(s *mspan, typ spanAllocType, spanclass spanClass, base, npages uintptr) {</code></span>
<span class="codeline" id="line-1378"><code>	// At this point, both s != nil and base != 0, and the heap</code></span>
<span class="codeline" id="line-1379"><code>	// lock is no longer held. Initialize the span.</code></span>
<span class="codeline" id="line-1380"><code>	s.init(base, npages)</code></span>
<span class="codeline" id="line-1381"><code>	if h.allocNeedsZero(base, npages) {</code></span>
<span class="codeline" id="line-1382"><code>		s.needzero = 1</code></span>
<span class="codeline" id="line-1383"><code>	}</code></span>
<span class="codeline" id="line-1384"><code>	nbytes := npages * pageSize</code></span>
<span class="codeline" id="line-1385"><code>	if typ.manual() {</code></span>
<span class="codeline" id="line-1386"><code>		s.manualFreeList = 0</code></span>
<span class="codeline" id="line-1387"><code>		s.nelems = 0</code></span>
<span class="codeline" id="line-1388"><code>		s.limit = s.base() + s.npages*pageSize</code></span>
<span class="codeline" id="line-1389"><code>		s.state.set(mSpanManual)</code></span>
<span class="codeline" id="line-1390"><code>	} else {</code></span>
<span class="codeline" id="line-1391"><code>		// We must set span properties before the span is published anywhere</code></span>
<span class="codeline" id="line-1392"><code>		// since we're not holding the heap lock.</code></span>
<span class="codeline" id="line-1393"><code>		s.spanclass = spanclass</code></span>
<span class="codeline" id="line-1394"><code>		if sizeclass := spanclass.sizeclass(); sizeclass == 0 {</code></span>
<span class="codeline" id="line-1395"><code>			s.elemsize = nbytes</code></span>
<span class="codeline" id="line-1396"><code>			s.nelems = 1</code></span>
<span class="codeline" id="line-1397"><code>			s.divMul = 0</code></span>
<span class="codeline" id="line-1398"><code>		} else {</code></span>
<span class="codeline" id="line-1399"><code>			s.elemsize = uintptr(class_to_size[sizeclass])</code></span>
<span class="codeline" id="line-1400"><code>			if goexperiment.AllocHeaders &amp;&amp; !s.spanclass.noscan() &amp;&amp; heapBitsInSpan(s.elemsize) {</code></span>
<span class="codeline" id="line-1401"><code>				// In the allocheaders experiment, reserve space for the pointer/scan bitmap at the end.</code></span>
<span class="codeline" id="line-1402"><code>				s.nelems = uint16((nbytes - (nbytes / goarch.PtrSize / 8)) / s.elemsize)</code></span>
<span class="codeline" id="line-1403"><code>			} else {</code></span>
<span class="codeline" id="line-1404"><code>				s.nelems = uint16(nbytes / s.elemsize)</code></span>
<span class="codeline" id="line-1405"><code>			}</code></span>
<span class="codeline" id="line-1406"><code>			s.divMul = class_to_divmagic[sizeclass]</code></span>
<span class="codeline" id="line-1407"><code>		}</code></span>
<span class="codeline" id="line-1408"><code></code></span>
<span class="codeline" id="line-1409"><code>		// Initialize mark and allocation structures.</code></span>
<span class="codeline" id="line-1410"><code>		s.freeindex = 0</code></span>
<span class="codeline" id="line-1411"><code>		s.freeIndexForScan = 0</code></span>
<span class="codeline" id="line-1412"><code>		s.allocCache = ^uint64(0) // all 1s indicating all free.</code></span>
<span class="codeline" id="line-1413"><code>		s.gcmarkBits = newMarkBits(uintptr(s.nelems))</code></span>
<span class="codeline" id="line-1414"><code>		s.allocBits = newAllocBits(uintptr(s.nelems))</code></span>
<span class="codeline" id="line-1415"><code></code></span>
<span class="codeline" id="line-1416"><code>		// It's safe to access h.sweepgen without the heap lock because it's</code></span>
<span class="codeline" id="line-1417"><code>		// only ever updated with the world stopped and we run on the</code></span>
<span class="codeline" id="line-1418"><code>		// systemstack which blocks a STW transition.</code></span>
<span class="codeline" id="line-1419"><code>		atomic.Store(&amp;s.sweepgen, h.sweepgen)</code></span>
<span class="codeline" id="line-1420"><code></code></span>
<span class="codeline" id="line-1421"><code>		// Now that the span is filled in, set its state. This</code></span>
<span class="codeline" id="line-1422"><code>		// is a publication barrier for the other fields in</code></span>
<span class="codeline" id="line-1423"><code>		// the span. While valid pointers into this span</code></span>
<span class="codeline" id="line-1424"><code>		// should never be visible until the span is returned,</code></span>
<span class="codeline" id="line-1425"><code>		// if the garbage collector finds an invalid pointer,</code></span>
<span class="codeline" id="line-1426"><code>		// access to the span may race with initialization of</code></span>
<span class="codeline" id="line-1427"><code>		// the span. We resolve this race by atomically</code></span>
<span class="codeline" id="line-1428"><code>		// setting the state after the span is fully</code></span>
<span class="codeline" id="line-1429"><code>		// initialized, and atomically checking the state in</code></span>
<span class="codeline" id="line-1430"><code>		// any situation where a pointer is suspect.</code></span>
<span class="codeline" id="line-1431"><code>		s.state.set(mSpanInUse)</code></span>
<span class="codeline" id="line-1432"><code>	}</code></span>
<span class="codeline" id="line-1433"><code></code></span>
<span class="codeline" id="line-1434"><code>	// Publish the span in various locations.</code></span>
<span class="codeline" id="line-1435"><code></code></span>
<span class="codeline" id="line-1436"><code>	// This is safe to call without the lock held because the slots</code></span>
<span class="codeline" id="line-1437"><code>	// related to this span will only ever be read or modified by</code></span>
<span class="codeline" id="line-1438"><code>	// this thread until pointers into the span are published (and</code></span>
<span class="codeline" id="line-1439"><code>	// we execute a publication barrier at the end of this function</code></span>
<span class="codeline" id="line-1440"><code>	// before that happens) or pageInUse is updated.</code></span>
<span class="codeline" id="line-1441"><code>	h.setSpans(s.base(), npages, s)</code></span>
<span class="codeline" id="line-1442"><code></code></span>
<span class="codeline" id="line-1443"><code>	if !typ.manual() {</code></span>
<span class="codeline" id="line-1444"><code>		// Mark in-use span in arena page bitmap.</code></span>
<span class="codeline" id="line-1445"><code>		//</code></span>
<span class="codeline" id="line-1446"><code>		// This publishes the span to the page sweeper, so</code></span>
<span class="codeline" id="line-1447"><code>		// it's imperative that the span be completely initialized</code></span>
<span class="codeline" id="line-1448"><code>		// prior to this line.</code></span>
<span class="codeline" id="line-1449"><code>		arena, pageIdx, pageMask := pageIndexOf(s.base())</code></span>
<span class="codeline" id="line-1450"><code>		atomic.Or8(&amp;arena.pageInUse[pageIdx], pageMask)</code></span>
<span class="codeline" id="line-1451"><code></code></span>
<span class="codeline" id="line-1452"><code>		// Update related page sweeper stats.</code></span>
<span class="codeline" id="line-1453"><code>		h.pagesInUse.Add(npages)</code></span>
<span class="codeline" id="line-1454"><code>	}</code></span>
<span class="codeline" id="line-1455"><code></code></span>
<span class="codeline" id="line-1456"><code>	// Make sure the newly allocated span will be observed</code></span>
<span class="codeline" id="line-1457"><code>	// by the GC before pointers into the span are published.</code></span>
<span class="codeline" id="line-1458"><code>	publicationBarrier()</code></span>
<span class="codeline" id="line-1459"><code>}</code></span>
<span class="codeline" id="line-1460"><code></code></span>
<span class="codeline" id="line-1461"><code>// Try to add at least npage pages of memory to the heap,</code></span>
<span class="codeline" id="line-1462"><code>// returning how much the heap grew by and whether it worked.</code></span>
<span class="codeline" id="line-1463"><code>//</code></span>
<span class="codeline" id="line-1464"><code>// h.lock must be held.</code></span>
<span class="codeline" id="line-1465"><code>func (h *mheap) grow(npage uintptr) (uintptr, bool) {</code></span>
<span class="codeline" id="line-1466"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1467"><code></code></span>
<span class="codeline" id="line-1468"><code>	// We must grow the heap in whole palloc chunks.</code></span>
<span class="codeline" id="line-1469"><code>	// We call sysMap below but note that because we</code></span>
<span class="codeline" id="line-1470"><code>	// round up to pallocChunkPages which is on the order</code></span>
<span class="codeline" id="line-1471"><code>	// of MiB (generally &gt;= to the huge page size) we</code></span>
<span class="codeline" id="line-1472"><code>	// won't be calling it too much.</code></span>
<span class="codeline" id="line-1473"><code>	ask := alignUp(npage, pallocChunkPages) * pageSize</code></span>
<span class="codeline" id="line-1474"><code></code></span>
<span class="codeline" id="line-1475"><code>	totalGrowth := uintptr(0)</code></span>
<span class="codeline" id="line-1476"><code>	// This may overflow because ask could be very large</code></span>
<span class="codeline" id="line-1477"><code>	// and is otherwise unrelated to h.curArena.base.</code></span>
<span class="codeline" id="line-1478"><code>	end := h.curArena.base + ask</code></span>
<span class="codeline" id="line-1479"><code>	nBase := alignUp(end, physPageSize)</code></span>
<span class="codeline" id="line-1480"><code>	if nBase &gt; h.curArena.end || /* overflow */ end &lt; h.curArena.base {</code></span>
<span class="codeline" id="line-1481"><code>		// Not enough room in the current arena. Allocate more</code></span>
<span class="codeline" id="line-1482"><code>		// arena space. This may not be contiguous with the</code></span>
<span class="codeline" id="line-1483"><code>		// current arena, so we have to request the full ask.</code></span>
<span class="codeline" id="line-1484"><code>		av, asize := h.sysAlloc(ask, &amp;h.arenaHints, true)</code></span>
<span class="codeline" id="line-1485"><code>		if av == nil {</code></span>
<span class="codeline" id="line-1486"><code>			inUse := gcController.heapFree.load() + gcController.heapReleased.load() + gcController.heapInUse.load()</code></span>
<span class="codeline" id="line-1487"><code>			print("runtime: out of memory: cannot allocate ", ask, "-byte block (", inUse, " in use)\n")</code></span>
<span class="codeline" id="line-1488"><code>			return 0, false</code></span>
<span class="codeline" id="line-1489"><code>		}</code></span>
<span class="codeline" id="line-1490"><code></code></span>
<span class="codeline" id="line-1491"><code>		if uintptr(av) == h.curArena.end {</code></span>
<span class="codeline" id="line-1492"><code>			// The new space is contiguous with the old</code></span>
<span class="codeline" id="line-1493"><code>			// space, so just extend the current space.</code></span>
<span class="codeline" id="line-1494"><code>			h.curArena.end = uintptr(av) + asize</code></span>
<span class="codeline" id="line-1495"><code>		} else {</code></span>
<span class="codeline" id="line-1496"><code>			// The new space is discontiguous. Track what</code></span>
<span class="codeline" id="line-1497"><code>			// remains of the current space and switch to</code></span>
<span class="codeline" id="line-1498"><code>			// the new space. This should be rare.</code></span>
<span class="codeline" id="line-1499"><code>			if size := h.curArena.end - h.curArena.base; size != 0 {</code></span>
<span class="codeline" id="line-1500"><code>				// Transition this space from Reserved to Prepared and mark it</code></span>
<span class="codeline" id="line-1501"><code>				// as released since we'll be able to start using it after updating</code></span>
<span class="codeline" id="line-1502"><code>				// the page allocator and releasing the lock at any time.</code></span>
<span class="codeline" id="line-1503"><code>				sysMap(unsafe.Pointer(h.curArena.base), size, &amp;gcController.heapReleased)</code></span>
<span class="codeline" id="line-1504"><code>				// Update stats.</code></span>
<span class="codeline" id="line-1505"><code>				stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-1506"><code>				atomic.Xaddint64(&amp;stats.released, int64(size))</code></span>
<span class="codeline" id="line-1507"><code>				memstats.heapStats.release()</code></span>
<span class="codeline" id="line-1508"><code>				// Update the page allocator's structures to make this</code></span>
<span class="codeline" id="line-1509"><code>				// space ready for allocation.</code></span>
<span class="codeline" id="line-1510"><code>				h.pages.grow(h.curArena.base, size)</code></span>
<span class="codeline" id="line-1511"><code>				totalGrowth += size</code></span>
<span class="codeline" id="line-1512"><code>			}</code></span>
<span class="codeline" id="line-1513"><code>			// Switch to the new space.</code></span>
<span class="codeline" id="line-1514"><code>			h.curArena.base = uintptr(av)</code></span>
<span class="codeline" id="line-1515"><code>			h.curArena.end = uintptr(av) + asize</code></span>
<span class="codeline" id="line-1516"><code>		}</code></span>
<span class="codeline" id="line-1517"><code></code></span>
<span class="codeline" id="line-1518"><code>		// Recalculate nBase.</code></span>
<span class="codeline" id="line-1519"><code>		// We know this won't overflow, because sysAlloc returned</code></span>
<span class="codeline" id="line-1520"><code>		// a valid region starting at h.curArena.base which is at</code></span>
<span class="codeline" id="line-1521"><code>		// least ask bytes in size.</code></span>
<span class="codeline" id="line-1522"><code>		nBase = alignUp(h.curArena.base+ask, physPageSize)</code></span>
<span class="codeline" id="line-1523"><code>	}</code></span>
<span class="codeline" id="line-1524"><code></code></span>
<span class="codeline" id="line-1525"><code>	// Grow into the current arena.</code></span>
<span class="codeline" id="line-1526"><code>	v := h.curArena.base</code></span>
<span class="codeline" id="line-1527"><code>	h.curArena.base = nBase</code></span>
<span class="codeline" id="line-1528"><code></code></span>
<span class="codeline" id="line-1529"><code>	// Transition the space we're going to use from Reserved to Prepared.</code></span>
<span class="codeline" id="line-1530"><code>	//</code></span>
<span class="codeline" id="line-1531"><code>	// The allocation is always aligned to the heap arena</code></span>
<span class="codeline" id="line-1532"><code>	// size which is always &gt; physPageSize, so its safe to</code></span>
<span class="codeline" id="line-1533"><code>	// just add directly to heapReleased.</code></span>
<span class="codeline" id="line-1534"><code>	sysMap(unsafe.Pointer(v), nBase-v, &amp;gcController.heapReleased)</code></span>
<span class="codeline" id="line-1535"><code></code></span>
<span class="codeline" id="line-1536"><code>	// The memory just allocated counts as both released</code></span>
<span class="codeline" id="line-1537"><code>	// and idle, even though it's not yet backed by spans.</code></span>
<span class="codeline" id="line-1538"><code>	stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-1539"><code>	atomic.Xaddint64(&amp;stats.released, int64(nBase-v))</code></span>
<span class="codeline" id="line-1540"><code>	memstats.heapStats.release()</code></span>
<span class="codeline" id="line-1541"><code></code></span>
<span class="codeline" id="line-1542"><code>	// Update the page allocator's structures to make this</code></span>
<span class="codeline" id="line-1543"><code>	// space ready for allocation.</code></span>
<span class="codeline" id="line-1544"><code>	h.pages.grow(v, nBase-v)</code></span>
<span class="codeline" id="line-1545"><code>	totalGrowth += nBase - v</code></span>
<span class="codeline" id="line-1546"><code>	return totalGrowth, true</code></span>
<span class="codeline" id="line-1547"><code>}</code></span>
<span class="codeline" id="line-1548"><code></code></span>
<span class="codeline" id="line-1549"><code>// Free the span back into the heap.</code></span>
<span class="codeline" id="line-1550"><code>func (h *mheap) freeSpan(s *mspan) {</code></span>
<span class="codeline" id="line-1551"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-1552"><code>		pageTraceFree(getg().m.p.ptr(), 0, s.base(), s.npages)</code></span>
<span class="codeline" id="line-1553"><code></code></span>
<span class="codeline" id="line-1554"><code>		lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1555"><code>		if msanenabled {</code></span>
<span class="codeline" id="line-1556"><code>			// Tell msan that this entire span is no longer in use.</code></span>
<span class="codeline" id="line-1557"><code>			base := unsafe.Pointer(s.base())</code></span>
<span class="codeline" id="line-1558"><code>			bytes := s.npages &lt;&lt; _PageShift</code></span>
<span class="codeline" id="line-1559"><code>			msanfree(base, bytes)</code></span>
<span class="codeline" id="line-1560"><code>		}</code></span>
<span class="codeline" id="line-1561"><code>		if asanenabled {</code></span>
<span class="codeline" id="line-1562"><code>			// Tell asan that this entire span is no longer in use.</code></span>
<span class="codeline" id="line-1563"><code>			base := unsafe.Pointer(s.base())</code></span>
<span class="codeline" id="line-1564"><code>			bytes := s.npages &lt;&lt; _PageShift</code></span>
<span class="codeline" id="line-1565"><code>			asanpoison(base, bytes)</code></span>
<span class="codeline" id="line-1566"><code>		}</code></span>
<span class="codeline" id="line-1567"><code>		h.freeSpanLocked(s, spanAllocHeap)</code></span>
<span class="codeline" id="line-1568"><code>		unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1569"><code>	})</code></span>
<span class="codeline" id="line-1570"><code>}</code></span>
<span class="codeline" id="line-1571"><code></code></span>
<span class="codeline" id="line-1572"><code>// freeManual frees a manually-managed span returned by allocManual.</code></span>
<span class="codeline" id="line-1573"><code>// typ must be the same as the spanAllocType passed to the allocManual that</code></span>
<span class="codeline" id="line-1574"><code>// allocated s.</code></span>
<span class="codeline" id="line-1575"><code>//</code></span>
<span class="codeline" id="line-1576"><code>// This must only be called when gcphase == _GCoff. See mSpanState for</code></span>
<span class="codeline" id="line-1577"><code>// an explanation.</code></span>
<span class="codeline" id="line-1578"><code>//</code></span>
<span class="codeline" id="line-1579"><code>// freeManual must be called on the system stack because it acquires</code></span>
<span class="codeline" id="line-1580"><code>// the heap lock. See mheap for details.</code></span>
<span class="codeline" id="line-1581"><code>//</code></span>
<span class="codeline" id="line-1582"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1583"><code>func (h *mheap) freeManual(s *mspan, typ spanAllocType) {</code></span>
<span class="codeline" id="line-1584"><code>	pageTraceFree(getg().m.p.ptr(), 0, s.base(), s.npages)</code></span>
<span class="codeline" id="line-1585"><code></code></span>
<span class="codeline" id="line-1586"><code>	s.needzero = 1</code></span>
<span class="codeline" id="line-1587"><code>	lock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1588"><code>	h.freeSpanLocked(s, typ)</code></span>
<span class="codeline" id="line-1589"><code>	unlock(&amp;h.lock)</code></span>
<span class="codeline" id="line-1590"><code>}</code></span>
<span class="codeline" id="line-1591"><code></code></span>
<span class="codeline" id="line-1592"><code>func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType) {</code></span>
<span class="codeline" id="line-1593"><code>	assertLockHeld(&amp;h.lock)</code></span>
<span class="codeline" id="line-1594"><code></code></span>
<span class="codeline" id="line-1595"><code>	switch s.state.get() {</code></span>
<span class="codeline" id="line-1596"><code>	case mSpanManual:</code></span>
<span class="codeline" id="line-1597"><code>		if s.allocCount != 0 {</code></span>
<span class="codeline" id="line-1598"><code>			throw("mheap.freeSpanLocked - invalid stack free")</code></span>
<span class="codeline" id="line-1599"><code>		}</code></span>
<span class="codeline" id="line-1600"><code>	case mSpanInUse:</code></span>
<span class="codeline" id="line-1601"><code>		if s.isUserArenaChunk {</code></span>
<span class="codeline" id="line-1602"><code>			throw("mheap.freeSpanLocked - invalid free of user arena chunk")</code></span>
<span class="codeline" id="line-1603"><code>		}</code></span>
<span class="codeline" id="line-1604"><code>		if s.allocCount != 0 || s.sweepgen != h.sweepgen {</code></span>
<span class="codeline" id="line-1605"><code>			print("mheap.freeSpanLocked - span ", s, " ptr ", hex(s.base()), " allocCount ", s.allocCount, " sweepgen ", s.sweepgen, "/", h.sweepgen, "\n")</code></span>
<span class="codeline" id="line-1606"><code>			throw("mheap.freeSpanLocked - invalid free")</code></span>
<span class="codeline" id="line-1607"><code>		}</code></span>
<span class="codeline" id="line-1608"><code>		h.pagesInUse.Add(-s.npages)</code></span>
<span class="codeline" id="line-1609"><code></code></span>
<span class="codeline" id="line-1610"><code>		// Clear in-use bit in arena page bitmap.</code></span>
<span class="codeline" id="line-1611"><code>		arena, pageIdx, pageMask := pageIndexOf(s.base())</code></span>
<span class="codeline" id="line-1612"><code>		atomic.And8(&amp;arena.pageInUse[pageIdx], ^pageMask)</code></span>
<span class="codeline" id="line-1613"><code>	default:</code></span>
<span class="codeline" id="line-1614"><code>		throw("mheap.freeSpanLocked - invalid span state")</code></span>
<span class="codeline" id="line-1615"><code>	}</code></span>
<span class="codeline" id="line-1616"><code></code></span>
<span class="codeline" id="line-1617"><code>	// Update stats.</code></span>
<span class="codeline" id="line-1618"><code>	//</code></span>
<span class="codeline" id="line-1619"><code>	// Mirrors the code in allocSpan.</code></span>
<span class="codeline" id="line-1620"><code>	nbytes := s.npages * pageSize</code></span>
<span class="codeline" id="line-1621"><code>	gcController.heapFree.add(int64(nbytes))</code></span>
<span class="codeline" id="line-1622"><code>	if typ == spanAllocHeap {</code></span>
<span class="codeline" id="line-1623"><code>		gcController.heapInUse.add(-int64(nbytes))</code></span>
<span class="codeline" id="line-1624"><code>	}</code></span>
<span class="codeline" id="line-1625"><code>	// Update consistent stats.</code></span>
<span class="codeline" id="line-1626"><code>	stats := memstats.heapStats.acquire()</code></span>
<span class="codeline" id="line-1627"><code>	switch typ {</code></span>
<span class="codeline" id="line-1628"><code>	case spanAllocHeap:</code></span>
<span class="codeline" id="line-1629"><code>		atomic.Xaddint64(&amp;stats.inHeap, -int64(nbytes))</code></span>
<span class="codeline" id="line-1630"><code>	case spanAllocStack:</code></span>
<span class="codeline" id="line-1631"><code>		atomic.Xaddint64(&amp;stats.inStacks, -int64(nbytes))</code></span>
<span class="codeline" id="line-1632"><code>	case spanAllocPtrScalarBits:</code></span>
<span class="codeline" id="line-1633"><code>		atomic.Xaddint64(&amp;stats.inPtrScalarBits, -int64(nbytes))</code></span>
<span class="codeline" id="line-1634"><code>	case spanAllocWorkBuf:</code></span>
<span class="codeline" id="line-1635"><code>		atomic.Xaddint64(&amp;stats.inWorkBufs, -int64(nbytes))</code></span>
<span class="codeline" id="line-1636"><code>	}</code></span>
<span class="codeline" id="line-1637"><code>	memstats.heapStats.release()</code></span>
<span class="codeline" id="line-1638"><code></code></span>
<span class="codeline" id="line-1639"><code>	// Mark the space as free.</code></span>
<span class="codeline" id="line-1640"><code>	h.pages.free(s.base(), s.npages)</code></span>
<span class="codeline" id="line-1641"><code></code></span>
<span class="codeline" id="line-1642"><code>	// Free the span structure. We no longer have a use for it.</code></span>
<span class="codeline" id="line-1643"><code>	s.state.set(mSpanDead)</code></span>
<span class="codeline" id="line-1644"><code>	h.freeMSpanLocked(s)</code></span>
<span class="codeline" id="line-1645"><code>}</code></span>
<span class="codeline" id="line-1646"><code></code></span>
<span class="codeline" id="line-1647"><code>// scavengeAll acquires the heap lock (blocking any additional</code></span>
<span class="codeline" id="line-1648"><code>// manipulation of the page allocator) and iterates over the whole</code></span>
<span class="codeline" id="line-1649"><code>// heap, scavenging every free page available.</code></span>
<span class="codeline" id="line-1650"><code>//</code></span>
<span class="codeline" id="line-1651"><code>// Must run on the system stack because it acquires the heap lock.</code></span>
<span class="codeline" id="line-1652"><code>//</code></span>
<span class="codeline" id="line-1653"><code>//go:systemstack</code></span>
<span class="codeline" id="line-1654"><code>func (h *mheap) scavengeAll() {</code></span>
<span class="codeline" id="line-1655"><code>	// Disallow malloc or panic while holding the heap lock. We do</code></span>
<span class="codeline" id="line-1656"><code>	// this here because this is a non-mallocgc entry-point to</code></span>
<span class="codeline" id="line-1657"><code>	// the mheap API.</code></span>
<span class="codeline" id="line-1658"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1659"><code>	gp.m.mallocing++</code></span>
<span class="codeline" id="line-1660"><code></code></span>
<span class="codeline" id="line-1661"><code>	// Force scavenge everything.</code></span>
<span class="codeline" id="line-1662"><code>	released := h.pages.scavenge(^uintptr(0), nil, true)</code></span>
<span class="codeline" id="line-1663"><code></code></span>
<span class="codeline" id="line-1664"><code>	gp.m.mallocing--</code></span>
<span class="codeline" id="line-1665"><code></code></span>
<span class="codeline" id="line-1666"><code>	if debug.scavtrace &gt; 0 {</code></span>
<span class="codeline" id="line-1667"><code>		printScavTrace(0, released, true)</code></span>
<span class="codeline" id="line-1668"><code>	}</code></span>
<span class="codeline" id="line-1669"><code>}</code></span>
<span class="codeline" id="line-1670"><code></code></span>
<span class="codeline" id="line-1671"><code>//go:linkname runtime_debug_freeOSMemory runtime/debug.freeOSMemory</code></span>
<span class="codeline" id="line-1672"><code>func runtime_debug_freeOSMemory() {</code></span>
<span class="codeline" id="line-1673"><code>	GC()</code></span>
<span class="codeline" id="line-1674"><code>	systemstack(func() { mheap_.scavengeAll() })</code></span>
<span class="codeline" id="line-1675"><code>}</code></span>
<span class="codeline" id="line-1676"><code></code></span>
<span class="codeline" id="line-1677"><code>// Initialize a new span with the given start and npages.</code></span>
<span class="codeline" id="line-1678"><code>func (span *mspan) init(base uintptr, npages uintptr) {</code></span>
<span class="codeline" id="line-1679"><code>	// span is *not* zeroed.</code></span>
<span class="codeline" id="line-1680"><code>	span.next = nil</code></span>
<span class="codeline" id="line-1681"><code>	span.prev = nil</code></span>
<span class="codeline" id="line-1682"><code>	span.list = nil</code></span>
<span class="codeline" id="line-1683"><code>	span.startAddr = base</code></span>
<span class="codeline" id="line-1684"><code>	span.npages = npages</code></span>
<span class="codeline" id="line-1685"><code>	span.allocCount = 0</code></span>
<span class="codeline" id="line-1686"><code>	span.spanclass = 0</code></span>
<span class="codeline" id="line-1687"><code>	span.elemsize = 0</code></span>
<span class="codeline" id="line-1688"><code>	span.speciallock.key = 0</code></span>
<span class="codeline" id="line-1689"><code>	span.specials = nil</code></span>
<span class="codeline" id="line-1690"><code>	span.needzero = 0</code></span>
<span class="codeline" id="line-1691"><code>	span.freeindex = 0</code></span>
<span class="codeline" id="line-1692"><code>	span.freeIndexForScan = 0</code></span>
<span class="codeline" id="line-1693"><code>	span.allocBits = nil</code></span>
<span class="codeline" id="line-1694"><code>	span.gcmarkBits = nil</code></span>
<span class="codeline" id="line-1695"><code>	span.pinnerBits = nil</code></span>
<span class="codeline" id="line-1696"><code>	span.state.set(mSpanDead)</code></span>
<span class="codeline" id="line-1697"><code>	lockInit(&amp;span.speciallock, lockRankMspanSpecial)</code></span>
<span class="codeline" id="line-1698"><code>}</code></span>
<span class="codeline" id="line-1699"><code></code></span>
<span class="codeline" id="line-1700"><code>func (span *mspan) inList() bool {</code></span>
<span class="codeline" id="line-1701"><code>	return span.list != nil</code></span>
<span class="codeline" id="line-1702"><code>}</code></span>
<span class="codeline" id="line-1703"><code></code></span>
<span class="codeline" id="line-1704"><code>// Initialize an empty doubly-linked list.</code></span>
<span class="codeline" id="line-1705"><code>func (list *mSpanList) init() {</code></span>
<span class="codeline" id="line-1706"><code>	list.first = nil</code></span>
<span class="codeline" id="line-1707"><code>	list.last = nil</code></span>
<span class="codeline" id="line-1708"><code>}</code></span>
<span class="codeline" id="line-1709"><code></code></span>
<span class="codeline" id="line-1710"><code>func (list *mSpanList) remove(span *mspan) {</code></span>
<span class="codeline" id="line-1711"><code>	if span.list != list {</code></span>
<span class="codeline" id="line-1712"><code>		print("runtime: failed mSpanList.remove span.npages=", span.npages,</code></span>
<span class="codeline" id="line-1713"><code>			" span=", span, " prev=", span.prev, " span.list=", span.list, " list=", list, "\n")</code></span>
<span class="codeline" id="line-1714"><code>		throw("mSpanList.remove")</code></span>
<span class="codeline" id="line-1715"><code>	}</code></span>
<span class="codeline" id="line-1716"><code>	if list.first == span {</code></span>
<span class="codeline" id="line-1717"><code>		list.first = span.next</code></span>
<span class="codeline" id="line-1718"><code>	} else {</code></span>
<span class="codeline" id="line-1719"><code>		span.prev.next = span.next</code></span>
<span class="codeline" id="line-1720"><code>	}</code></span>
<span class="codeline" id="line-1721"><code>	if list.last == span {</code></span>
<span class="codeline" id="line-1722"><code>		list.last = span.prev</code></span>
<span class="codeline" id="line-1723"><code>	} else {</code></span>
<span class="codeline" id="line-1724"><code>		span.next.prev = span.prev</code></span>
<span class="codeline" id="line-1725"><code>	}</code></span>
<span class="codeline" id="line-1726"><code>	span.next = nil</code></span>
<span class="codeline" id="line-1727"><code>	span.prev = nil</code></span>
<span class="codeline" id="line-1728"><code>	span.list = nil</code></span>
<span class="codeline" id="line-1729"><code>}</code></span>
<span class="codeline" id="line-1730"><code></code></span>
<span class="codeline" id="line-1731"><code>func (list *mSpanList) isEmpty() bool {</code></span>
<span class="codeline" id="line-1732"><code>	return list.first == nil</code></span>
<span class="codeline" id="line-1733"><code>}</code></span>
<span class="codeline" id="line-1734"><code></code></span>
<span class="codeline" id="line-1735"><code>func (list *mSpanList) insert(span *mspan) {</code></span>
<span class="codeline" id="line-1736"><code>	if span.next != nil || span.prev != nil || span.list != nil {</code></span>
<span class="codeline" id="line-1737"><code>		println("runtime: failed mSpanList.insert", span, span.next, span.prev, span.list)</code></span>
<span class="codeline" id="line-1738"><code>		throw("mSpanList.insert")</code></span>
<span class="codeline" id="line-1739"><code>	}</code></span>
<span class="codeline" id="line-1740"><code>	span.next = list.first</code></span>
<span class="codeline" id="line-1741"><code>	if list.first != nil {</code></span>
<span class="codeline" id="line-1742"><code>		// The list contains at least one span; link it in.</code></span>
<span class="codeline" id="line-1743"><code>		// The last span in the list doesn't change.</code></span>
<span class="codeline" id="line-1744"><code>		list.first.prev = span</code></span>
<span class="codeline" id="line-1745"><code>	} else {</code></span>
<span class="codeline" id="line-1746"><code>		// The list contains no spans, so this is also the last span.</code></span>
<span class="codeline" id="line-1747"><code>		list.last = span</code></span>
<span class="codeline" id="line-1748"><code>	}</code></span>
<span class="codeline" id="line-1749"><code>	list.first = span</code></span>
<span class="codeline" id="line-1750"><code>	span.list = list</code></span>
<span class="codeline" id="line-1751"><code>}</code></span>
<span class="codeline" id="line-1752"><code></code></span>
<span class="codeline" id="line-1753"><code>func (list *mSpanList) insertBack(span *mspan) {</code></span>
<span class="codeline" id="line-1754"><code>	if span.next != nil || span.prev != nil || span.list != nil {</code></span>
<span class="codeline" id="line-1755"><code>		println("runtime: failed mSpanList.insertBack", span, span.next, span.prev, span.list)</code></span>
<span class="codeline" id="line-1756"><code>		throw("mSpanList.insertBack")</code></span>
<span class="codeline" id="line-1757"><code>	}</code></span>
<span class="codeline" id="line-1758"><code>	span.prev = list.last</code></span>
<span class="codeline" id="line-1759"><code>	if list.last != nil {</code></span>
<span class="codeline" id="line-1760"><code>		// The list contains at least one span.</code></span>
<span class="codeline" id="line-1761"><code>		list.last.next = span</code></span>
<span class="codeline" id="line-1762"><code>	} else {</code></span>
<span class="codeline" id="line-1763"><code>		// The list contains no spans, so this is also the first span.</code></span>
<span class="codeline" id="line-1764"><code>		list.first = span</code></span>
<span class="codeline" id="line-1765"><code>	}</code></span>
<span class="codeline" id="line-1766"><code>	list.last = span</code></span>
<span class="codeline" id="line-1767"><code>	span.list = list</code></span>
<span class="codeline" id="line-1768"><code>}</code></span>
<span class="codeline" id="line-1769"><code></code></span>
<span class="codeline" id="line-1770"><code>// takeAll removes all spans from other and inserts them at the front</code></span>
<span class="codeline" id="line-1771"><code>// of list.</code></span>
<span class="codeline" id="line-1772"><code>func (list *mSpanList) takeAll(other *mSpanList) {</code></span>
<span class="codeline" id="line-1773"><code>	if other.isEmpty() {</code></span>
<span class="codeline" id="line-1774"><code>		return</code></span>
<span class="codeline" id="line-1775"><code>	}</code></span>
<span class="codeline" id="line-1776"><code></code></span>
<span class="codeline" id="line-1777"><code>	// Reparent everything in other to list.</code></span>
<span class="codeline" id="line-1778"><code>	for s := other.first; s != nil; s = s.next {</code></span>
<span class="codeline" id="line-1779"><code>		s.list = list</code></span>
<span class="codeline" id="line-1780"><code>	}</code></span>
<span class="codeline" id="line-1781"><code></code></span>
<span class="codeline" id="line-1782"><code>	// Concatenate the lists.</code></span>
<span class="codeline" id="line-1783"><code>	if list.isEmpty() {</code></span>
<span class="codeline" id="line-1784"><code>		*list = *other</code></span>
<span class="codeline" id="line-1785"><code>	} else {</code></span>
<span class="codeline" id="line-1786"><code>		// Neither list is empty. Put other before list.</code></span>
<span class="codeline" id="line-1787"><code>		other.last.next = list.first</code></span>
<span class="codeline" id="line-1788"><code>		list.first.prev = other.last</code></span>
<span class="codeline" id="line-1789"><code>		list.first = other.first</code></span>
<span class="codeline" id="line-1790"><code>	}</code></span>
<span class="codeline" id="line-1791"><code></code></span>
<span class="codeline" id="line-1792"><code>	other.first, other.last = nil, nil</code></span>
<span class="codeline" id="line-1793"><code>}</code></span>
<span class="codeline" id="line-1794"><code></code></span>
<span class="codeline" id="line-1795"><code>const (</code></span>
<span class="codeline" id="line-1796"><code>	_KindSpecialFinalizer = 1</code></span>
<span class="codeline" id="line-1797"><code>	_KindSpecialProfile   = 2</code></span>
<span class="codeline" id="line-1798"><code>	// _KindSpecialReachable is a special used for tracking</code></span>
<span class="codeline" id="line-1799"><code>	// reachability during testing.</code></span>
<span class="codeline" id="line-1800"><code>	_KindSpecialReachable = 3</code></span>
<span class="codeline" id="line-1801"><code>	// _KindSpecialPinCounter is a special used for objects that are pinned</code></span>
<span class="codeline" id="line-1802"><code>	// multiple times</code></span>
<span class="codeline" id="line-1803"><code>	_KindSpecialPinCounter = 4</code></span>
<span class="codeline" id="line-1804"><code>	// Note: The finalizer special must be first because if we're freeing</code></span>
<span class="codeline" id="line-1805"><code>	// an object, a finalizer special will cause the freeing operation</code></span>
<span class="codeline" id="line-1806"><code>	// to abort, and we want to keep the other special records around</code></span>
<span class="codeline" id="line-1807"><code>	// if that happens.</code></span>
<span class="codeline" id="line-1808"><code>)</code></span>
<span class="codeline" id="line-1809"><code></code></span>
<span class="codeline" id="line-1810"><code>type special struct {</code></span>
<span class="codeline" id="line-1811"><code>	_      sys.NotInHeap</code></span>
<span class="codeline" id="line-1812"><code>	next   *special // linked list in span</code></span>
<span class="codeline" id="line-1813"><code>	offset uint16   // span offset of object</code></span>
<span class="codeline" id="line-1814"><code>	kind   byte     // kind of special</code></span>
<span class="codeline" id="line-1815"><code>}</code></span>
<span class="codeline" id="line-1816"><code></code></span>
<span class="codeline" id="line-1817"><code>// spanHasSpecials marks a span as having specials in the arena bitmap.</code></span>
<span class="codeline" id="line-1818"><code>func spanHasSpecials(s *mspan) {</code></span>
<span class="codeline" id="line-1819"><code>	arenaPage := (s.base() / pageSize) % pagesPerArena</code></span>
<span class="codeline" id="line-1820"><code>	ai := arenaIndex(s.base())</code></span>
<span class="codeline" id="line-1821"><code>	ha := mheap_.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-1822"><code>	atomic.Or8(&amp;ha.pageSpecials[arenaPage/8], uint8(1)&lt;&lt;(arenaPage%8))</code></span>
<span class="codeline" id="line-1823"><code>}</code></span>
<span class="codeline" id="line-1824"><code></code></span>
<span class="codeline" id="line-1825"><code>// spanHasNoSpecials marks a span as having no specials in the arena bitmap.</code></span>
<span class="codeline" id="line-1826"><code>func spanHasNoSpecials(s *mspan) {</code></span>
<span class="codeline" id="line-1827"><code>	arenaPage := (s.base() / pageSize) % pagesPerArena</code></span>
<span class="codeline" id="line-1828"><code>	ai := arenaIndex(s.base())</code></span>
<span class="codeline" id="line-1829"><code>	ha := mheap_.arenas[ai.l1()][ai.l2()]</code></span>
<span class="codeline" id="line-1830"><code>	atomic.And8(&amp;ha.pageSpecials[arenaPage/8], ^(uint8(1) &lt;&lt; (arenaPage % 8)))</code></span>
<span class="codeline" id="line-1831"><code>}</code></span>
<span class="codeline" id="line-1832"><code></code></span>
<span class="codeline" id="line-1833"><code>// Adds the special record s to the list of special records for</code></span>
<span class="codeline" id="line-1834"><code>// the object p. All fields of s should be filled in except for</code></span>
<span class="codeline" id="line-1835"><code>// offset &amp; next, which this routine will fill in.</code></span>
<span class="codeline" id="line-1836"><code>// Returns true if the special was successfully added, false otherwise.</code></span>
<span class="codeline" id="line-1837"><code>// (The add will fail only if a record with the same p and s-&gt;kind</code></span>
<span class="codeline" id="line-1838"><code>// already exists.)</code></span>
<span class="codeline" id="line-1839"><code>func addspecial(p unsafe.Pointer, s *special) bool {</code></span>
<span class="codeline" id="line-1840"><code>	span := spanOfHeap(uintptr(p))</code></span>
<span class="codeline" id="line-1841"><code>	if span == nil {</code></span>
<span class="codeline" id="line-1842"><code>		throw("addspecial on invalid pointer")</code></span>
<span class="codeline" id="line-1843"><code>	}</code></span>
<span class="codeline" id="line-1844"><code></code></span>
<span class="codeline" id="line-1845"><code>	// Ensure that the span is swept.</code></span>
<span class="codeline" id="line-1846"><code>	// Sweeping accesses the specials list w/o locks, so we have</code></span>
<span class="codeline" id="line-1847"><code>	// to synchronize with it. And it's just much safer.</code></span>
<span class="codeline" id="line-1848"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-1849"><code>	span.ensureSwept()</code></span>
<span class="codeline" id="line-1850"><code></code></span>
<span class="codeline" id="line-1851"><code>	offset := uintptr(p) - span.base()</code></span>
<span class="codeline" id="line-1852"><code>	kind := s.kind</code></span>
<span class="codeline" id="line-1853"><code></code></span>
<span class="codeline" id="line-1854"><code>	lock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1855"><code></code></span>
<span class="codeline" id="line-1856"><code>	// Find splice point, check for existing record.</code></span>
<span class="codeline" id="line-1857"><code>	iter, exists := span.specialFindSplicePoint(offset, kind)</code></span>
<span class="codeline" id="line-1858"><code>	if !exists {</code></span>
<span class="codeline" id="line-1859"><code>		// Splice in record, fill in offset.</code></span>
<span class="codeline" id="line-1860"><code>		s.offset = uint16(offset)</code></span>
<span class="codeline" id="line-1861"><code>		s.next = *iter</code></span>
<span class="codeline" id="line-1862"><code>		*iter = s</code></span>
<span class="codeline" id="line-1863"><code>		spanHasSpecials(span)</code></span>
<span class="codeline" id="line-1864"><code>	}</code></span>
<span class="codeline" id="line-1865"><code></code></span>
<span class="codeline" id="line-1866"><code>	unlock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1867"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-1868"><code>	return !exists // already exists</code></span>
<span class="codeline" id="line-1869"><code>}</code></span>
<span class="codeline" id="line-1870"><code></code></span>
<span class="codeline" id="line-1871"><code>// Removes the Special record of the given kind for the object p.</code></span>
<span class="codeline" id="line-1872"><code>// Returns the record if the record existed, nil otherwise.</code></span>
<span class="codeline" id="line-1873"><code>// The caller must FixAlloc_Free the result.</code></span>
<span class="codeline" id="line-1874"><code>func removespecial(p unsafe.Pointer, kind uint8) *special {</code></span>
<span class="codeline" id="line-1875"><code>	span := spanOfHeap(uintptr(p))</code></span>
<span class="codeline" id="line-1876"><code>	if span == nil {</code></span>
<span class="codeline" id="line-1877"><code>		throw("removespecial on invalid pointer")</code></span>
<span class="codeline" id="line-1878"><code>	}</code></span>
<span class="codeline" id="line-1879"><code></code></span>
<span class="codeline" id="line-1880"><code>	// Ensure that the span is swept.</code></span>
<span class="codeline" id="line-1881"><code>	// Sweeping accesses the specials list w/o locks, so we have</code></span>
<span class="codeline" id="line-1882"><code>	// to synchronize with it. And it's just much safer.</code></span>
<span class="codeline" id="line-1883"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-1884"><code>	span.ensureSwept()</code></span>
<span class="codeline" id="line-1885"><code></code></span>
<span class="codeline" id="line-1886"><code>	offset := uintptr(p) - span.base()</code></span>
<span class="codeline" id="line-1887"><code></code></span>
<span class="codeline" id="line-1888"><code>	var result *special</code></span>
<span class="codeline" id="line-1889"><code>	lock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1890"><code></code></span>
<span class="codeline" id="line-1891"><code>	iter, exists := span.specialFindSplicePoint(offset, kind)</code></span>
<span class="codeline" id="line-1892"><code>	if exists {</code></span>
<span class="codeline" id="line-1893"><code>		s := *iter</code></span>
<span class="codeline" id="line-1894"><code>		*iter = s.next</code></span>
<span class="codeline" id="line-1895"><code>		result = s</code></span>
<span class="codeline" id="line-1896"><code>	}</code></span>
<span class="codeline" id="line-1897"><code>	if span.specials == nil {</code></span>
<span class="codeline" id="line-1898"><code>		spanHasNoSpecials(span)</code></span>
<span class="codeline" id="line-1899"><code>	}</code></span>
<span class="codeline" id="line-1900"><code>	unlock(&amp;span.speciallock)</code></span>
<span class="codeline" id="line-1901"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-1902"><code>	return result</code></span>
<span class="codeline" id="line-1903"><code>}</code></span>
<span class="codeline" id="line-1904"><code></code></span>
<span class="codeline" id="line-1905"><code>// Find a splice point in the sorted list and check for an already existing</code></span>
<span class="codeline" id="line-1906"><code>// record. Returns a pointer to the next-reference in the list predecessor.</code></span>
<span class="codeline" id="line-1907"><code>// Returns true, if the referenced item is an exact match.</code></span>
<span class="codeline" id="line-1908"><code>func (span *mspan) specialFindSplicePoint(offset uintptr, kind byte) (**special, bool) {</code></span>
<span class="codeline" id="line-1909"><code>	// Find splice point, check for existing record.</code></span>
<span class="codeline" id="line-1910"><code>	iter := &amp;span.specials</code></span>
<span class="codeline" id="line-1911"><code>	found := false</code></span>
<span class="codeline" id="line-1912"><code>	for {</code></span>
<span class="codeline" id="line-1913"><code>		s := *iter</code></span>
<span class="codeline" id="line-1914"><code>		if s == nil {</code></span>
<span class="codeline" id="line-1915"><code>			break</code></span>
<span class="codeline" id="line-1916"><code>		}</code></span>
<span class="codeline" id="line-1917"><code>		if offset == uintptr(s.offset) &amp;&amp; kind == s.kind {</code></span>
<span class="codeline" id="line-1918"><code>			found = true</code></span>
<span class="codeline" id="line-1919"><code>			break</code></span>
<span class="codeline" id="line-1920"><code>		}</code></span>
<span class="codeline" id="line-1921"><code>		if offset &lt; uintptr(s.offset) || (offset == uintptr(s.offset) &amp;&amp; kind &lt; s.kind) {</code></span>
<span class="codeline" id="line-1922"><code>			break</code></span>
<span class="codeline" id="line-1923"><code>		}</code></span>
<span class="codeline" id="line-1924"><code>		iter = &amp;s.next</code></span>
<span class="codeline" id="line-1925"><code>	}</code></span>
<span class="codeline" id="line-1926"><code>	return iter, found</code></span>
<span class="codeline" id="line-1927"><code>}</code></span>
<span class="codeline" id="line-1928"><code></code></span>
<span class="codeline" id="line-1929"><code>// The described object has a finalizer set for it.</code></span>
<span class="codeline" id="line-1930"><code>//</code></span>
<span class="codeline" id="line-1931"><code>// specialfinalizer is allocated from non-GC'd memory, so any heap</code></span>
<span class="codeline" id="line-1932"><code>// pointers must be specially handled.</code></span>
<span class="codeline" id="line-1933"><code>type specialfinalizer struct {</code></span>
<span class="codeline" id="line-1934"><code>	_       sys.NotInHeap</code></span>
<span class="codeline" id="line-1935"><code>	special special</code></span>
<span class="codeline" id="line-1936"><code>	fn      *funcval // May be a heap pointer.</code></span>
<span class="codeline" id="line-1937"><code>	nret    uintptr</code></span>
<span class="codeline" id="line-1938"><code>	fint    *_type   // May be a heap pointer, but always live.</code></span>
<span class="codeline" id="line-1939"><code>	ot      *ptrtype // May be a heap pointer, but always live.</code></span>
<span class="codeline" id="line-1940"><code>}</code></span>
<span class="codeline" id="line-1941"><code></code></span>
<span class="codeline" id="line-1942"><code>// Adds a finalizer to the object p. Returns true if it succeeded.</code></span>
<span class="codeline" id="line-1943"><code>func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool {</code></span>
<span class="codeline" id="line-1944"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1945"><code>	s := (*specialfinalizer)(mheap_.specialfinalizeralloc.alloc())</code></span>
<span class="codeline" id="line-1946"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1947"><code>	s.special.kind = _KindSpecialFinalizer</code></span>
<span class="codeline" id="line-1948"><code>	s.fn = f</code></span>
<span class="codeline" id="line-1949"><code>	s.nret = nret</code></span>
<span class="codeline" id="line-1950"><code>	s.fint = fint</code></span>
<span class="codeline" id="line-1951"><code>	s.ot = ot</code></span>
<span class="codeline" id="line-1952"><code>	if addspecial(p, &amp;s.special) {</code></span>
<span class="codeline" id="line-1953"><code>		// This is responsible for maintaining the same</code></span>
<span class="codeline" id="line-1954"><code>		// GC-related invariants as markrootSpans in any</code></span>
<span class="codeline" id="line-1955"><code>		// situation where it's possible that markrootSpans</code></span>
<span class="codeline" id="line-1956"><code>		// has already run but mark termination hasn't yet.</code></span>
<span class="codeline" id="line-1957"><code>		if gcphase != _GCoff {</code></span>
<span class="codeline" id="line-1958"><code>			base, span, _ := findObject(uintptr(p), 0, 0)</code></span>
<span class="codeline" id="line-1959"><code>			mp := acquirem()</code></span>
<span class="codeline" id="line-1960"><code>			gcw := &amp;mp.p.ptr().gcw</code></span>
<span class="codeline" id="line-1961"><code>			// Mark everything reachable from the object</code></span>
<span class="codeline" id="line-1962"><code>			// so it's retained for the finalizer.</code></span>
<span class="codeline" id="line-1963"><code>			if !span.spanclass.noscan() {</code></span>
<span class="codeline" id="line-1964"><code>				scanobject(base, gcw)</code></span>
<span class="codeline" id="line-1965"><code>			}</code></span>
<span class="codeline" id="line-1966"><code>			// Mark the finalizer itself, since the</code></span>
<span class="codeline" id="line-1967"><code>			// special isn't part of the GC'd heap.</code></span>
<span class="codeline" id="line-1968"><code>			scanblock(uintptr(unsafe.Pointer(&amp;s.fn)), goarch.PtrSize, &amp;oneptrmask[0], gcw, nil)</code></span>
<span class="codeline" id="line-1969"><code>			releasem(mp)</code></span>
<span class="codeline" id="line-1970"><code>		}</code></span>
<span class="codeline" id="line-1971"><code>		return true</code></span>
<span class="codeline" id="line-1972"><code>	}</code></span>
<span class="codeline" id="line-1973"><code></code></span>
<span class="codeline" id="line-1974"><code>	// There was an old finalizer</code></span>
<span class="codeline" id="line-1975"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1976"><code>	mheap_.specialfinalizeralloc.free(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1977"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1978"><code>	return false</code></span>
<span class="codeline" id="line-1979"><code>}</code></span>
<span class="codeline" id="line-1980"><code></code></span>
<span class="codeline" id="line-1981"><code>// Removes the finalizer (if any) from the object p.</code></span>
<span class="codeline" id="line-1982"><code>func removefinalizer(p unsafe.Pointer) {</code></span>
<span class="codeline" id="line-1983"><code>	s := (*specialfinalizer)(unsafe.Pointer(removespecial(p, _KindSpecialFinalizer)))</code></span>
<span class="codeline" id="line-1984"><code>	if s == nil {</code></span>
<span class="codeline" id="line-1985"><code>		return // there wasn't a finalizer to remove</code></span>
<span class="codeline" id="line-1986"><code>	}</code></span>
<span class="codeline" id="line-1987"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1988"><code>	mheap_.specialfinalizeralloc.free(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-1989"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-1990"><code>}</code></span>
<span class="codeline" id="line-1991"><code></code></span>
<span class="codeline" id="line-1992"><code>// The described object is being heap profiled.</code></span>
<span class="codeline" id="line-1993"><code>type specialprofile struct {</code></span>
<span class="codeline" id="line-1994"><code>	_       sys.NotInHeap</code></span>
<span class="codeline" id="line-1995"><code>	special special</code></span>
<span class="codeline" id="line-1996"><code>	b       *bucket</code></span>
<span class="codeline" id="line-1997"><code>}</code></span>
<span class="codeline" id="line-1998"><code></code></span>
<span class="codeline" id="line-1999"><code>// Set the heap profile bucket associated with addr to b.</code></span>
<span class="codeline" id="line-2000"><code>func setprofilebucket(p unsafe.Pointer, b *bucket) {</code></span>
<span class="codeline" id="line-2001"><code>	lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2002"><code>	s := (*specialprofile)(mheap_.specialprofilealloc.alloc())</code></span>
<span class="codeline" id="line-2003"><code>	unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2004"><code>	s.special.kind = _KindSpecialProfile</code></span>
<span class="codeline" id="line-2005"><code>	s.b = b</code></span>
<span class="codeline" id="line-2006"><code>	if !addspecial(p, &amp;s.special) {</code></span>
<span class="codeline" id="line-2007"><code>		throw("setprofilebucket: profile already set")</code></span>
<span class="codeline" id="line-2008"><code>	}</code></span>
<span class="codeline" id="line-2009"><code>}</code></span>
<span class="codeline" id="line-2010"><code></code></span>
<span class="codeline" id="line-2011"><code>// specialReachable tracks whether an object is reachable on the next</code></span>
<span class="codeline" id="line-2012"><code>// GC cycle. This is used by testing.</code></span>
<span class="codeline" id="line-2013"><code>type specialReachable struct {</code></span>
<span class="codeline" id="line-2014"><code>	special   special</code></span>
<span class="codeline" id="line-2015"><code>	done      bool</code></span>
<span class="codeline" id="line-2016"><code>	reachable bool</code></span>
<span class="codeline" id="line-2017"><code>}</code></span>
<span class="codeline" id="line-2018"><code></code></span>
<span class="codeline" id="line-2019"><code>// specialPinCounter tracks whether an object is pinned multiple times.</code></span>
<span class="codeline" id="line-2020"><code>type specialPinCounter struct {</code></span>
<span class="codeline" id="line-2021"><code>	special special</code></span>
<span class="codeline" id="line-2022"><code>	counter uintptr</code></span>
<span class="codeline" id="line-2023"><code>}</code></span>
<span class="codeline" id="line-2024"><code></code></span>
<span class="codeline" id="line-2025"><code>// specialsIter helps iterate over specials lists.</code></span>
<span class="codeline" id="line-2026"><code>type specialsIter struct {</code></span>
<span class="codeline" id="line-2027"><code>	pprev **special</code></span>
<span class="codeline" id="line-2028"><code>	s     *special</code></span>
<span class="codeline" id="line-2029"><code>}</code></span>
<span class="codeline" id="line-2030"><code></code></span>
<span class="codeline" id="line-2031"><code>func newSpecialsIter(span *mspan) specialsIter {</code></span>
<span class="codeline" id="line-2032"><code>	return specialsIter{&amp;span.specials, span.specials}</code></span>
<span class="codeline" id="line-2033"><code>}</code></span>
<span class="codeline" id="line-2034"><code></code></span>
<span class="codeline" id="line-2035"><code>func (i *specialsIter) valid() bool {</code></span>
<span class="codeline" id="line-2036"><code>	return i.s != nil</code></span>
<span class="codeline" id="line-2037"><code>}</code></span>
<span class="codeline" id="line-2038"><code></code></span>
<span class="codeline" id="line-2039"><code>func (i *specialsIter) next() {</code></span>
<span class="codeline" id="line-2040"><code>	i.pprev = &amp;i.s.next</code></span>
<span class="codeline" id="line-2041"><code>	i.s = *i.pprev</code></span>
<span class="codeline" id="line-2042"><code>}</code></span>
<span class="codeline" id="line-2043"><code></code></span>
<span class="codeline" id="line-2044"><code>// unlinkAndNext removes the current special from the list and moves</code></span>
<span class="codeline" id="line-2045"><code>// the iterator to the next special. It returns the unlinked special.</code></span>
<span class="codeline" id="line-2046"><code>func (i *specialsIter) unlinkAndNext() *special {</code></span>
<span class="codeline" id="line-2047"><code>	cur := i.s</code></span>
<span class="codeline" id="line-2048"><code>	i.s = cur.next</code></span>
<span class="codeline" id="line-2049"><code>	*i.pprev = i.s</code></span>
<span class="codeline" id="line-2050"><code>	return cur</code></span>
<span class="codeline" id="line-2051"><code>}</code></span>
<span class="codeline" id="line-2052"><code></code></span>
<span class="codeline" id="line-2053"><code>// freeSpecial performs any cleanup on special s and deallocates it.</code></span>
<span class="codeline" id="line-2054"><code>// s must already be unlinked from the specials list.</code></span>
<span class="codeline" id="line-2055"><code>func freeSpecial(s *special, p unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-2056"><code>	switch s.kind {</code></span>
<span class="codeline" id="line-2057"><code>	case _KindSpecialFinalizer:</code></span>
<span class="codeline" id="line-2058"><code>		sf := (*specialfinalizer)(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-2059"><code>		queuefinalizer(p, sf.fn, sf.nret, sf.fint, sf.ot)</code></span>
<span class="codeline" id="line-2060"><code>		lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2061"><code>		mheap_.specialfinalizeralloc.free(unsafe.Pointer(sf))</code></span>
<span class="codeline" id="line-2062"><code>		unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2063"><code>	case _KindSpecialProfile:</code></span>
<span class="codeline" id="line-2064"><code>		sp := (*specialprofile)(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-2065"><code>		mProf_Free(sp.b, size)</code></span>
<span class="codeline" id="line-2066"><code>		lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2067"><code>		mheap_.specialprofilealloc.free(unsafe.Pointer(sp))</code></span>
<span class="codeline" id="line-2068"><code>		unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2069"><code>	case _KindSpecialReachable:</code></span>
<span class="codeline" id="line-2070"><code>		sp := (*specialReachable)(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-2071"><code>		sp.done = true</code></span>
<span class="codeline" id="line-2072"><code>		// The creator frees these.</code></span>
<span class="codeline" id="line-2073"><code>	case _KindSpecialPinCounter:</code></span>
<span class="codeline" id="line-2074"><code>		lock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2075"><code>		mheap_.specialPinCounterAlloc.free(unsafe.Pointer(s))</code></span>
<span class="codeline" id="line-2076"><code>		unlock(&amp;mheap_.speciallock)</code></span>
<span class="codeline" id="line-2077"><code>	default:</code></span>
<span class="codeline" id="line-2078"><code>		throw("bad special kind")</code></span>
<span class="codeline" id="line-2079"><code>		panic("not reached")</code></span>
<span class="codeline" id="line-2080"><code>	}</code></span>
<span class="codeline" id="line-2081"><code>}</code></span>
<span class="codeline" id="line-2082"><code></code></span>
<span class="codeline" id="line-2083"><code>// gcBits is an alloc/mark bitmap. This is always used as gcBits.x.</code></span>
<span class="codeline" id="line-2084"><code>type gcBits struct {</code></span>
<span class="codeline" id="line-2085"><code>	_ sys.NotInHeap</code></span>
<span class="codeline" id="line-2086"><code>	x uint8</code></span>
<span class="codeline" id="line-2087"><code>}</code></span>
<span class="codeline" id="line-2088"><code></code></span>
<span class="codeline" id="line-2089"><code>// bytep returns a pointer to the n'th byte of b.</code></span>
<span class="codeline" id="line-2090"><code>func (b *gcBits) bytep(n uintptr) *uint8 {</code></span>
<span class="codeline" id="line-2091"><code>	return addb(&amp;b.x, n)</code></span>
<span class="codeline" id="line-2092"><code>}</code></span>
<span class="codeline" id="line-2093"><code></code></span>
<span class="codeline" id="line-2094"><code>// bitp returns a pointer to the byte containing bit n and a mask for</code></span>
<span class="codeline" id="line-2095"><code>// selecting that bit from *bytep.</code></span>
<span class="codeline" id="line-2096"><code>func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8) {</code></span>
<span class="codeline" id="line-2097"><code>	return b.bytep(n / 8), 1 &lt;&lt; (n % 8)</code></span>
<span class="codeline" id="line-2098"><code>}</code></span>
<span class="codeline" id="line-2099"><code></code></span>
<span class="codeline" id="line-2100"><code>const gcBitsChunkBytes = uintptr(64 &lt;&lt; 10)</code></span>
<span class="codeline" id="line-2101"><code>const gcBitsHeaderBytes = unsafe.Sizeof(gcBitsHeader{})</code></span>
<span class="codeline" id="line-2102"><code></code></span>
<span class="codeline" id="line-2103"><code>type gcBitsHeader struct {</code></span>
<span class="codeline" id="line-2104"><code>	free uintptr // free is the index into bits of the next free byte.</code></span>
<span class="codeline" id="line-2105"><code>	next uintptr // *gcBits triggers recursive type bug. (issue 14620)</code></span>
<span class="codeline" id="line-2106"><code>}</code></span>
<span class="codeline" id="line-2107"><code></code></span>
<span class="codeline" id="line-2108"><code>type gcBitsArena struct {</code></span>
<span class="codeline" id="line-2109"><code>	_ sys.NotInHeap</code></span>
<span class="codeline" id="line-2110"><code>	// gcBitsHeader // side step recursive type bug (issue 14620) by including fields by hand.</code></span>
<span class="codeline" id="line-2111"><code>	free uintptr // free is the index into bits of the next free byte; read/write atomically</code></span>
<span class="codeline" id="line-2112"><code>	next *gcBitsArena</code></span>
<span class="codeline" id="line-2113"><code>	bits [gcBitsChunkBytes - gcBitsHeaderBytes]gcBits</code></span>
<span class="codeline" id="line-2114"><code>}</code></span>
<span class="codeline" id="line-2115"><code></code></span>
<span class="codeline" id="line-2116"><code>var gcBitsArenas struct {</code></span>
<span class="codeline" id="line-2117"><code>	lock     mutex</code></span>
<span class="codeline" id="line-2118"><code>	free     *gcBitsArena</code></span>
<span class="codeline" id="line-2119"><code>	next     *gcBitsArena // Read atomically. Write atomically under lock.</code></span>
<span class="codeline" id="line-2120"><code>	current  *gcBitsArena</code></span>
<span class="codeline" id="line-2121"><code>	previous *gcBitsArena</code></span>
<span class="codeline" id="line-2122"><code>}</code></span>
<span class="codeline" id="line-2123"><code></code></span>
<span class="codeline" id="line-2124"><code>// tryAlloc allocates from b or returns nil if b does not have enough room.</code></span>
<span class="codeline" id="line-2125"><code>// This is safe to call concurrently.</code></span>
<span class="codeline" id="line-2126"><code>func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits {</code></span>
<span class="codeline" id="line-2127"><code>	if b == nil || atomic.Loaduintptr(&amp;b.free)+bytes &gt; uintptr(len(b.bits)) {</code></span>
<span class="codeline" id="line-2128"><code>		return nil</code></span>
<span class="codeline" id="line-2129"><code>	}</code></span>
<span class="codeline" id="line-2130"><code>	// Try to allocate from this block.</code></span>
<span class="codeline" id="line-2131"><code>	end := atomic.Xadduintptr(&amp;b.free, bytes)</code></span>
<span class="codeline" id="line-2132"><code>	if end &gt; uintptr(len(b.bits)) {</code></span>
<span class="codeline" id="line-2133"><code>		return nil</code></span>
<span class="codeline" id="line-2134"><code>	}</code></span>
<span class="codeline" id="line-2135"><code>	// There was enough room.</code></span>
<span class="codeline" id="line-2136"><code>	start := end - bytes</code></span>
<span class="codeline" id="line-2137"><code>	return &amp;b.bits[start]</code></span>
<span class="codeline" id="line-2138"><code>}</code></span>
<span class="codeline" id="line-2139"><code></code></span>
<span class="codeline" id="line-2140"><code>// newMarkBits returns a pointer to 8 byte aligned bytes</code></span>
<span class="codeline" id="line-2141"><code>// to be used for a span's mark bits.</code></span>
<span class="codeline" id="line-2142"><code>func newMarkBits(nelems uintptr) *gcBits {</code></span>
<span class="codeline" id="line-2143"><code>	blocksNeeded := (nelems + 63) / 64</code></span>
<span class="codeline" id="line-2144"><code>	bytesNeeded := blocksNeeded * 8</code></span>
<span class="codeline" id="line-2145"><code></code></span>
<span class="codeline" id="line-2146"><code>	// Try directly allocating from the current head arena.</code></span>
<span class="codeline" id="line-2147"><code>	head := (*gcBitsArena)(atomic.Loadp(unsafe.Pointer(&amp;gcBitsArenas.next)))</code></span>
<span class="codeline" id="line-2148"><code>	if p := head.tryAlloc(bytesNeeded); p != nil {</code></span>
<span class="codeline" id="line-2149"><code>		return p</code></span>
<span class="codeline" id="line-2150"><code>	}</code></span>
<span class="codeline" id="line-2151"><code></code></span>
<span class="codeline" id="line-2152"><code>	// There's not enough room in the head arena. We may need to</code></span>
<span class="codeline" id="line-2153"><code>	// allocate a new arena.</code></span>
<span class="codeline" id="line-2154"><code>	lock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2155"><code>	// Try the head arena again, since it may have changed. Now</code></span>
<span class="codeline" id="line-2156"><code>	// that we hold the lock, the list head can't change, but its</code></span>
<span class="codeline" id="line-2157"><code>	// free position still can.</code></span>
<span class="codeline" id="line-2158"><code>	if p := gcBitsArenas.next.tryAlloc(bytesNeeded); p != nil {</code></span>
<span class="codeline" id="line-2159"><code>		unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2160"><code>		return p</code></span>
<span class="codeline" id="line-2161"><code>	}</code></span>
<span class="codeline" id="line-2162"><code></code></span>
<span class="codeline" id="line-2163"><code>	// Allocate a new arena. This may temporarily drop the lock.</code></span>
<span class="codeline" id="line-2164"><code>	fresh := newArenaMayUnlock()</code></span>
<span class="codeline" id="line-2165"><code>	// If newArenaMayUnlock dropped the lock, another thread may</code></span>
<span class="codeline" id="line-2166"><code>	// have put a fresh arena on the "next" list. Try allocating</code></span>
<span class="codeline" id="line-2167"><code>	// from next again.</code></span>
<span class="codeline" id="line-2168"><code>	if p := gcBitsArenas.next.tryAlloc(bytesNeeded); p != nil {</code></span>
<span class="codeline" id="line-2169"><code>		// Put fresh back on the free list.</code></span>
<span class="codeline" id="line-2170"><code>		// TODO: Mark it "already zeroed"</code></span>
<span class="codeline" id="line-2171"><code>		fresh.next = gcBitsArenas.free</code></span>
<span class="codeline" id="line-2172"><code>		gcBitsArenas.free = fresh</code></span>
<span class="codeline" id="line-2173"><code>		unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2174"><code>		return p</code></span>
<span class="codeline" id="line-2175"><code>	}</code></span>
<span class="codeline" id="line-2176"><code></code></span>
<span class="codeline" id="line-2177"><code>	// Allocate from the fresh arena. We haven't linked it in yet, so</code></span>
<span class="codeline" id="line-2178"><code>	// this cannot race and is guaranteed to succeed.</code></span>
<span class="codeline" id="line-2179"><code>	p := fresh.tryAlloc(bytesNeeded)</code></span>
<span class="codeline" id="line-2180"><code>	if p == nil {</code></span>
<span class="codeline" id="line-2181"><code>		throw("markBits overflow")</code></span>
<span class="codeline" id="line-2182"><code>	}</code></span>
<span class="codeline" id="line-2183"><code></code></span>
<span class="codeline" id="line-2184"><code>	// Add the fresh arena to the "next" list.</code></span>
<span class="codeline" id="line-2185"><code>	fresh.next = gcBitsArenas.next</code></span>
<span class="codeline" id="line-2186"><code>	atomic.StorepNoWB(unsafe.Pointer(&amp;gcBitsArenas.next), unsafe.Pointer(fresh))</code></span>
<span class="codeline" id="line-2187"><code></code></span>
<span class="codeline" id="line-2188"><code>	unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2189"><code>	return p</code></span>
<span class="codeline" id="line-2190"><code>}</code></span>
<span class="codeline" id="line-2191"><code></code></span>
<span class="codeline" id="line-2192"><code>// newAllocBits returns a pointer to 8 byte aligned bytes</code></span>
<span class="codeline" id="line-2193"><code>// to be used for this span's alloc bits.</code></span>
<span class="codeline" id="line-2194"><code>// newAllocBits is used to provide newly initialized spans</code></span>
<span class="codeline" id="line-2195"><code>// allocation bits. For spans not being initialized the</code></span>
<span class="codeline" id="line-2196"><code>// mark bits are repurposed as allocation bits when</code></span>
<span class="codeline" id="line-2197"><code>// the span is swept.</code></span>
<span class="codeline" id="line-2198"><code>func newAllocBits(nelems uintptr) *gcBits {</code></span>
<span class="codeline" id="line-2199"><code>	return newMarkBits(nelems)</code></span>
<span class="codeline" id="line-2200"><code>}</code></span>
<span class="codeline" id="line-2201"><code></code></span>
<span class="codeline" id="line-2202"><code>// nextMarkBitArenaEpoch establishes a new epoch for the arenas</code></span>
<span class="codeline" id="line-2203"><code>// holding the mark bits. The arenas are named relative to the</code></span>
<span class="codeline" id="line-2204"><code>// current GC cycle which is demarcated by the call to finishweep_m.</code></span>
<span class="codeline" id="line-2205"><code>//</code></span>
<span class="codeline" id="line-2206"><code>// All current spans have been swept.</code></span>
<span class="codeline" id="line-2207"><code>// During that sweep each span allocated room for its gcmarkBits in</code></span>
<span class="codeline" id="line-2208"><code>// gcBitsArenas.next block. gcBitsArenas.next becomes the gcBitsArenas.current</code></span>
<span class="codeline" id="line-2209"><code>// where the GC will mark objects and after each span is swept these bits</code></span>
<span class="codeline" id="line-2210"><code>// will be used to allocate objects.</code></span>
<span class="codeline" id="line-2211"><code>// gcBitsArenas.current becomes gcBitsArenas.previous where the span's</code></span>
<span class="codeline" id="line-2212"><code>// gcAllocBits live until all the spans have been swept during this GC cycle.</code></span>
<span class="codeline" id="line-2213"><code>// The span's sweep extinguishes all the references to gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2214"><code>// by pointing gcAllocBits into the gcBitsArenas.current.</code></span>
<span class="codeline" id="line-2215"><code>// The gcBitsArenas.previous is released to the gcBitsArenas.free list.</code></span>
<span class="codeline" id="line-2216"><code>func nextMarkBitArenaEpoch() {</code></span>
<span class="codeline" id="line-2217"><code>	lock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2218"><code>	if gcBitsArenas.previous != nil {</code></span>
<span class="codeline" id="line-2219"><code>		if gcBitsArenas.free == nil {</code></span>
<span class="codeline" id="line-2220"><code>			gcBitsArenas.free = gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2221"><code>		} else {</code></span>
<span class="codeline" id="line-2222"><code>			// Find end of previous arenas.</code></span>
<span class="codeline" id="line-2223"><code>			last := gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2224"><code>			for last = gcBitsArenas.previous; last.next != nil; last = last.next {</code></span>
<span class="codeline" id="line-2225"><code>			}</code></span>
<span class="codeline" id="line-2226"><code>			last.next = gcBitsArenas.free</code></span>
<span class="codeline" id="line-2227"><code>			gcBitsArenas.free = gcBitsArenas.previous</code></span>
<span class="codeline" id="line-2228"><code>		}</code></span>
<span class="codeline" id="line-2229"><code>	}</code></span>
<span class="codeline" id="line-2230"><code>	gcBitsArenas.previous = gcBitsArenas.current</code></span>
<span class="codeline" id="line-2231"><code>	gcBitsArenas.current = gcBitsArenas.next</code></span>
<span class="codeline" id="line-2232"><code>	atomic.StorepNoWB(unsafe.Pointer(&amp;gcBitsArenas.next), nil) // newMarkBits calls newArena when needed</code></span>
<span class="codeline" id="line-2233"><code>	unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2234"><code>}</code></span>
<span class="codeline" id="line-2235"><code></code></span>
<span class="codeline" id="line-2236"><code>// newArenaMayUnlock allocates and zeroes a gcBits arena.</code></span>
<span class="codeline" id="line-2237"><code>// The caller must hold gcBitsArena.lock. This may temporarily release it.</code></span>
<span class="codeline" id="line-2238"><code>func newArenaMayUnlock() *gcBitsArena {</code></span>
<span class="codeline" id="line-2239"><code>	var result *gcBitsArena</code></span>
<span class="codeline" id="line-2240"><code>	if gcBitsArenas.free == nil {</code></span>
<span class="codeline" id="line-2241"><code>		unlock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2242"><code>		result = (*gcBitsArena)(sysAlloc(gcBitsChunkBytes, &amp;memstats.gcMiscSys))</code></span>
<span class="codeline" id="line-2243"><code>		if result == nil {</code></span>
<span class="codeline" id="line-2244"><code>			throw("runtime: cannot allocate memory")</code></span>
<span class="codeline" id="line-2245"><code>		}</code></span>
<span class="codeline" id="line-2246"><code>		lock(&amp;gcBitsArenas.lock)</code></span>
<span class="codeline" id="line-2247"><code>	} else {</code></span>
<span class="codeline" id="line-2248"><code>		result = gcBitsArenas.free</code></span>
<span class="codeline" id="line-2249"><code>		gcBitsArenas.free = gcBitsArenas.free.next</code></span>
<span class="codeline" id="line-2250"><code>		memclrNoHeapPointers(unsafe.Pointer(result), gcBitsChunkBytes)</code></span>
<span class="codeline" id="line-2251"><code>	}</code></span>
<span class="codeline" id="line-2252"><code>	result.next = nil</code></span>
<span class="codeline" id="line-2253"><code>	// If result.bits is not 8 byte aligned adjust index so</code></span>
<span class="codeline" id="line-2254"><code>	// that &amp;result.bits[result.free] is 8 byte aligned.</code></span>
<span class="codeline" id="line-2255"><code>	if unsafe.Offsetof(gcBitsArena{}.bits)&amp;7 == 0 {</code></span>
<span class="codeline" id="line-2256"><code>		result.free = 0</code></span>
<span class="codeline" id="line-2257"><code>	} else {</code></span>
<span class="codeline" id="line-2258"><code>		result.free = 8 - (uintptr(unsafe.Pointer(&amp;result.bits[0])) &amp; 7)</code></span>
<span class="codeline" id="line-2259"><code>	}</code></span>
<span class="codeline" id="line-2260"><code>	return result</code></span>
<span class="codeline" id="line-2261"><code>}</code></span>
</pre><pre id="footer">
<table><tr><td><img src="../../png/go101-twitter.png"></td>
<td>The pages are generated with <a href="https://go101.org/apps-and-libs/golds.html"><b>Golds</b></a> <i>v0.6.8</i>. (GOOS=linux GOARCH=amd64)
<b>Golds</b> is a <a href="https://go101.org">Go 101</a> project developed by <a href="https://tapirgames.com">Tapir Liu</a>.
PR and bug reports are welcome and can be submitted to <a href="https://github.com/go101/golds">the issue list</a>.
Please follow <a href="https://twitter.com/go100and1">@Go100and1</a> (reachable from the left QR code) to get the latest news of <b>Golds</b>.</td></tr></table></pre>