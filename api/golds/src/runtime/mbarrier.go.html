<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Source: mbarrier.go in package runtime</title>
<link href="../../css/dark-v0.6.8.css" rel="stylesheet">
<script src="../../jvs/golds-v0.6.8.js"></script>
<body onload="onPageLoad()"><div>

<pre id="header"><code><span class="title">Source File</span>
	mbarrier.go

<span class="title">Belonging Package</span>
	<a href="../../pkg/runtime.html">runtime</a>
</code></pre>

<pre class="line-numbers">
<span class="codeline" id="line-1"><code>// Copyright 2015 The Go Authors. All rights reserved.</code></span>
<span class="codeline" id="line-2"><code>// Use of this source code is governed by a BSD-style</code></span>
<span class="codeline" id="line-3"><code>// license that can be found in the LICENSE file.</code></span>
<span class="codeline" id="line-4"><code></code></span>
<span class="codeline" id="line-5"><code>// Garbage collector: write barriers.</code></span>
<span class="codeline" id="line-6"><code>//</code></span>
<span class="codeline" id="line-7"><code>// For the concurrent garbage collector, the Go compiler implements</code></span>
<span class="codeline" id="line-8"><code>// updates to pointer-valued fields that may be in heap objects by</code></span>
<span class="codeline" id="line-9"><code>// emitting calls to write barriers. The main write barrier for</code></span>
<span class="codeline" id="line-10"><code>// individual pointer writes is gcWriteBarrier and is implemented in</code></span>
<span class="codeline" id="line-11"><code>// assembly. This file contains write barrier entry points for bulk</code></span>
<span class="codeline" id="line-12"><code>// operations. See also mwbbuf.go.</code></span>
<span class="codeline" id="line-13"><code></code></span>
<span class="codeline" id="line-14"><code>package runtime</code></span>
<span class="codeline" id="line-15"><code></code></span>
<span class="codeline" id="line-16"><code>import (</code></span>
<span class="codeline" id="line-17"><code>	"internal/abi"</code></span>
<span class="codeline" id="line-18"><code>	"internal/goarch"</code></span>
<span class="codeline" id="line-19"><code>	"internal/goexperiment"</code></span>
<span class="codeline" id="line-20"><code>	"unsafe"</code></span>
<span class="codeline" id="line-21"><code>)</code></span>
<span class="codeline" id="line-22"><code></code></span>
<span class="codeline" id="line-23"><code>// Go uses a hybrid barrier that combines a Yuasa-style deletion</code></span>
<span class="codeline" id="line-24"><code>// barrier—which shades the object whose reference is being</code></span>
<span class="codeline" id="line-25"><code>// overwritten—with Dijkstra insertion barrier—which shades the object</code></span>
<span class="codeline" id="line-26"><code>// whose reference is being written. The insertion part of the barrier</code></span>
<span class="codeline" id="line-27"><code>// is necessary while the calling goroutine's stack is grey. In</code></span>
<span class="codeline" id="line-28"><code>// pseudocode, the barrier is:</code></span>
<span class="codeline" id="line-29"><code>//</code></span>
<span class="codeline" id="line-30"><code>//     writePointer(slot, ptr):</code></span>
<span class="codeline" id="line-31"><code>//         shade(*slot)</code></span>
<span class="codeline" id="line-32"><code>//         if current stack is grey:</code></span>
<span class="codeline" id="line-33"><code>//             shade(ptr)</code></span>
<span class="codeline" id="line-34"><code>//         *slot = ptr</code></span>
<span class="codeline" id="line-35"><code>//</code></span>
<span class="codeline" id="line-36"><code>// slot is the destination in Go code.</code></span>
<span class="codeline" id="line-37"><code>// ptr is the value that goes into the slot in Go code.</code></span>
<span class="codeline" id="line-38"><code>//</code></span>
<span class="codeline" id="line-39"><code>// Shade indicates that it has seen a white pointer by adding the referent</code></span>
<span class="codeline" id="line-40"><code>// to wbuf as well as marking it.</code></span>
<span class="codeline" id="line-41"><code>//</code></span>
<span class="codeline" id="line-42"><code>// The two shades and the condition work together to prevent a mutator</code></span>
<span class="codeline" id="line-43"><code>// from hiding an object from the garbage collector:</code></span>
<span class="codeline" id="line-44"><code>//</code></span>
<span class="codeline" id="line-45"><code>// 1. shade(*slot) prevents a mutator from hiding an object by moving</code></span>
<span class="codeline" id="line-46"><code>// the sole pointer to it from the heap to its stack. If it attempts</code></span>
<span class="codeline" id="line-47"><code>// to unlink an object from the heap, this will shade it.</code></span>
<span class="codeline" id="line-48"><code>//</code></span>
<span class="codeline" id="line-49"><code>// 2. shade(ptr) prevents a mutator from hiding an object by moving</code></span>
<span class="codeline" id="line-50"><code>// the sole pointer to it from its stack into a black object in the</code></span>
<span class="codeline" id="line-51"><code>// heap. If it attempts to install the pointer into a black object,</code></span>
<span class="codeline" id="line-52"><code>// this will shade it.</code></span>
<span class="codeline" id="line-53"><code>//</code></span>
<span class="codeline" id="line-54"><code>// 3. Once a goroutine's stack is black, the shade(ptr) becomes</code></span>
<span class="codeline" id="line-55"><code>// unnecessary. shade(ptr) prevents hiding an object by moving it from</code></span>
<span class="codeline" id="line-56"><code>// the stack to the heap, but this requires first having a pointer</code></span>
<span class="codeline" id="line-57"><code>// hidden on the stack. Immediately after a stack is scanned, it only</code></span>
<span class="codeline" id="line-58"><code>// points to shaded objects, so it's not hiding anything, and the</code></span>
<span class="codeline" id="line-59"><code>// shade(*slot) prevents it from hiding any other pointers on its</code></span>
<span class="codeline" id="line-60"><code>// stack.</code></span>
<span class="codeline" id="line-61"><code>//</code></span>
<span class="codeline" id="line-62"><code>// For a detailed description of this barrier and proof of</code></span>
<span class="codeline" id="line-63"><code>// correctness, see https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md</code></span>
<span class="codeline" id="line-64"><code>//</code></span>
<span class="codeline" id="line-65"><code>//</code></span>
<span class="codeline" id="line-66"><code>//</code></span>
<span class="codeline" id="line-67"><code>// Dealing with memory ordering:</code></span>
<span class="codeline" id="line-68"><code>//</code></span>
<span class="codeline" id="line-69"><code>// Both the Yuasa and Dijkstra barriers can be made conditional on the</code></span>
<span class="codeline" id="line-70"><code>// color of the object containing the slot. We chose not to make these</code></span>
<span class="codeline" id="line-71"><code>// conditional because the cost of ensuring that the object holding</code></span>
<span class="codeline" id="line-72"><code>// the slot doesn't concurrently change color without the mutator</code></span>
<span class="codeline" id="line-73"><code>// noticing seems prohibitive.</code></span>
<span class="codeline" id="line-74"><code>//</code></span>
<span class="codeline" id="line-75"><code>// Consider the following example where the mutator writes into</code></span>
<span class="codeline" id="line-76"><code>// a slot and then loads the slot's mark bit while the GC thread</code></span>
<span class="codeline" id="line-77"><code>// writes to the slot's mark bit and then as part of scanning reads</code></span>
<span class="codeline" id="line-78"><code>// the slot.</code></span>
<span class="codeline" id="line-79"><code>//</code></span>
<span class="codeline" id="line-80"><code>// Initially both [slot] and [slotmark] are 0 (nil)</code></span>
<span class="codeline" id="line-81"><code>// Mutator thread          GC thread</code></span>
<span class="codeline" id="line-82"><code>// st [slot], ptr          st [slotmark], 1</code></span>
<span class="codeline" id="line-83"><code>//</code></span>
<span class="codeline" id="line-84"><code>// ld r1, [slotmark]       ld r2, [slot]</code></span>
<span class="codeline" id="line-85"><code>//</code></span>
<span class="codeline" id="line-86"><code>// Without an expensive memory barrier between the st and the ld, the final</code></span>
<span class="codeline" id="line-87"><code>// result on most HW (including 386/amd64) can be r1==r2==0. This is a classic</code></span>
<span class="codeline" id="line-88"><code>// example of what can happen when loads are allowed to be reordered with older</code></span>
<span class="codeline" id="line-89"><code>// stores (avoiding such reorderings lies at the heart of the classic</code></span>
<span class="codeline" id="line-90"><code>// Peterson/Dekker algorithms for mutual exclusion). Rather than require memory</code></span>
<span class="codeline" id="line-91"><code>// barriers, which will slow down both the mutator and the GC, we always grey</code></span>
<span class="codeline" id="line-92"><code>// the ptr object regardless of the slot's color.</code></span>
<span class="codeline" id="line-93"><code>//</code></span>
<span class="codeline" id="line-94"><code>// Another place where we intentionally omit memory barriers is when</code></span>
<span class="codeline" id="line-95"><code>// accessing mheap_.arena_used to check if a pointer points into the</code></span>
<span class="codeline" id="line-96"><code>// heap. On relaxed memory machines, it's possible for a mutator to</code></span>
<span class="codeline" id="line-97"><code>// extend the size of the heap by updating arena_used, allocate an</code></span>
<span class="codeline" id="line-98"><code>// object from this new region, and publish a pointer to that object,</code></span>
<span class="codeline" id="line-99"><code>// but for tracing running on another processor to observe the pointer</code></span>
<span class="codeline" id="line-100"><code>// but use the old value of arena_used. In this case, tracing will not</code></span>
<span class="codeline" id="line-101"><code>// mark the object, even though it's reachable. However, the mutator</code></span>
<span class="codeline" id="line-102"><code>// is guaranteed to execute a write barrier when it publishes the</code></span>
<span class="codeline" id="line-103"><code>// pointer, so it will take care of marking the object. A general</code></span>
<span class="codeline" id="line-104"><code>// consequence of this is that the garbage collector may cache the</code></span>
<span class="codeline" id="line-105"><code>// value of mheap_.arena_used. (See issue #9984.)</code></span>
<span class="codeline" id="line-106"><code>//</code></span>
<span class="codeline" id="line-107"><code>//</code></span>
<span class="codeline" id="line-108"><code>// Stack writes:</code></span>
<span class="codeline" id="line-109"><code>//</code></span>
<span class="codeline" id="line-110"><code>// The compiler omits write barriers for writes to the current frame,</code></span>
<span class="codeline" id="line-111"><code>// but if a stack pointer has been passed down the call stack, the</code></span>
<span class="codeline" id="line-112"><code>// compiler will generate a write barrier for writes through that</code></span>
<span class="codeline" id="line-113"><code>// pointer (because it doesn't know it's not a heap pointer).</code></span>
<span class="codeline" id="line-114"><code>//</code></span>
<span class="codeline" id="line-115"><code>//</code></span>
<span class="codeline" id="line-116"><code>// Global writes:</code></span>
<span class="codeline" id="line-117"><code>//</code></span>
<span class="codeline" id="line-118"><code>// The Go garbage collector requires write barriers when heap pointers</code></span>
<span class="codeline" id="line-119"><code>// are stored in globals. Many garbage collectors ignore writes to</code></span>
<span class="codeline" id="line-120"><code>// globals and instead pick up global -&gt; heap pointers during</code></span>
<span class="codeline" id="line-121"><code>// termination. This increases pause time, so we instead rely on write</code></span>
<span class="codeline" id="line-122"><code>// barriers for writes to globals so that we don't have to rescan</code></span>
<span class="codeline" id="line-123"><code>// global during mark termination.</code></span>
<span class="codeline" id="line-124"><code>//</code></span>
<span class="codeline" id="line-125"><code>//</code></span>
<span class="codeline" id="line-126"><code>// Publication ordering:</code></span>
<span class="codeline" id="line-127"><code>//</code></span>
<span class="codeline" id="line-128"><code>// The write barrier is *pre-publication*, meaning that the write</code></span>
<span class="codeline" id="line-129"><code>// barrier happens prior to the *slot = ptr write that may make ptr</code></span>
<span class="codeline" id="line-130"><code>// reachable by some goroutine that currently cannot reach it.</code></span>
<span class="codeline" id="line-131"><code>//</code></span>
<span class="codeline" id="line-132"><code>//</code></span>
<span class="codeline" id="line-133"><code>// Signal handler pointer writes:</code></span>
<span class="codeline" id="line-134"><code>//</code></span>
<span class="codeline" id="line-135"><code>// In general, the signal handler cannot safely invoke the write</code></span>
<span class="codeline" id="line-136"><code>// barrier because it may run without a P or even during the write</code></span>
<span class="codeline" id="line-137"><code>// barrier.</code></span>
<span class="codeline" id="line-138"><code>//</code></span>
<span class="codeline" id="line-139"><code>// There is exactly one exception: profbuf.go omits a barrier during</code></span>
<span class="codeline" id="line-140"><code>// signal handler profile logging. That's safe only because of the</code></span>
<span class="codeline" id="line-141"><code>// deletion barrier. See profbuf.go for a detailed argument. If we</code></span>
<span class="codeline" id="line-142"><code>// remove the deletion barrier, we'll have to work out a new way to</code></span>
<span class="codeline" id="line-143"><code>// handle the profile logging.</code></span>
<span class="codeline" id="line-144"><code></code></span>
<span class="codeline" id="line-145"><code>// typedmemmove copies a value of type typ to dst from src.</code></span>
<span class="codeline" id="line-146"><code>// Must be nosplit, see #16026.</code></span>
<span class="codeline" id="line-147"><code>//</code></span>
<span class="codeline" id="line-148"><code>// TODO: Perfect for go:nosplitrec since we can't have a safe point</code></span>
<span class="codeline" id="line-149"><code>// anywhere in the bulk barrier or memmove.</code></span>
<span class="codeline" id="line-150"><code>//</code></span>
<span class="codeline" id="line-151"><code>//go:nosplit</code></span>
<span class="codeline" id="line-152"><code>func typedmemmove(typ *abi.Type, dst, src unsafe.Pointer) {</code></span>
<span class="codeline" id="line-153"><code>	if dst == src {</code></span>
<span class="codeline" id="line-154"><code>		return</code></span>
<span class="codeline" id="line-155"><code>	}</code></span>
<span class="codeline" id="line-156"><code>	if writeBarrier.enabled &amp;&amp; typ.PtrBytes != 0 {</code></span>
<span class="codeline" id="line-157"><code>		// This always copies a full value of type typ so it's safe</code></span>
<span class="codeline" id="line-158"><code>		// to pass typ along as an optimization. See the comment on</code></span>
<span class="codeline" id="line-159"><code>		// bulkBarrierPreWrite.</code></span>
<span class="codeline" id="line-160"><code>		bulkBarrierPreWrite(uintptr(dst), uintptr(src), typ.PtrBytes, typ)</code></span>
<span class="codeline" id="line-161"><code>	}</code></span>
<span class="codeline" id="line-162"><code>	// There's a race here: if some other goroutine can write to</code></span>
<span class="codeline" id="line-163"><code>	// src, it may change some pointer in src after we've</code></span>
<span class="codeline" id="line-164"><code>	// performed the write barrier but before we perform the</code></span>
<span class="codeline" id="line-165"><code>	// memory copy. This safe because the write performed by that</code></span>
<span class="codeline" id="line-166"><code>	// other goroutine must also be accompanied by a write</code></span>
<span class="codeline" id="line-167"><code>	// barrier, so at worst we've unnecessarily greyed the old</code></span>
<span class="codeline" id="line-168"><code>	// pointer that was in src.</code></span>
<span class="codeline" id="line-169"><code>	memmove(dst, src, typ.Size_)</code></span>
<span class="codeline" id="line-170"><code>	if goexperiment.CgoCheck2 {</code></span>
<span class="codeline" id="line-171"><code>		cgoCheckMemmove2(typ, dst, src, 0, typ.Size_)</code></span>
<span class="codeline" id="line-172"><code>	}</code></span>
<span class="codeline" id="line-173"><code>}</code></span>
<span class="codeline" id="line-174"><code></code></span>
<span class="codeline" id="line-175"><code>// wbZero performs the write barrier operations necessary before</code></span>
<span class="codeline" id="line-176"><code>// zeroing a region of memory at address dst of type typ.</code></span>
<span class="codeline" id="line-177"><code>// Does not actually do the zeroing.</code></span>
<span class="codeline" id="line-178"><code>//</code></span>
<span class="codeline" id="line-179"><code>//go:nowritebarrierrec</code></span>
<span class="codeline" id="line-180"><code>//go:nosplit</code></span>
<span class="codeline" id="line-181"><code>func wbZero(typ *_type, dst unsafe.Pointer) {</code></span>
<span class="codeline" id="line-182"><code>	// This always copies a full value of type typ so it's safe</code></span>
<span class="codeline" id="line-183"><code>	// to pass typ along as an optimization. See the comment on</code></span>
<span class="codeline" id="line-184"><code>	// bulkBarrierPreWrite.</code></span>
<span class="codeline" id="line-185"><code>	bulkBarrierPreWrite(uintptr(dst), 0, typ.PtrBytes, typ)</code></span>
<span class="codeline" id="line-186"><code>}</code></span>
<span class="codeline" id="line-187"><code></code></span>
<span class="codeline" id="line-188"><code>// wbMove performs the write barrier operations necessary before</code></span>
<span class="codeline" id="line-189"><code>// copying a region of memory from src to dst of type typ.</code></span>
<span class="codeline" id="line-190"><code>// Does not actually do the copying.</code></span>
<span class="codeline" id="line-191"><code>//</code></span>
<span class="codeline" id="line-192"><code>//go:nowritebarrierrec</code></span>
<span class="codeline" id="line-193"><code>//go:nosplit</code></span>
<span class="codeline" id="line-194"><code>func wbMove(typ *_type, dst, src unsafe.Pointer) {</code></span>
<span class="codeline" id="line-195"><code>	// This always copies a full value of type typ so it's safe to</code></span>
<span class="codeline" id="line-196"><code>	// pass a type here.</code></span>
<span class="codeline" id="line-197"><code>	//</code></span>
<span class="codeline" id="line-198"><code>	// See the comment on bulkBarrierPreWrite.</code></span>
<span class="codeline" id="line-199"><code>	bulkBarrierPreWrite(uintptr(dst), uintptr(src), typ.PtrBytes, typ)</code></span>
<span class="codeline" id="line-200"><code>}</code></span>
<span class="codeline" id="line-201"><code></code></span>
<span class="codeline" id="line-202"><code>//go:linkname reflect_typedmemmove reflect.typedmemmove</code></span>
<span class="codeline" id="line-203"><code>func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer) {</code></span>
<span class="codeline" id="line-204"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-205"><code>		raceWriteObjectPC(typ, dst, getcallerpc(), abi.FuncPCABIInternal(reflect_typedmemmove))</code></span>
<span class="codeline" id="line-206"><code>		raceReadObjectPC(typ, src, getcallerpc(), abi.FuncPCABIInternal(reflect_typedmemmove))</code></span>
<span class="codeline" id="line-207"><code>	}</code></span>
<span class="codeline" id="line-208"><code>	if msanenabled {</code></span>
<span class="codeline" id="line-209"><code>		msanwrite(dst, typ.Size_)</code></span>
<span class="codeline" id="line-210"><code>		msanread(src, typ.Size_)</code></span>
<span class="codeline" id="line-211"><code>	}</code></span>
<span class="codeline" id="line-212"><code>	if asanenabled {</code></span>
<span class="codeline" id="line-213"><code>		asanwrite(dst, typ.Size_)</code></span>
<span class="codeline" id="line-214"><code>		asanread(src, typ.Size_)</code></span>
<span class="codeline" id="line-215"><code>	}</code></span>
<span class="codeline" id="line-216"><code>	typedmemmove(typ, dst, src)</code></span>
<span class="codeline" id="line-217"><code>}</code></span>
<span class="codeline" id="line-218"><code></code></span>
<span class="codeline" id="line-219"><code>//go:linkname reflectlite_typedmemmove internal/reflectlite.typedmemmove</code></span>
<span class="codeline" id="line-220"><code>func reflectlite_typedmemmove(typ *_type, dst, src unsafe.Pointer) {</code></span>
<span class="codeline" id="line-221"><code>	reflect_typedmemmove(typ, dst, src)</code></span>
<span class="codeline" id="line-222"><code>}</code></span>
<span class="codeline" id="line-223"><code></code></span>
<span class="codeline" id="line-224"><code>// reflectcallmove is invoked by reflectcall to copy the return values</code></span>
<span class="codeline" id="line-225"><code>// out of the stack and into the heap, invoking the necessary write</code></span>
<span class="codeline" id="line-226"><code>// barriers. dst, src, and size describe the return value area to</code></span>
<span class="codeline" id="line-227"><code>// copy. typ describes the entire frame (not just the return values).</code></span>
<span class="codeline" id="line-228"><code>// typ may be nil, which indicates write barriers are not needed.</code></span>
<span class="codeline" id="line-229"><code>//</code></span>
<span class="codeline" id="line-230"><code>// It must be nosplit and must only call nosplit functions because the</code></span>
<span class="codeline" id="line-231"><code>// stack map of reflectcall is wrong.</code></span>
<span class="codeline" id="line-232"><code>//</code></span>
<span class="codeline" id="line-233"><code>//go:nosplit</code></span>
<span class="codeline" id="line-234"><code>func reflectcallmove(typ *_type, dst, src unsafe.Pointer, size uintptr, regs *abi.RegArgs) {</code></span>
<span class="codeline" id="line-235"><code>	if writeBarrier.enabled &amp;&amp; typ != nil &amp;&amp; typ.PtrBytes != 0 &amp;&amp; size &gt;= goarch.PtrSize {</code></span>
<span class="codeline" id="line-236"><code>		// Pass nil for the type. dst does not point to value of type typ,</code></span>
<span class="codeline" id="line-237"><code>		// but rather points into one, so applying the optimization is not</code></span>
<span class="codeline" id="line-238"><code>		// safe. See the comment on this function.</code></span>
<span class="codeline" id="line-239"><code>		bulkBarrierPreWrite(uintptr(dst), uintptr(src), size, nil)</code></span>
<span class="codeline" id="line-240"><code>	}</code></span>
<span class="codeline" id="line-241"><code>	memmove(dst, src, size)</code></span>
<span class="codeline" id="line-242"><code></code></span>
<span class="codeline" id="line-243"><code>	// Move pointers returned in registers to a place where the GC can see them.</code></span>
<span class="codeline" id="line-244"><code>	for i := range regs.Ints {</code></span>
<span class="codeline" id="line-245"><code>		if regs.ReturnIsPtr.Get(i) {</code></span>
<span class="codeline" id="line-246"><code>			regs.Ptrs[i] = unsafe.Pointer(regs.Ints[i])</code></span>
<span class="codeline" id="line-247"><code>		}</code></span>
<span class="codeline" id="line-248"><code>	}</code></span>
<span class="codeline" id="line-249"><code>}</code></span>
<span class="codeline" id="line-250"><code></code></span>
<span class="codeline" id="line-251"><code>//go:nosplit</code></span>
<span class="codeline" id="line-252"><code>func typedslicecopy(typ *_type, dstPtr unsafe.Pointer, dstLen int, srcPtr unsafe.Pointer, srcLen int) int {</code></span>
<span class="codeline" id="line-253"><code>	n := dstLen</code></span>
<span class="codeline" id="line-254"><code>	if n &gt; srcLen {</code></span>
<span class="codeline" id="line-255"><code>		n = srcLen</code></span>
<span class="codeline" id="line-256"><code>	}</code></span>
<span class="codeline" id="line-257"><code>	if n == 0 {</code></span>
<span class="codeline" id="line-258"><code>		return 0</code></span>
<span class="codeline" id="line-259"><code>	}</code></span>
<span class="codeline" id="line-260"><code></code></span>
<span class="codeline" id="line-261"><code>	// The compiler emits calls to typedslicecopy before</code></span>
<span class="codeline" id="line-262"><code>	// instrumentation runs, so unlike the other copying and</code></span>
<span class="codeline" id="line-263"><code>	// assignment operations, it's not instrumented in the calling</code></span>
<span class="codeline" id="line-264"><code>	// code and needs its own instrumentation.</code></span>
<span class="codeline" id="line-265"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-266"><code>		callerpc := getcallerpc()</code></span>
<span class="codeline" id="line-267"><code>		pc := abi.FuncPCABIInternal(slicecopy)</code></span>
<span class="codeline" id="line-268"><code>		racewriterangepc(dstPtr, uintptr(n)*typ.Size_, callerpc, pc)</code></span>
<span class="codeline" id="line-269"><code>		racereadrangepc(srcPtr, uintptr(n)*typ.Size_, callerpc, pc)</code></span>
<span class="codeline" id="line-270"><code>	}</code></span>
<span class="codeline" id="line-271"><code>	if msanenabled {</code></span>
<span class="codeline" id="line-272"><code>		msanwrite(dstPtr, uintptr(n)*typ.Size_)</code></span>
<span class="codeline" id="line-273"><code>		msanread(srcPtr, uintptr(n)*typ.Size_)</code></span>
<span class="codeline" id="line-274"><code>	}</code></span>
<span class="codeline" id="line-275"><code>	if asanenabled {</code></span>
<span class="codeline" id="line-276"><code>		asanwrite(dstPtr, uintptr(n)*typ.Size_)</code></span>
<span class="codeline" id="line-277"><code>		asanread(srcPtr, uintptr(n)*typ.Size_)</code></span>
<span class="codeline" id="line-278"><code>	}</code></span>
<span class="codeline" id="line-279"><code></code></span>
<span class="codeline" id="line-280"><code>	if goexperiment.CgoCheck2 {</code></span>
<span class="codeline" id="line-281"><code>		cgoCheckSliceCopy(typ, dstPtr, srcPtr, n)</code></span>
<span class="codeline" id="line-282"><code>	}</code></span>
<span class="codeline" id="line-283"><code></code></span>
<span class="codeline" id="line-284"><code>	if dstPtr == srcPtr {</code></span>
<span class="codeline" id="line-285"><code>		return n</code></span>
<span class="codeline" id="line-286"><code>	}</code></span>
<span class="codeline" id="line-287"><code></code></span>
<span class="codeline" id="line-288"><code>	// Note: No point in checking typ.PtrBytes here:</code></span>
<span class="codeline" id="line-289"><code>	// compiler only emits calls to typedslicecopy for types with pointers,</code></span>
<span class="codeline" id="line-290"><code>	// and growslice and reflect_typedslicecopy check for pointers</code></span>
<span class="codeline" id="line-291"><code>	// before calling typedslicecopy.</code></span>
<span class="codeline" id="line-292"><code>	size := uintptr(n) * typ.Size_</code></span>
<span class="codeline" id="line-293"><code>	if writeBarrier.enabled {</code></span>
<span class="codeline" id="line-294"><code>		// This always copies one or more full values of type typ so</code></span>
<span class="codeline" id="line-295"><code>		// it's safe to pass typ along as an optimization. See the comment on</code></span>
<span class="codeline" id="line-296"><code>		// bulkBarrierPreWrite.</code></span>
<span class="codeline" id="line-297"><code>		pwsize := size - typ.Size_ + typ.PtrBytes</code></span>
<span class="codeline" id="line-298"><code>		bulkBarrierPreWrite(uintptr(dstPtr), uintptr(srcPtr), pwsize, typ)</code></span>
<span class="codeline" id="line-299"><code>	}</code></span>
<span class="codeline" id="line-300"><code>	// See typedmemmove for a discussion of the race between the</code></span>
<span class="codeline" id="line-301"><code>	// barrier and memmove.</code></span>
<span class="codeline" id="line-302"><code>	memmove(dstPtr, srcPtr, size)</code></span>
<span class="codeline" id="line-303"><code>	return n</code></span>
<span class="codeline" id="line-304"><code>}</code></span>
<span class="codeline" id="line-305"><code></code></span>
<span class="codeline" id="line-306"><code>//go:linkname reflect_typedslicecopy reflect.typedslicecopy</code></span>
<span class="codeline" id="line-307"><code>func reflect_typedslicecopy(elemType *_type, dst, src slice) int {</code></span>
<span class="codeline" id="line-308"><code>	if elemType.PtrBytes == 0 {</code></span>
<span class="codeline" id="line-309"><code>		return slicecopy(dst.array, dst.len, src.array, src.len, elemType.Size_)</code></span>
<span class="codeline" id="line-310"><code>	}</code></span>
<span class="codeline" id="line-311"><code>	return typedslicecopy(elemType, dst.array, dst.len, src.array, src.len)</code></span>
<span class="codeline" id="line-312"><code>}</code></span>
<span class="codeline" id="line-313"><code></code></span>
<span class="codeline" id="line-314"><code>// typedmemclr clears the typed memory at ptr with type typ. The</code></span>
<span class="codeline" id="line-315"><code>// memory at ptr must already be initialized (and hence in type-safe</code></span>
<span class="codeline" id="line-316"><code>// state). If the memory is being initialized for the first time, see</code></span>
<span class="codeline" id="line-317"><code>// memclrNoHeapPointers.</code></span>
<span class="codeline" id="line-318"><code>//</code></span>
<span class="codeline" id="line-319"><code>// If the caller knows that typ has pointers, it can alternatively</code></span>
<span class="codeline" id="line-320"><code>// call memclrHasPointers.</code></span>
<span class="codeline" id="line-321"><code>//</code></span>
<span class="codeline" id="line-322"><code>// TODO: A "go:nosplitrec" annotation would be perfect for this.</code></span>
<span class="codeline" id="line-323"><code>//</code></span>
<span class="codeline" id="line-324"><code>//go:nosplit</code></span>
<span class="codeline" id="line-325"><code>func typedmemclr(typ *_type, ptr unsafe.Pointer) {</code></span>
<span class="codeline" id="line-326"><code>	if writeBarrier.enabled &amp;&amp; typ.PtrBytes != 0 {</code></span>
<span class="codeline" id="line-327"><code>		// This always clears a whole value of type typ, so it's</code></span>
<span class="codeline" id="line-328"><code>		// safe to pass a type here and apply the optimization.</code></span>
<span class="codeline" id="line-329"><code>		// See the comment on bulkBarrierPreWrite.</code></span>
<span class="codeline" id="line-330"><code>		bulkBarrierPreWrite(uintptr(ptr), 0, typ.PtrBytes, typ)</code></span>
<span class="codeline" id="line-331"><code>	}</code></span>
<span class="codeline" id="line-332"><code>	memclrNoHeapPointers(ptr, typ.Size_)</code></span>
<span class="codeline" id="line-333"><code>}</code></span>
<span class="codeline" id="line-334"><code></code></span>
<span class="codeline" id="line-335"><code>//go:linkname reflect_typedmemclr reflect.typedmemclr</code></span>
<span class="codeline" id="line-336"><code>func reflect_typedmemclr(typ *_type, ptr unsafe.Pointer) {</code></span>
<span class="codeline" id="line-337"><code>	typedmemclr(typ, ptr)</code></span>
<span class="codeline" id="line-338"><code>}</code></span>
<span class="codeline" id="line-339"><code></code></span>
<span class="codeline" id="line-340"><code>//go:linkname reflect_typedmemclrpartial reflect.typedmemclrpartial</code></span>
<span class="codeline" id="line-341"><code>func reflect_typedmemclrpartial(typ *_type, ptr unsafe.Pointer, off, size uintptr) {</code></span>
<span class="codeline" id="line-342"><code>	if writeBarrier.enabled &amp;&amp; typ.PtrBytes != 0 {</code></span>
<span class="codeline" id="line-343"><code>		// Pass nil for the type. ptr does not point to value of type typ,</code></span>
<span class="codeline" id="line-344"><code>		// but rather points into one so it's not safe to apply the optimization.</code></span>
<span class="codeline" id="line-345"><code>		// See the comment on this function in the reflect package and the</code></span>
<span class="codeline" id="line-346"><code>		// comment on bulkBarrierPreWrite.</code></span>
<span class="codeline" id="line-347"><code>		bulkBarrierPreWrite(uintptr(ptr), 0, size, nil)</code></span>
<span class="codeline" id="line-348"><code>	}</code></span>
<span class="codeline" id="line-349"><code>	memclrNoHeapPointers(ptr, size)</code></span>
<span class="codeline" id="line-350"><code>}</code></span>
<span class="codeline" id="line-351"><code></code></span>
<span class="codeline" id="line-352"><code>//go:linkname reflect_typedarrayclear reflect.typedarrayclear</code></span>
<span class="codeline" id="line-353"><code>func reflect_typedarrayclear(typ *_type, ptr unsafe.Pointer, len int) {</code></span>
<span class="codeline" id="line-354"><code>	size := typ.Size_ * uintptr(len)</code></span>
<span class="codeline" id="line-355"><code>	if writeBarrier.enabled &amp;&amp; typ.PtrBytes != 0 {</code></span>
<span class="codeline" id="line-356"><code>		// This always clears whole elements of an array, so it's</code></span>
<span class="codeline" id="line-357"><code>		// safe to pass a type here. See the comment on bulkBarrierPreWrite.</code></span>
<span class="codeline" id="line-358"><code>		bulkBarrierPreWrite(uintptr(ptr), 0, size, typ)</code></span>
<span class="codeline" id="line-359"><code>	}</code></span>
<span class="codeline" id="line-360"><code>	memclrNoHeapPointers(ptr, size)</code></span>
<span class="codeline" id="line-361"><code>}</code></span>
<span class="codeline" id="line-362"><code></code></span>
<span class="codeline" id="line-363"><code>// memclrHasPointers clears n bytes of typed memory starting at ptr.</code></span>
<span class="codeline" id="line-364"><code>// The caller must ensure that the type of the object at ptr has</code></span>
<span class="codeline" id="line-365"><code>// pointers, usually by checking typ.PtrBytes. However, ptr</code></span>
<span class="codeline" id="line-366"><code>// does not have to point to the start of the allocation.</code></span>
<span class="codeline" id="line-367"><code>//</code></span>
<span class="codeline" id="line-368"><code>//go:nosplit</code></span>
<span class="codeline" id="line-369"><code>func memclrHasPointers(ptr unsafe.Pointer, n uintptr) {</code></span>
<span class="codeline" id="line-370"><code>	// Pass nil for the type since we don't have one here anyway.</code></span>
<span class="codeline" id="line-371"><code>	bulkBarrierPreWrite(uintptr(ptr), 0, n, nil)</code></span>
<span class="codeline" id="line-372"><code>	memclrNoHeapPointers(ptr, n)</code></span>
<span class="codeline" id="line-373"><code>}</code></span>
</pre><pre id="footer">
<table><tr><td><img src="../../png/go101-twitter.png"></td>
<td>The pages are generated with <a href="https://go101.org/apps-and-libs/golds.html"><b>Golds</b></a> <i>v0.6.8</i>. (GOOS=linux GOARCH=amd64)
<b>Golds</b> is a <a href="https://go101.org">Go 101</a> project developed by <a href="https://tapirgames.com">Tapir Liu</a>.
PR and bug reports are welcome and can be submitted to <a href="https://github.com/go101/golds">the issue list</a>.
Please follow <a href="https://twitter.com/go100and1">@Go100and1</a> (reachable from the left QR code) to get the latest news of <b>Golds</b>.</td></tr></table></pre>