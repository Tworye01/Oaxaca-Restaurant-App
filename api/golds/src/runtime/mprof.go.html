<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Source: mprof.go in package runtime</title>
<link href="../../css/dark-v0.6.8.css" rel="stylesheet">
<script src="../../jvs/golds-v0.6.8.js"></script>
<body onload="onPageLoad()"><div>

<pre id="header"><code><span class="title">Source File</span>
	mprof.go

<span class="title">Belonging Package</span>
	<a href="../../pkg/runtime.html">runtime</a>
</code></pre>

<pre class="line-numbers">
<span class="codeline" id="line-1"><code>// Copyright 2009 The Go Authors. All rights reserved.</code></span>
<span class="codeline" id="line-2"><code>// Use of this source code is governed by a BSD-style</code></span>
<span class="codeline" id="line-3"><code>// license that can be found in the LICENSE file.</code></span>
<span class="codeline" id="line-4"><code></code></span>
<span class="codeline" id="line-5"><code>// Malloc profiling.</code></span>
<span class="codeline" id="line-6"><code>// Patterned after tcmalloc's algorithms; shorter code.</code></span>
<span class="codeline" id="line-7"><code></code></span>
<span class="codeline" id="line-8"><code>package runtime</code></span>
<span class="codeline" id="line-9"><code></code></span>
<span class="codeline" id="line-10"><code>import (</code></span>
<span class="codeline" id="line-11"><code>	"internal/abi"</code></span>
<span class="codeline" id="line-12"><code>	"runtime/internal/atomic"</code></span>
<span class="codeline" id="line-13"><code>	"runtime/internal/sys"</code></span>
<span class="codeline" id="line-14"><code>	"unsafe"</code></span>
<span class="codeline" id="line-15"><code>)</code></span>
<span class="codeline" id="line-16"><code></code></span>
<span class="codeline" id="line-17"><code>// NOTE(rsc): Everything here could use cas if contention became an issue.</code></span>
<span class="codeline" id="line-18"><code>var (</code></span>
<span class="codeline" id="line-19"><code>	// profInsertLock protects changes to the start of all *bucket linked lists</code></span>
<span class="codeline" id="line-20"><code>	profInsertLock mutex</code></span>
<span class="codeline" id="line-21"><code>	// profBlockLock protects the contents of every blockRecord struct</code></span>
<span class="codeline" id="line-22"><code>	profBlockLock mutex</code></span>
<span class="codeline" id="line-23"><code>	// profMemActiveLock protects the active field of every memRecord struct</code></span>
<span class="codeline" id="line-24"><code>	profMemActiveLock mutex</code></span>
<span class="codeline" id="line-25"><code>	// profMemFutureLock is a set of locks that protect the respective elements</code></span>
<span class="codeline" id="line-26"><code>	// of the future array of every memRecord struct</code></span>
<span class="codeline" id="line-27"><code>	profMemFutureLock [len(memRecord{}.future)]mutex</code></span>
<span class="codeline" id="line-28"><code>)</code></span>
<span class="codeline" id="line-29"><code></code></span>
<span class="codeline" id="line-30"><code>// All memory allocations are local and do not escape outside of the profiler.</code></span>
<span class="codeline" id="line-31"><code>// The profiler is forbidden from referring to garbage-collected memory.</code></span>
<span class="codeline" id="line-32"><code></code></span>
<span class="codeline" id="line-33"><code>const (</code></span>
<span class="codeline" id="line-34"><code>	// profile types</code></span>
<span class="codeline" id="line-35"><code>	memProfile bucketType = 1 + iota</code></span>
<span class="codeline" id="line-36"><code>	blockProfile</code></span>
<span class="codeline" id="line-37"><code>	mutexProfile</code></span>
<span class="codeline" id="line-38"><code></code></span>
<span class="codeline" id="line-39"><code>	// size of bucket hash table</code></span>
<span class="codeline" id="line-40"><code>	buckHashSize = 179999</code></span>
<span class="codeline" id="line-41"><code></code></span>
<span class="codeline" id="line-42"><code>	// maxStack is the max depth of stack to record in bucket.</code></span>
<span class="codeline" id="line-43"><code>	// Note that it's only used internally as a guard against</code></span>
<span class="codeline" id="line-44"><code>	// wildly out-of-bounds slicing of the PCs that come after</code></span>
<span class="codeline" id="line-45"><code>	// a bucket struct, and it could increase in the future.</code></span>
<span class="codeline" id="line-46"><code>	maxStack = 32</code></span>
<span class="codeline" id="line-47"><code>)</code></span>
<span class="codeline" id="line-48"><code></code></span>
<span class="codeline" id="line-49"><code>type bucketType int</code></span>
<span class="codeline" id="line-50"><code></code></span>
<span class="codeline" id="line-51"><code>// A bucket holds per-call-stack profiling information.</code></span>
<span class="codeline" id="line-52"><code>// The representation is a bit sleazy, inherited from C.</code></span>
<span class="codeline" id="line-53"><code>// This struct defines the bucket header. It is followed in</code></span>
<span class="codeline" id="line-54"><code>// memory by the stack words and then the actual record</code></span>
<span class="codeline" id="line-55"><code>// data, either a memRecord or a blockRecord.</code></span>
<span class="codeline" id="line-56"><code>//</code></span>
<span class="codeline" id="line-57"><code>// Per-call-stack profiling information.</code></span>
<span class="codeline" id="line-58"><code>// Lookup by hashing call stack into a linked-list hash table.</code></span>
<span class="codeline" id="line-59"><code>//</code></span>
<span class="codeline" id="line-60"><code>// None of the fields in this bucket header are modified after</code></span>
<span class="codeline" id="line-61"><code>// creation, including its next and allnext links.</code></span>
<span class="codeline" id="line-62"><code>//</code></span>
<span class="codeline" id="line-63"><code>// No heap pointers.</code></span>
<span class="codeline" id="line-64"><code>type bucket struct {</code></span>
<span class="codeline" id="line-65"><code>	_       sys.NotInHeap</code></span>
<span class="codeline" id="line-66"><code>	next    *bucket</code></span>
<span class="codeline" id="line-67"><code>	allnext *bucket</code></span>
<span class="codeline" id="line-68"><code>	typ     bucketType // memBucket or blockBucket (includes mutexProfile)</code></span>
<span class="codeline" id="line-69"><code>	hash    uintptr</code></span>
<span class="codeline" id="line-70"><code>	size    uintptr</code></span>
<span class="codeline" id="line-71"><code>	nstk    uintptr</code></span>
<span class="codeline" id="line-72"><code>}</code></span>
<span class="codeline" id="line-73"><code></code></span>
<span class="codeline" id="line-74"><code>// A memRecord is the bucket data for a bucket of type memProfile,</code></span>
<span class="codeline" id="line-75"><code>// part of the memory profile.</code></span>
<span class="codeline" id="line-76"><code>type memRecord struct {</code></span>
<span class="codeline" id="line-77"><code>	// The following complex 3-stage scheme of stats accumulation</code></span>
<span class="codeline" id="line-78"><code>	// is required to obtain a consistent picture of mallocs and frees</code></span>
<span class="codeline" id="line-79"><code>	// for some point in time.</code></span>
<span class="codeline" id="line-80"><code>	// The problem is that mallocs come in real time, while frees</code></span>
<span class="codeline" id="line-81"><code>	// come only after a GC during concurrent sweeping. So if we would</code></span>
<span class="codeline" id="line-82"><code>	// naively count them, we would get a skew toward mallocs.</code></span>
<span class="codeline" id="line-83"><code>	//</code></span>
<span class="codeline" id="line-84"><code>	// Hence, we delay information to get consistent snapshots as</code></span>
<span class="codeline" id="line-85"><code>	// of mark termination. Allocations count toward the next mark</code></span>
<span class="codeline" id="line-86"><code>	// termination's snapshot, while sweep frees count toward the</code></span>
<span class="codeline" id="line-87"><code>	// previous mark termination's snapshot:</code></span>
<span class="codeline" id="line-88"><code>	//</code></span>
<span class="codeline" id="line-89"><code>	//              MT          MT          MT          MT</code></span>
<span class="codeline" id="line-90"><code>	//             .·|         .·|         .·|         .·|</code></span>
<span class="codeline" id="line-91"><code>	//          .·˙  |      .·˙  |      .·˙  |      .·˙  |</code></span>
<span class="codeline" id="line-92"><code>	//       .·˙     |   .·˙     |   .·˙     |   .·˙     |</code></span>
<span class="codeline" id="line-93"><code>	//    .·˙        |.·˙        |.·˙        |.·˙        |</code></span>
<span class="codeline" id="line-94"><code>	//</code></span>
<span class="codeline" id="line-95"><code>	//       alloc → ▲ ← free</code></span>
<span class="codeline" id="line-96"><code>	//               ┠┅┅┅┅┅┅┅┅┅┅┅P</code></span>
<span class="codeline" id="line-97"><code>	//       C+2     →    C+1    →  C</code></span>
<span class="codeline" id="line-98"><code>	//</code></span>
<span class="codeline" id="line-99"><code>	//                   alloc → ▲ ← free</code></span>
<span class="codeline" id="line-100"><code>	//                           ┠┅┅┅┅┅┅┅┅┅┅┅P</code></span>
<span class="codeline" id="line-101"><code>	//                   C+2     →    C+1    →  C</code></span>
<span class="codeline" id="line-102"><code>	//</code></span>
<span class="codeline" id="line-103"><code>	// Since we can't publish a consistent snapshot until all of</code></span>
<span class="codeline" id="line-104"><code>	// the sweep frees are accounted for, we wait until the next</code></span>
<span class="codeline" id="line-105"><code>	// mark termination ("MT" above) to publish the previous mark</code></span>
<span class="codeline" id="line-106"><code>	// termination's snapshot ("P" above). To do this, allocation</code></span>
<span class="codeline" id="line-107"><code>	// and free events are accounted to *future* heap profile</code></span>
<span class="codeline" id="line-108"><code>	// cycles ("C+n" above) and we only publish a cycle once all</code></span>
<span class="codeline" id="line-109"><code>	// of the events from that cycle must be done. Specifically:</code></span>
<span class="codeline" id="line-110"><code>	//</code></span>
<span class="codeline" id="line-111"><code>	// Mallocs are accounted to cycle C+2.</code></span>
<span class="codeline" id="line-112"><code>	// Explicit frees are accounted to cycle C+2.</code></span>
<span class="codeline" id="line-113"><code>	// GC frees (done during sweeping) are accounted to cycle C+1.</code></span>
<span class="codeline" id="line-114"><code>	//</code></span>
<span class="codeline" id="line-115"><code>	// After mark termination, we increment the global heap</code></span>
<span class="codeline" id="line-116"><code>	// profile cycle counter and accumulate the stats from cycle C</code></span>
<span class="codeline" id="line-117"><code>	// into the active profile.</code></span>
<span class="codeline" id="line-118"><code></code></span>
<span class="codeline" id="line-119"><code>	// active is the currently published profile. A profiling</code></span>
<span class="codeline" id="line-120"><code>	// cycle can be accumulated into active once its complete.</code></span>
<span class="codeline" id="line-121"><code>	active memRecordCycle</code></span>
<span class="codeline" id="line-122"><code></code></span>
<span class="codeline" id="line-123"><code>	// future records the profile events we're counting for cycles</code></span>
<span class="codeline" id="line-124"><code>	// that have not yet been published. This is ring buffer</code></span>
<span class="codeline" id="line-125"><code>	// indexed by the global heap profile cycle C and stores</code></span>
<span class="codeline" id="line-126"><code>	// cycles C, C+1, and C+2. Unlike active, these counts are</code></span>
<span class="codeline" id="line-127"><code>	// only for a single cycle; they are not cumulative across</code></span>
<span class="codeline" id="line-128"><code>	// cycles.</code></span>
<span class="codeline" id="line-129"><code>	//</code></span>
<span class="codeline" id="line-130"><code>	// We store cycle C here because there's a window between when</code></span>
<span class="codeline" id="line-131"><code>	// C becomes the active cycle and when we've flushed it to</code></span>
<span class="codeline" id="line-132"><code>	// active.</code></span>
<span class="codeline" id="line-133"><code>	future [3]memRecordCycle</code></span>
<span class="codeline" id="line-134"><code>}</code></span>
<span class="codeline" id="line-135"><code></code></span>
<span class="codeline" id="line-136"><code>// memRecordCycle</code></span>
<span class="codeline" id="line-137"><code>type memRecordCycle struct {</code></span>
<span class="codeline" id="line-138"><code>	allocs, frees           uintptr</code></span>
<span class="codeline" id="line-139"><code>	alloc_bytes, free_bytes uintptr</code></span>
<span class="codeline" id="line-140"><code>}</code></span>
<span class="codeline" id="line-141"><code></code></span>
<span class="codeline" id="line-142"><code>// add accumulates b into a. It does not zero b.</code></span>
<span class="codeline" id="line-143"><code>func (a *memRecordCycle) add(b *memRecordCycle) {</code></span>
<span class="codeline" id="line-144"><code>	a.allocs += b.allocs</code></span>
<span class="codeline" id="line-145"><code>	a.frees += b.frees</code></span>
<span class="codeline" id="line-146"><code>	a.alloc_bytes += b.alloc_bytes</code></span>
<span class="codeline" id="line-147"><code>	a.free_bytes += b.free_bytes</code></span>
<span class="codeline" id="line-148"><code>}</code></span>
<span class="codeline" id="line-149"><code></code></span>
<span class="codeline" id="line-150"><code>// A blockRecord is the bucket data for a bucket of type blockProfile,</code></span>
<span class="codeline" id="line-151"><code>// which is used in blocking and mutex profiles.</code></span>
<span class="codeline" id="line-152"><code>type blockRecord struct {</code></span>
<span class="codeline" id="line-153"><code>	count  float64</code></span>
<span class="codeline" id="line-154"><code>	cycles int64</code></span>
<span class="codeline" id="line-155"><code>}</code></span>
<span class="codeline" id="line-156"><code></code></span>
<span class="codeline" id="line-157"><code>var (</code></span>
<span class="codeline" id="line-158"><code>	mbuckets atomic.UnsafePointer // *bucket, memory profile buckets</code></span>
<span class="codeline" id="line-159"><code>	bbuckets atomic.UnsafePointer // *bucket, blocking profile buckets</code></span>
<span class="codeline" id="line-160"><code>	xbuckets atomic.UnsafePointer // *bucket, mutex profile buckets</code></span>
<span class="codeline" id="line-161"><code>	buckhash atomic.UnsafePointer // *buckhashArray</code></span>
<span class="codeline" id="line-162"><code></code></span>
<span class="codeline" id="line-163"><code>	mProfCycle mProfCycleHolder</code></span>
<span class="codeline" id="line-164"><code>)</code></span>
<span class="codeline" id="line-165"><code></code></span>
<span class="codeline" id="line-166"><code>type buckhashArray [buckHashSize]atomic.UnsafePointer // *bucket</code></span>
<span class="codeline" id="line-167"><code></code></span>
<span class="codeline" id="line-168"><code>const mProfCycleWrap = uint32(len(memRecord{}.future)) * (2 &lt;&lt; 24)</code></span>
<span class="codeline" id="line-169"><code></code></span>
<span class="codeline" id="line-170"><code>// mProfCycleHolder holds the global heap profile cycle number (wrapped at</code></span>
<span class="codeline" id="line-171"><code>// mProfCycleWrap, stored starting at bit 1), and a flag (stored at bit 0) to</code></span>
<span class="codeline" id="line-172"><code>// indicate whether future[cycle] in all buckets has been queued to flush into</code></span>
<span class="codeline" id="line-173"><code>// the active profile.</code></span>
<span class="codeline" id="line-174"><code>type mProfCycleHolder struct {</code></span>
<span class="codeline" id="line-175"><code>	value atomic.Uint32</code></span>
<span class="codeline" id="line-176"><code>}</code></span>
<span class="codeline" id="line-177"><code></code></span>
<span class="codeline" id="line-178"><code>// read returns the current cycle count.</code></span>
<span class="codeline" id="line-179"><code>func (c *mProfCycleHolder) read() (cycle uint32) {</code></span>
<span class="codeline" id="line-180"><code>	v := c.value.Load()</code></span>
<span class="codeline" id="line-181"><code>	cycle = v &gt;&gt; 1</code></span>
<span class="codeline" id="line-182"><code>	return cycle</code></span>
<span class="codeline" id="line-183"><code>}</code></span>
<span class="codeline" id="line-184"><code></code></span>
<span class="codeline" id="line-185"><code>// setFlushed sets the flushed flag. It returns the current cycle count and the</code></span>
<span class="codeline" id="line-186"><code>// previous value of the flushed flag.</code></span>
<span class="codeline" id="line-187"><code>func (c *mProfCycleHolder) setFlushed() (cycle uint32, alreadyFlushed bool) {</code></span>
<span class="codeline" id="line-188"><code>	for {</code></span>
<span class="codeline" id="line-189"><code>		prev := c.value.Load()</code></span>
<span class="codeline" id="line-190"><code>		cycle = prev &gt;&gt; 1</code></span>
<span class="codeline" id="line-191"><code>		alreadyFlushed = (prev &amp; 0x1) != 0</code></span>
<span class="codeline" id="line-192"><code>		next := prev | 0x1</code></span>
<span class="codeline" id="line-193"><code>		if c.value.CompareAndSwap(prev, next) {</code></span>
<span class="codeline" id="line-194"><code>			return cycle, alreadyFlushed</code></span>
<span class="codeline" id="line-195"><code>		}</code></span>
<span class="codeline" id="line-196"><code>	}</code></span>
<span class="codeline" id="line-197"><code>}</code></span>
<span class="codeline" id="line-198"><code></code></span>
<span class="codeline" id="line-199"><code>// increment increases the cycle count by one, wrapping the value at</code></span>
<span class="codeline" id="line-200"><code>// mProfCycleWrap. It clears the flushed flag.</code></span>
<span class="codeline" id="line-201"><code>func (c *mProfCycleHolder) increment() {</code></span>
<span class="codeline" id="line-202"><code>	// We explicitly wrap mProfCycle rather than depending on</code></span>
<span class="codeline" id="line-203"><code>	// uint wraparound because the memRecord.future ring does not</code></span>
<span class="codeline" id="line-204"><code>	// itself wrap at a power of two.</code></span>
<span class="codeline" id="line-205"><code>	for {</code></span>
<span class="codeline" id="line-206"><code>		prev := c.value.Load()</code></span>
<span class="codeline" id="line-207"><code>		cycle := prev &gt;&gt; 1</code></span>
<span class="codeline" id="line-208"><code>		cycle = (cycle + 1) % mProfCycleWrap</code></span>
<span class="codeline" id="line-209"><code>		next := cycle &lt;&lt; 1</code></span>
<span class="codeline" id="line-210"><code>		if c.value.CompareAndSwap(prev, next) {</code></span>
<span class="codeline" id="line-211"><code>			break</code></span>
<span class="codeline" id="line-212"><code>		}</code></span>
<span class="codeline" id="line-213"><code>	}</code></span>
<span class="codeline" id="line-214"><code>}</code></span>
<span class="codeline" id="line-215"><code></code></span>
<span class="codeline" id="line-216"><code>// newBucket allocates a bucket with the given type and number of stack entries.</code></span>
<span class="codeline" id="line-217"><code>func newBucket(typ bucketType, nstk int) *bucket {</code></span>
<span class="codeline" id="line-218"><code>	size := unsafe.Sizeof(bucket{}) + uintptr(nstk)*unsafe.Sizeof(uintptr(0))</code></span>
<span class="codeline" id="line-219"><code>	switch typ {</code></span>
<span class="codeline" id="line-220"><code>	default:</code></span>
<span class="codeline" id="line-221"><code>		throw("invalid profile bucket type")</code></span>
<span class="codeline" id="line-222"><code>	case memProfile:</code></span>
<span class="codeline" id="line-223"><code>		size += unsafe.Sizeof(memRecord{})</code></span>
<span class="codeline" id="line-224"><code>	case blockProfile, mutexProfile:</code></span>
<span class="codeline" id="line-225"><code>		size += unsafe.Sizeof(blockRecord{})</code></span>
<span class="codeline" id="line-226"><code>	}</code></span>
<span class="codeline" id="line-227"><code></code></span>
<span class="codeline" id="line-228"><code>	b := (*bucket)(persistentalloc(size, 0, &amp;memstats.buckhash_sys))</code></span>
<span class="codeline" id="line-229"><code>	b.typ = typ</code></span>
<span class="codeline" id="line-230"><code>	b.nstk = uintptr(nstk)</code></span>
<span class="codeline" id="line-231"><code>	return b</code></span>
<span class="codeline" id="line-232"><code>}</code></span>
<span class="codeline" id="line-233"><code></code></span>
<span class="codeline" id="line-234"><code>// stk returns the slice in b holding the stack.</code></span>
<span class="codeline" id="line-235"><code>func (b *bucket) stk() []uintptr {</code></span>
<span class="codeline" id="line-236"><code>	stk := (*[maxStack]uintptr)(add(unsafe.Pointer(b), unsafe.Sizeof(*b)))</code></span>
<span class="codeline" id="line-237"><code>	if b.nstk &gt; maxStack {</code></span>
<span class="codeline" id="line-238"><code>		// prove that slicing works; otherwise a failure requires a P</code></span>
<span class="codeline" id="line-239"><code>		throw("bad profile stack count")</code></span>
<span class="codeline" id="line-240"><code>	}</code></span>
<span class="codeline" id="line-241"><code>	return stk[:b.nstk:b.nstk]</code></span>
<span class="codeline" id="line-242"><code>}</code></span>
<span class="codeline" id="line-243"><code></code></span>
<span class="codeline" id="line-244"><code>// mp returns the memRecord associated with the memProfile bucket b.</code></span>
<span class="codeline" id="line-245"><code>func (b *bucket) mp() *memRecord {</code></span>
<span class="codeline" id="line-246"><code>	if b.typ != memProfile {</code></span>
<span class="codeline" id="line-247"><code>		throw("bad use of bucket.mp")</code></span>
<span class="codeline" id="line-248"><code>	}</code></span>
<span class="codeline" id="line-249"><code>	data := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(uintptr(0)))</code></span>
<span class="codeline" id="line-250"><code>	return (*memRecord)(data)</code></span>
<span class="codeline" id="line-251"><code>}</code></span>
<span class="codeline" id="line-252"><code></code></span>
<span class="codeline" id="line-253"><code>// bp returns the blockRecord associated with the blockProfile bucket b.</code></span>
<span class="codeline" id="line-254"><code>func (b *bucket) bp() *blockRecord {</code></span>
<span class="codeline" id="line-255"><code>	if b.typ != blockProfile &amp;&amp; b.typ != mutexProfile {</code></span>
<span class="codeline" id="line-256"><code>		throw("bad use of bucket.bp")</code></span>
<span class="codeline" id="line-257"><code>	}</code></span>
<span class="codeline" id="line-258"><code>	data := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(uintptr(0)))</code></span>
<span class="codeline" id="line-259"><code>	return (*blockRecord)(data)</code></span>
<span class="codeline" id="line-260"><code>}</code></span>
<span class="codeline" id="line-261"><code></code></span>
<span class="codeline" id="line-262"><code>// Return the bucket for stk[0:nstk], allocating new bucket if needed.</code></span>
<span class="codeline" id="line-263"><code>func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket {</code></span>
<span class="codeline" id="line-264"><code>	bh := (*buckhashArray)(buckhash.Load())</code></span>
<span class="codeline" id="line-265"><code>	if bh == nil {</code></span>
<span class="codeline" id="line-266"><code>		lock(&amp;profInsertLock)</code></span>
<span class="codeline" id="line-267"><code>		// check again under the lock</code></span>
<span class="codeline" id="line-268"><code>		bh = (*buckhashArray)(buckhash.Load())</code></span>
<span class="codeline" id="line-269"><code>		if bh == nil {</code></span>
<span class="codeline" id="line-270"><code>			bh = (*buckhashArray)(sysAlloc(unsafe.Sizeof(buckhashArray{}), &amp;memstats.buckhash_sys))</code></span>
<span class="codeline" id="line-271"><code>			if bh == nil {</code></span>
<span class="codeline" id="line-272"><code>				throw("runtime: cannot allocate memory")</code></span>
<span class="codeline" id="line-273"><code>			}</code></span>
<span class="codeline" id="line-274"><code>			buckhash.StoreNoWB(unsafe.Pointer(bh))</code></span>
<span class="codeline" id="line-275"><code>		}</code></span>
<span class="codeline" id="line-276"><code>		unlock(&amp;profInsertLock)</code></span>
<span class="codeline" id="line-277"><code>	}</code></span>
<span class="codeline" id="line-278"><code></code></span>
<span class="codeline" id="line-279"><code>	// Hash stack.</code></span>
<span class="codeline" id="line-280"><code>	var h uintptr</code></span>
<span class="codeline" id="line-281"><code>	for _, pc := range stk {</code></span>
<span class="codeline" id="line-282"><code>		h += pc</code></span>
<span class="codeline" id="line-283"><code>		h += h &lt;&lt; 10</code></span>
<span class="codeline" id="line-284"><code>		h ^= h &gt;&gt; 6</code></span>
<span class="codeline" id="line-285"><code>	}</code></span>
<span class="codeline" id="line-286"><code>	// hash in size</code></span>
<span class="codeline" id="line-287"><code>	h += size</code></span>
<span class="codeline" id="line-288"><code>	h += h &lt;&lt; 10</code></span>
<span class="codeline" id="line-289"><code>	h ^= h &gt;&gt; 6</code></span>
<span class="codeline" id="line-290"><code>	// finalize</code></span>
<span class="codeline" id="line-291"><code>	h += h &lt;&lt; 3</code></span>
<span class="codeline" id="line-292"><code>	h ^= h &gt;&gt; 11</code></span>
<span class="codeline" id="line-293"><code></code></span>
<span class="codeline" id="line-294"><code>	i := int(h % buckHashSize)</code></span>
<span class="codeline" id="line-295"><code>	// first check optimistically, without the lock</code></span>
<span class="codeline" id="line-296"><code>	for b := (*bucket)(bh[i].Load()); b != nil; b = b.next {</code></span>
<span class="codeline" id="line-297"><code>		if b.typ == typ &amp;&amp; b.hash == h &amp;&amp; b.size == size &amp;&amp; eqslice(b.stk(), stk) {</code></span>
<span class="codeline" id="line-298"><code>			return b</code></span>
<span class="codeline" id="line-299"><code>		}</code></span>
<span class="codeline" id="line-300"><code>	}</code></span>
<span class="codeline" id="line-301"><code></code></span>
<span class="codeline" id="line-302"><code>	if !alloc {</code></span>
<span class="codeline" id="line-303"><code>		return nil</code></span>
<span class="codeline" id="line-304"><code>	}</code></span>
<span class="codeline" id="line-305"><code></code></span>
<span class="codeline" id="line-306"><code>	lock(&amp;profInsertLock)</code></span>
<span class="codeline" id="line-307"><code>	// check again under the insertion lock</code></span>
<span class="codeline" id="line-308"><code>	for b := (*bucket)(bh[i].Load()); b != nil; b = b.next {</code></span>
<span class="codeline" id="line-309"><code>		if b.typ == typ &amp;&amp; b.hash == h &amp;&amp; b.size == size &amp;&amp; eqslice(b.stk(), stk) {</code></span>
<span class="codeline" id="line-310"><code>			unlock(&amp;profInsertLock)</code></span>
<span class="codeline" id="line-311"><code>			return b</code></span>
<span class="codeline" id="line-312"><code>		}</code></span>
<span class="codeline" id="line-313"><code>	}</code></span>
<span class="codeline" id="line-314"><code></code></span>
<span class="codeline" id="line-315"><code>	// Create new bucket.</code></span>
<span class="codeline" id="line-316"><code>	b := newBucket(typ, len(stk))</code></span>
<span class="codeline" id="line-317"><code>	copy(b.stk(), stk)</code></span>
<span class="codeline" id="line-318"><code>	b.hash = h</code></span>
<span class="codeline" id="line-319"><code>	b.size = size</code></span>
<span class="codeline" id="line-320"><code></code></span>
<span class="codeline" id="line-321"><code>	var allnext *atomic.UnsafePointer</code></span>
<span class="codeline" id="line-322"><code>	if typ == memProfile {</code></span>
<span class="codeline" id="line-323"><code>		allnext = &amp;mbuckets</code></span>
<span class="codeline" id="line-324"><code>	} else if typ == mutexProfile {</code></span>
<span class="codeline" id="line-325"><code>		allnext = &amp;xbuckets</code></span>
<span class="codeline" id="line-326"><code>	} else {</code></span>
<span class="codeline" id="line-327"><code>		allnext = &amp;bbuckets</code></span>
<span class="codeline" id="line-328"><code>	}</code></span>
<span class="codeline" id="line-329"><code></code></span>
<span class="codeline" id="line-330"><code>	b.next = (*bucket)(bh[i].Load())</code></span>
<span class="codeline" id="line-331"><code>	b.allnext = (*bucket)(allnext.Load())</code></span>
<span class="codeline" id="line-332"><code></code></span>
<span class="codeline" id="line-333"><code>	bh[i].StoreNoWB(unsafe.Pointer(b))</code></span>
<span class="codeline" id="line-334"><code>	allnext.StoreNoWB(unsafe.Pointer(b))</code></span>
<span class="codeline" id="line-335"><code></code></span>
<span class="codeline" id="line-336"><code>	unlock(&amp;profInsertLock)</code></span>
<span class="codeline" id="line-337"><code>	return b</code></span>
<span class="codeline" id="line-338"><code>}</code></span>
<span class="codeline" id="line-339"><code></code></span>
<span class="codeline" id="line-340"><code>func eqslice(x, y []uintptr) bool {</code></span>
<span class="codeline" id="line-341"><code>	if len(x) != len(y) {</code></span>
<span class="codeline" id="line-342"><code>		return false</code></span>
<span class="codeline" id="line-343"><code>	}</code></span>
<span class="codeline" id="line-344"><code>	for i, xi := range x {</code></span>
<span class="codeline" id="line-345"><code>		if xi != y[i] {</code></span>
<span class="codeline" id="line-346"><code>			return false</code></span>
<span class="codeline" id="line-347"><code>		}</code></span>
<span class="codeline" id="line-348"><code>	}</code></span>
<span class="codeline" id="line-349"><code>	return true</code></span>
<span class="codeline" id="line-350"><code>}</code></span>
<span class="codeline" id="line-351"><code></code></span>
<span class="codeline" id="line-352"><code>// mProf_NextCycle publishes the next heap profile cycle and creates a</code></span>
<span class="codeline" id="line-353"><code>// fresh heap profile cycle. This operation is fast and can be done</code></span>
<span class="codeline" id="line-354"><code>// during STW. The caller must call mProf_Flush before calling</code></span>
<span class="codeline" id="line-355"><code>// mProf_NextCycle again.</code></span>
<span class="codeline" id="line-356"><code>//</code></span>
<span class="codeline" id="line-357"><code>// This is called by mark termination during STW so allocations and</code></span>
<span class="codeline" id="line-358"><code>// frees after the world is started again count towards a new heap</code></span>
<span class="codeline" id="line-359"><code>// profiling cycle.</code></span>
<span class="codeline" id="line-360"><code>func mProf_NextCycle() {</code></span>
<span class="codeline" id="line-361"><code>	mProfCycle.increment()</code></span>
<span class="codeline" id="line-362"><code>}</code></span>
<span class="codeline" id="line-363"><code></code></span>
<span class="codeline" id="line-364"><code>// mProf_Flush flushes the events from the current heap profiling</code></span>
<span class="codeline" id="line-365"><code>// cycle into the active profile. After this it is safe to start a new</code></span>
<span class="codeline" id="line-366"><code>// heap profiling cycle with mProf_NextCycle.</code></span>
<span class="codeline" id="line-367"><code>//</code></span>
<span class="codeline" id="line-368"><code>// This is called by GC after mark termination starts the world. In</code></span>
<span class="codeline" id="line-369"><code>// contrast with mProf_NextCycle, this is somewhat expensive, but safe</code></span>
<span class="codeline" id="line-370"><code>// to do concurrently.</code></span>
<span class="codeline" id="line-371"><code>func mProf_Flush() {</code></span>
<span class="codeline" id="line-372"><code>	cycle, alreadyFlushed := mProfCycle.setFlushed()</code></span>
<span class="codeline" id="line-373"><code>	if alreadyFlushed {</code></span>
<span class="codeline" id="line-374"><code>		return</code></span>
<span class="codeline" id="line-375"><code>	}</code></span>
<span class="codeline" id="line-376"><code></code></span>
<span class="codeline" id="line-377"><code>	index := cycle % uint32(len(memRecord{}.future))</code></span>
<span class="codeline" id="line-378"><code>	lock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-379"><code>	lock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-380"><code>	mProf_FlushLocked(index)</code></span>
<span class="codeline" id="line-381"><code>	unlock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-382"><code>	unlock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-383"><code>}</code></span>
<span class="codeline" id="line-384"><code></code></span>
<span class="codeline" id="line-385"><code>// mProf_FlushLocked flushes the events from the heap profiling cycle at index</code></span>
<span class="codeline" id="line-386"><code>// into the active profile. The caller must hold the lock for the active profile</code></span>
<span class="codeline" id="line-387"><code>// (profMemActiveLock) and for the profiling cycle at index</code></span>
<span class="codeline" id="line-388"><code>// (profMemFutureLock[index]).</code></span>
<span class="codeline" id="line-389"><code>func mProf_FlushLocked(index uint32) {</code></span>
<span class="codeline" id="line-390"><code>	assertLockHeld(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-391"><code>	assertLockHeld(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-392"><code>	head := (*bucket)(mbuckets.Load())</code></span>
<span class="codeline" id="line-393"><code>	for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-394"><code>		mp := b.mp()</code></span>
<span class="codeline" id="line-395"><code></code></span>
<span class="codeline" id="line-396"><code>		// Flush cycle C into the published profile and clear</code></span>
<span class="codeline" id="line-397"><code>		// it for reuse.</code></span>
<span class="codeline" id="line-398"><code>		mpc := &amp;mp.future[index]</code></span>
<span class="codeline" id="line-399"><code>		mp.active.add(mpc)</code></span>
<span class="codeline" id="line-400"><code>		*mpc = memRecordCycle{}</code></span>
<span class="codeline" id="line-401"><code>	}</code></span>
<span class="codeline" id="line-402"><code>}</code></span>
<span class="codeline" id="line-403"><code></code></span>
<span class="codeline" id="line-404"><code>// mProf_PostSweep records that all sweep frees for this GC cycle have</code></span>
<span class="codeline" id="line-405"><code>// completed. This has the effect of publishing the heap profile</code></span>
<span class="codeline" id="line-406"><code>// snapshot as of the last mark termination without advancing the heap</code></span>
<span class="codeline" id="line-407"><code>// profile cycle.</code></span>
<span class="codeline" id="line-408"><code>func mProf_PostSweep() {</code></span>
<span class="codeline" id="line-409"><code>	// Flush cycle C+1 to the active profile so everything as of</code></span>
<span class="codeline" id="line-410"><code>	// the last mark termination becomes visible. *Don't* advance</code></span>
<span class="codeline" id="line-411"><code>	// the cycle, since we're still accumulating allocs in cycle</code></span>
<span class="codeline" id="line-412"><code>	// C+2, which have to become C+1 in the next mark termination</code></span>
<span class="codeline" id="line-413"><code>	// and so on.</code></span>
<span class="codeline" id="line-414"><code>	cycle := mProfCycle.read() + 1</code></span>
<span class="codeline" id="line-415"><code></code></span>
<span class="codeline" id="line-416"><code>	index := cycle % uint32(len(memRecord{}.future))</code></span>
<span class="codeline" id="line-417"><code>	lock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-418"><code>	lock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-419"><code>	mProf_FlushLocked(index)</code></span>
<span class="codeline" id="line-420"><code>	unlock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-421"><code>	unlock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-422"><code>}</code></span>
<span class="codeline" id="line-423"><code></code></span>
<span class="codeline" id="line-424"><code>// Called by malloc to record a profiled block.</code></span>
<span class="codeline" id="line-425"><code>func mProf_Malloc(p unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-426"><code>	var stk [maxStack]uintptr</code></span>
<span class="codeline" id="line-427"><code>	nstk := callers(4, stk[:])</code></span>
<span class="codeline" id="line-428"><code></code></span>
<span class="codeline" id="line-429"><code>	index := (mProfCycle.read() + 2) % uint32(len(memRecord{}.future))</code></span>
<span class="codeline" id="line-430"><code></code></span>
<span class="codeline" id="line-431"><code>	b := stkbucket(memProfile, size, stk[:nstk], true)</code></span>
<span class="codeline" id="line-432"><code>	mp := b.mp()</code></span>
<span class="codeline" id="line-433"><code>	mpc := &amp;mp.future[index]</code></span>
<span class="codeline" id="line-434"><code></code></span>
<span class="codeline" id="line-435"><code>	lock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-436"><code>	mpc.allocs++</code></span>
<span class="codeline" id="line-437"><code>	mpc.alloc_bytes += size</code></span>
<span class="codeline" id="line-438"><code>	unlock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-439"><code></code></span>
<span class="codeline" id="line-440"><code>	// Setprofilebucket locks a bunch of other mutexes, so we call it outside of</code></span>
<span class="codeline" id="line-441"><code>	// the profiler locks. This reduces potential contention and chances of</code></span>
<span class="codeline" id="line-442"><code>	// deadlocks. Since the object must be alive during the call to</code></span>
<span class="codeline" id="line-443"><code>	// mProf_Malloc, it's fine to do this non-atomically.</code></span>
<span class="codeline" id="line-444"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-445"><code>		setprofilebucket(p, b)</code></span>
<span class="codeline" id="line-446"><code>	})</code></span>
<span class="codeline" id="line-447"><code>}</code></span>
<span class="codeline" id="line-448"><code></code></span>
<span class="codeline" id="line-449"><code>// Called when freeing a profiled block.</code></span>
<span class="codeline" id="line-450"><code>func mProf_Free(b *bucket, size uintptr) {</code></span>
<span class="codeline" id="line-451"><code>	index := (mProfCycle.read() + 1) % uint32(len(memRecord{}.future))</code></span>
<span class="codeline" id="line-452"><code></code></span>
<span class="codeline" id="line-453"><code>	mp := b.mp()</code></span>
<span class="codeline" id="line-454"><code>	mpc := &amp;mp.future[index]</code></span>
<span class="codeline" id="line-455"><code></code></span>
<span class="codeline" id="line-456"><code>	lock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-457"><code>	mpc.frees++</code></span>
<span class="codeline" id="line-458"><code>	mpc.free_bytes += size</code></span>
<span class="codeline" id="line-459"><code>	unlock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-460"><code>}</code></span>
<span class="codeline" id="line-461"><code></code></span>
<span class="codeline" id="line-462"><code>var blockprofilerate uint64 // in CPU ticks</code></span>
<span class="codeline" id="line-463"><code></code></span>
<span class="codeline" id="line-464"><code>// SetBlockProfileRate controls the fraction of goroutine blocking events</code></span>
<span class="codeline" id="line-465"><code>// that are reported in the blocking profile. The profiler aims to sample</code></span>
<span class="codeline" id="line-466"><code>// an average of one blocking event per rate nanoseconds spent blocked.</code></span>
<span class="codeline" id="line-467"><code>//</code></span>
<span class="codeline" id="line-468"><code>// To include every blocking event in the profile, pass rate = 1.</code></span>
<span class="codeline" id="line-469"><code>// To turn off profiling entirely, pass rate &lt;= 0.</code></span>
<span class="codeline" id="line-470"><code>func SetBlockProfileRate(rate int) {</code></span>
<span class="codeline" id="line-471"><code>	var r int64</code></span>
<span class="codeline" id="line-472"><code>	if rate &lt;= 0 {</code></span>
<span class="codeline" id="line-473"><code>		r = 0 // disable profiling</code></span>
<span class="codeline" id="line-474"><code>	} else if rate == 1 {</code></span>
<span class="codeline" id="line-475"><code>		r = 1 // profile everything</code></span>
<span class="codeline" id="line-476"><code>	} else {</code></span>
<span class="codeline" id="line-477"><code>		// convert ns to cycles, use float64 to prevent overflow during multiplication</code></span>
<span class="codeline" id="line-478"><code>		r = int64(float64(rate) * float64(ticksPerSecond()) / (1000 * 1000 * 1000))</code></span>
<span class="codeline" id="line-479"><code>		if r == 0 {</code></span>
<span class="codeline" id="line-480"><code>			r = 1</code></span>
<span class="codeline" id="line-481"><code>		}</code></span>
<span class="codeline" id="line-482"><code>	}</code></span>
<span class="codeline" id="line-483"><code></code></span>
<span class="codeline" id="line-484"><code>	atomic.Store64(&amp;blockprofilerate, uint64(r))</code></span>
<span class="codeline" id="line-485"><code>}</code></span>
<span class="codeline" id="line-486"><code></code></span>
<span class="codeline" id="line-487"><code>func blockevent(cycles int64, skip int) {</code></span>
<span class="codeline" id="line-488"><code>	if cycles &lt;= 0 {</code></span>
<span class="codeline" id="line-489"><code>		cycles = 1</code></span>
<span class="codeline" id="line-490"><code>	}</code></span>
<span class="codeline" id="line-491"><code></code></span>
<span class="codeline" id="line-492"><code>	rate := int64(atomic.Load64(&amp;blockprofilerate))</code></span>
<span class="codeline" id="line-493"><code>	if blocksampled(cycles, rate) {</code></span>
<span class="codeline" id="line-494"><code>		saveblockevent(cycles, rate, skip+1, blockProfile)</code></span>
<span class="codeline" id="line-495"><code>	}</code></span>
<span class="codeline" id="line-496"><code>}</code></span>
<span class="codeline" id="line-497"><code></code></span>
<span class="codeline" id="line-498"><code>// blocksampled returns true for all events where cycles &gt;= rate. Shorter</code></span>
<span class="codeline" id="line-499"><code>// events have a cycles/rate random chance of returning true.</code></span>
<span class="codeline" id="line-500"><code>func blocksampled(cycles, rate int64) bool {</code></span>
<span class="codeline" id="line-501"><code>	if rate &lt;= 0 || (rate &gt; cycles &amp;&amp; cheaprand64()%rate &gt; cycles) {</code></span>
<span class="codeline" id="line-502"><code>		return false</code></span>
<span class="codeline" id="line-503"><code>	}</code></span>
<span class="codeline" id="line-504"><code>	return true</code></span>
<span class="codeline" id="line-505"><code>}</code></span>
<span class="codeline" id="line-506"><code></code></span>
<span class="codeline" id="line-507"><code>func saveblockevent(cycles, rate int64, skip int, which bucketType) {</code></span>
<span class="codeline" id="line-508"><code>	gp := getg()</code></span>
<span class="codeline" id="line-509"><code>	var nstk int</code></span>
<span class="codeline" id="line-510"><code>	var stk [maxStack]uintptr</code></span>
<span class="codeline" id="line-511"><code>	if gp.m.curg == nil || gp.m.curg == gp {</code></span>
<span class="codeline" id="line-512"><code>		nstk = callers(skip, stk[:])</code></span>
<span class="codeline" id="line-513"><code>	} else {</code></span>
<span class="codeline" id="line-514"><code>		nstk = gcallers(gp.m.curg, skip, stk[:])</code></span>
<span class="codeline" id="line-515"><code>	}</code></span>
<span class="codeline" id="line-516"><code></code></span>
<span class="codeline" id="line-517"><code>	saveBlockEventStack(cycles, rate, stk[:nstk], which)</code></span>
<span class="codeline" id="line-518"><code>}</code></span>
<span class="codeline" id="line-519"><code></code></span>
<span class="codeline" id="line-520"><code>// lockTimer assists with profiling contention on runtime-internal locks.</code></span>
<span class="codeline" id="line-521"><code>//</code></span>
<span class="codeline" id="line-522"><code>// There are several steps between the time that an M experiences contention and</code></span>
<span class="codeline" id="line-523"><code>// when that contention may be added to the profile. This comes from our</code></span>
<span class="codeline" id="line-524"><code>// constraints: We need to keep the critical section of each lock small,</code></span>
<span class="codeline" id="line-525"><code>// especially when those locks are contended. The reporting code cannot acquire</code></span>
<span class="codeline" id="line-526"><code>// new locks until the M has released all other locks, which means no memory</code></span>
<span class="codeline" id="line-527"><code>// allocations and encourages use of (temporary) M-local storage.</code></span>
<span class="codeline" id="line-528"><code>//</code></span>
<span class="codeline" id="line-529"><code>// The M will have space for storing one call stack that caused contention, and</code></span>
<span class="codeline" id="line-530"><code>// for the magnitude of that contention. It will also have space to store the</code></span>
<span class="codeline" id="line-531"><code>// magnitude of additional contention the M caused, since it only has space to</code></span>
<span class="codeline" id="line-532"><code>// remember one call stack and might encounter several contention events before</code></span>
<span class="codeline" id="line-533"><code>// it releases all of its locks and is thus able to transfer the local buffer</code></span>
<span class="codeline" id="line-534"><code>// into the profile.</code></span>
<span class="codeline" id="line-535"><code>//</code></span>
<span class="codeline" id="line-536"><code>// The M will collect the call stack when it unlocks the contended lock. That</code></span>
<span class="codeline" id="line-537"><code>// minimizes the impact on the critical section of the contended lock, and</code></span>
<span class="codeline" id="line-538"><code>// matches the mutex profile's behavior for contention in sync.Mutex: measured</code></span>
<span class="codeline" id="line-539"><code>// at the Unlock method.</code></span>
<span class="codeline" id="line-540"><code>//</code></span>
<span class="codeline" id="line-541"><code>// The profile for contention on sync.Mutex blames the caller of Unlock for the</code></span>
<span class="codeline" id="line-542"><code>// amount of contention experienced by the callers of Lock which had to wait.</code></span>
<span class="codeline" id="line-543"><code>// When there are several critical sections, this allows identifying which of</code></span>
<span class="codeline" id="line-544"><code>// them is responsible.</code></span>
<span class="codeline" id="line-545"><code>//</code></span>
<span class="codeline" id="line-546"><code>// Matching that behavior for runtime-internal locks will require identifying</code></span>
<span class="codeline" id="line-547"><code>// which Ms are blocked on the mutex. The semaphore-based implementation is</code></span>
<span class="codeline" id="line-548"><code>// ready to allow that, but the futex-based implementation will require a bit</code></span>
<span class="codeline" id="line-549"><code>// more work. Until then, we report contention on runtime-internal locks with a</code></span>
<span class="codeline" id="line-550"><code>// call stack taken from the unlock call (like the rest of the user-space</code></span>
<span class="codeline" id="line-551"><code>// "mutex" profile), but assign it a duration value based on how long the</code></span>
<span class="codeline" id="line-552"><code>// previous lock call took (like the user-space "block" profile).</code></span>
<span class="codeline" id="line-553"><code>//</code></span>
<span class="codeline" id="line-554"><code>// Thus, reporting the call stacks of runtime-internal lock contention is</code></span>
<span class="codeline" id="line-555"><code>// guarded by GODEBUG for now. Set GODEBUG=runtimecontentionstacks=1 to enable.</code></span>
<span class="codeline" id="line-556"><code>//</code></span>
<span class="codeline" id="line-557"><code>// TODO(rhysh): plumb through the delay duration, remove GODEBUG, update comment</code></span>
<span class="codeline" id="line-558"><code>//</code></span>
<span class="codeline" id="line-559"><code>// The M will track this by storing a pointer to the lock; lock/unlock pairs for</code></span>
<span class="codeline" id="line-560"><code>// runtime-internal locks are always on the same M.</code></span>
<span class="codeline" id="line-561"><code>//</code></span>
<span class="codeline" id="line-562"><code>// Together, that demands several steps for recording contention. First, when</code></span>
<span class="codeline" id="line-563"><code>// finally acquiring a contended lock, the M decides whether it should plan to</code></span>
<span class="codeline" id="line-564"><code>// profile that event by storing a pointer to the lock in its "to be profiled</code></span>
<span class="codeline" id="line-565"><code>// upon unlock" field. If that field is already set, it uses the relative</code></span>
<span class="codeline" id="line-566"><code>// magnitudes to weight a random choice between itself and the other lock, with</code></span>
<span class="codeline" id="line-567"><code>// the loser's time being added to the "additional contention" field. Otherwise</code></span>
<span class="codeline" id="line-568"><code>// if the M's call stack buffer is occupied, it does the comparison against that</code></span>
<span class="codeline" id="line-569"><code>// sample's magnitude.</code></span>
<span class="codeline" id="line-570"><code>//</code></span>
<span class="codeline" id="line-571"><code>// Second, having unlocked a mutex the M checks to see if it should capture the</code></span>
<span class="codeline" id="line-572"><code>// call stack into its local buffer. Finally, when the M unlocks its last mutex,</code></span>
<span class="codeline" id="line-573"><code>// it transfers the local buffer into the profile. As part of that step, it also</code></span>
<span class="codeline" id="line-574"><code>// transfers any "additional contention" time to the profile. Any lock</code></span>
<span class="codeline" id="line-575"><code>// contention that it experiences while adding samples to the profile will be</code></span>
<span class="codeline" id="line-576"><code>// recorded later as "additional contention" and not include a call stack, to</code></span>
<span class="codeline" id="line-577"><code>// avoid an echo.</code></span>
<span class="codeline" id="line-578"><code>type lockTimer struct {</code></span>
<span class="codeline" id="line-579"><code>	lock      *mutex</code></span>
<span class="codeline" id="line-580"><code>	timeRate  int64</code></span>
<span class="codeline" id="line-581"><code>	timeStart int64</code></span>
<span class="codeline" id="line-582"><code>	tickStart int64</code></span>
<span class="codeline" id="line-583"><code>}</code></span>
<span class="codeline" id="line-584"><code></code></span>
<span class="codeline" id="line-585"><code>func (lt *lockTimer) begin() {</code></span>
<span class="codeline" id="line-586"><code>	rate := int64(atomic.Load64(&amp;mutexprofilerate))</code></span>
<span class="codeline" id="line-587"><code></code></span>
<span class="codeline" id="line-588"><code>	lt.timeRate = gTrackingPeriod</code></span>
<span class="codeline" id="line-589"><code>	if rate != 0 &amp;&amp; rate &lt; lt.timeRate {</code></span>
<span class="codeline" id="line-590"><code>		lt.timeRate = rate</code></span>
<span class="codeline" id="line-591"><code>	}</code></span>
<span class="codeline" id="line-592"><code>	if int64(cheaprand())%lt.timeRate == 0 {</code></span>
<span class="codeline" id="line-593"><code>		lt.timeStart = nanotime()</code></span>
<span class="codeline" id="line-594"><code>	}</code></span>
<span class="codeline" id="line-595"><code></code></span>
<span class="codeline" id="line-596"><code>	if rate &gt; 0 &amp;&amp; int64(cheaprand())%rate == 0 {</code></span>
<span class="codeline" id="line-597"><code>		lt.tickStart = cputicks()</code></span>
<span class="codeline" id="line-598"><code>	}</code></span>
<span class="codeline" id="line-599"><code>}</code></span>
<span class="codeline" id="line-600"><code></code></span>
<span class="codeline" id="line-601"><code>func (lt *lockTimer) end() {</code></span>
<span class="codeline" id="line-602"><code>	gp := getg()</code></span>
<span class="codeline" id="line-603"><code></code></span>
<span class="codeline" id="line-604"><code>	if lt.timeStart != 0 {</code></span>
<span class="codeline" id="line-605"><code>		nowTime := nanotime()</code></span>
<span class="codeline" id="line-606"><code>		gp.m.mLockProfile.waitTime.Add((nowTime - lt.timeStart) * lt.timeRate)</code></span>
<span class="codeline" id="line-607"><code>	}</code></span>
<span class="codeline" id="line-608"><code></code></span>
<span class="codeline" id="line-609"><code>	if lt.tickStart != 0 {</code></span>
<span class="codeline" id="line-610"><code>		nowTick := cputicks()</code></span>
<span class="codeline" id="line-611"><code>		gp.m.mLockProfile.recordLock(nowTick-lt.tickStart, lt.lock)</code></span>
<span class="codeline" id="line-612"><code>	}</code></span>
<span class="codeline" id="line-613"><code>}</code></span>
<span class="codeline" id="line-614"><code></code></span>
<span class="codeline" id="line-615"><code>type mLockProfile struct {</code></span>
<span class="codeline" id="line-616"><code>	waitTime   atomic.Int64      // total nanoseconds spent waiting in runtime.lockWithRank</code></span>
<span class="codeline" id="line-617"><code>	stack      [maxStack]uintptr // stack that experienced contention in runtime.lockWithRank</code></span>
<span class="codeline" id="line-618"><code>	pending    uintptr           // *mutex that experienced contention (to be traceback-ed)</code></span>
<span class="codeline" id="line-619"><code>	cycles     int64             // cycles attributable to "pending" (if set), otherwise to "stack"</code></span>
<span class="codeline" id="line-620"><code>	cyclesLost int64             // contention for which we weren't able to record a call stack</code></span>
<span class="codeline" id="line-621"><code>	disabled   bool              // attribute all time to "lost"</code></span>
<span class="codeline" id="line-622"><code>}</code></span>
<span class="codeline" id="line-623"><code></code></span>
<span class="codeline" id="line-624"><code>func (prof *mLockProfile) recordLock(cycles int64, l *mutex) {</code></span>
<span class="codeline" id="line-625"><code>	if cycles &lt;= 0 {</code></span>
<span class="codeline" id="line-626"><code>		return</code></span>
<span class="codeline" id="line-627"><code>	}</code></span>
<span class="codeline" id="line-628"><code></code></span>
<span class="codeline" id="line-629"><code>	if prof.disabled {</code></span>
<span class="codeline" id="line-630"><code>		// We're experiencing contention while attempting to report contention.</code></span>
<span class="codeline" id="line-631"><code>		// Make a note of its magnitude, but don't allow it to be the sole cause</code></span>
<span class="codeline" id="line-632"><code>		// of another contention report.</code></span>
<span class="codeline" id="line-633"><code>		prof.cyclesLost += cycles</code></span>
<span class="codeline" id="line-634"><code>		return</code></span>
<span class="codeline" id="line-635"><code>	}</code></span>
<span class="codeline" id="line-636"><code></code></span>
<span class="codeline" id="line-637"><code>	if uintptr(unsafe.Pointer(l)) == prof.pending {</code></span>
<span class="codeline" id="line-638"><code>		// Optimization: we'd already planned to profile this same lock (though</code></span>
<span class="codeline" id="line-639"><code>		// possibly from a different unlock site).</code></span>
<span class="codeline" id="line-640"><code>		prof.cycles += cycles</code></span>
<span class="codeline" id="line-641"><code>		return</code></span>
<span class="codeline" id="line-642"><code>	}</code></span>
<span class="codeline" id="line-643"><code></code></span>
<span class="codeline" id="line-644"><code>	if prev := prof.cycles; prev &gt; 0 {</code></span>
<span class="codeline" id="line-645"><code>		// We can only store one call stack for runtime-internal lock contention</code></span>
<span class="codeline" id="line-646"><code>		// on this M, and we've already got one. Decide which should stay, and</code></span>
<span class="codeline" id="line-647"><code>		// add the other to the report for runtime._LostContendedRuntimeLock.</code></span>
<span class="codeline" id="line-648"><code>		prevScore := uint64(cheaprand64()) % uint64(prev)</code></span>
<span class="codeline" id="line-649"><code>		thisScore := uint64(cheaprand64()) % uint64(cycles)</code></span>
<span class="codeline" id="line-650"><code>		if prevScore &gt; thisScore {</code></span>
<span class="codeline" id="line-651"><code>			prof.cyclesLost += cycles</code></span>
<span class="codeline" id="line-652"><code>			return</code></span>
<span class="codeline" id="line-653"><code>		} else {</code></span>
<span class="codeline" id="line-654"><code>			prof.cyclesLost += prev</code></span>
<span class="codeline" id="line-655"><code>		}</code></span>
<span class="codeline" id="line-656"><code>	}</code></span>
<span class="codeline" id="line-657"><code>	// Saving the *mutex as a uintptr is safe because:</code></span>
<span class="codeline" id="line-658"><code>	//  - lockrank_on.go does this too, which gives it regular exercise</code></span>
<span class="codeline" id="line-659"><code>	//  - the lock would only move if it's stack allocated, which means it</code></span>
<span class="codeline" id="line-660"><code>	//      cannot experience multi-M contention</code></span>
<span class="codeline" id="line-661"><code>	prof.pending = uintptr(unsafe.Pointer(l))</code></span>
<span class="codeline" id="line-662"><code>	prof.cycles = cycles</code></span>
<span class="codeline" id="line-663"><code>}</code></span>
<span class="codeline" id="line-664"><code></code></span>
<span class="codeline" id="line-665"><code>// From unlock2, we might not be holding a p in this code.</code></span>
<span class="codeline" id="line-666"><code>//</code></span>
<span class="codeline" id="line-667"><code>//go:nowritebarrierrec</code></span>
<span class="codeline" id="line-668"><code>func (prof *mLockProfile) recordUnlock(l *mutex) {</code></span>
<span class="codeline" id="line-669"><code>	if uintptr(unsafe.Pointer(l)) == prof.pending {</code></span>
<span class="codeline" id="line-670"><code>		prof.captureStack()</code></span>
<span class="codeline" id="line-671"><code>	}</code></span>
<span class="codeline" id="line-672"><code>	if gp := getg(); gp.m.locks == 1 &amp;&amp; gp.m.mLockProfile.cycles != 0 {</code></span>
<span class="codeline" id="line-673"><code>		prof.store()</code></span>
<span class="codeline" id="line-674"><code>	}</code></span>
<span class="codeline" id="line-675"><code>}</code></span>
<span class="codeline" id="line-676"><code></code></span>
<span class="codeline" id="line-677"><code>func (prof *mLockProfile) captureStack() {</code></span>
<span class="codeline" id="line-678"><code>	skip := 3 // runtime.(*mLockProfile).recordUnlock runtime.unlock2 runtime.unlockWithRank</code></span>
<span class="codeline" id="line-679"><code>	if staticLockRanking {</code></span>
<span class="codeline" id="line-680"><code>		// When static lock ranking is enabled, we'll always be on the system</code></span>
<span class="codeline" id="line-681"><code>		// stack at this point. There will be a runtime.unlockWithRank.func1</code></span>
<span class="codeline" id="line-682"><code>		// frame, and if the call to runtime.unlock took place on a user stack</code></span>
<span class="codeline" id="line-683"><code>		// then there'll also be a runtime.systemstack frame. To keep stack</code></span>
<span class="codeline" id="line-684"><code>		// traces somewhat consistent whether or not static lock ranking is</code></span>
<span class="codeline" id="line-685"><code>		// enabled, we'd like to skip those. But it's hard to tell how long</code></span>
<span class="codeline" id="line-686"><code>		// we've been on the system stack so accept an extra frame in that case,</code></span>
<span class="codeline" id="line-687"><code>		// with a leaf of "runtime.unlockWithRank runtime.unlock" instead of</code></span>
<span class="codeline" id="line-688"><code>		// "runtime.unlock".</code></span>
<span class="codeline" id="line-689"><code>		skip += 1 // runtime.unlockWithRank.func1</code></span>
<span class="codeline" id="line-690"><code>	}</code></span>
<span class="codeline" id="line-691"><code>	prof.pending = 0</code></span>
<span class="codeline" id="line-692"><code></code></span>
<span class="codeline" id="line-693"><code>	if debug.runtimeContentionStacks.Load() == 0 {</code></span>
<span class="codeline" id="line-694"><code>		prof.stack[0] = abi.FuncPCABIInternal(_LostContendedRuntimeLock) + sys.PCQuantum</code></span>
<span class="codeline" id="line-695"><code>		prof.stack[1] = 0</code></span>
<span class="codeline" id="line-696"><code>		return</code></span>
<span class="codeline" id="line-697"><code>	}</code></span>
<span class="codeline" id="line-698"><code></code></span>
<span class="codeline" id="line-699"><code>	var nstk int</code></span>
<span class="codeline" id="line-700"><code>	gp := getg()</code></span>
<span class="codeline" id="line-701"><code>	sp := getcallersp()</code></span>
<span class="codeline" id="line-702"><code>	pc := getcallerpc()</code></span>
<span class="codeline" id="line-703"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-704"><code>		var u unwinder</code></span>
<span class="codeline" id="line-705"><code>		u.initAt(pc, sp, 0, gp, unwindSilentErrors|unwindJumpStack)</code></span>
<span class="codeline" id="line-706"><code>		nstk = tracebackPCs(&amp;u, skip, prof.stack[:])</code></span>
<span class="codeline" id="line-707"><code>	})</code></span>
<span class="codeline" id="line-708"><code>	if nstk &lt; len(prof.stack) {</code></span>
<span class="codeline" id="line-709"><code>		prof.stack[nstk] = 0</code></span>
<span class="codeline" id="line-710"><code>	}</code></span>
<span class="codeline" id="line-711"><code>}</code></span>
<span class="codeline" id="line-712"><code></code></span>
<span class="codeline" id="line-713"><code>func (prof *mLockProfile) store() {</code></span>
<span class="codeline" id="line-714"><code>	// Report any contention we experience within this function as "lost"; it's</code></span>
<span class="codeline" id="line-715"><code>	// important that the act of reporting a contention event not lead to a</code></span>
<span class="codeline" id="line-716"><code>	// reportable contention event. This also means we can use prof.stack</code></span>
<span class="codeline" id="line-717"><code>	// without copying, since it won't change during this function.</code></span>
<span class="codeline" id="line-718"><code>	mp := acquirem()</code></span>
<span class="codeline" id="line-719"><code>	prof.disabled = true</code></span>
<span class="codeline" id="line-720"><code></code></span>
<span class="codeline" id="line-721"><code>	nstk := maxStack</code></span>
<span class="codeline" id="line-722"><code>	for i := 0; i &lt; nstk; i++ {</code></span>
<span class="codeline" id="line-723"><code>		if pc := prof.stack[i]; pc == 0 {</code></span>
<span class="codeline" id="line-724"><code>			nstk = i</code></span>
<span class="codeline" id="line-725"><code>			break</code></span>
<span class="codeline" id="line-726"><code>		}</code></span>
<span class="codeline" id="line-727"><code>	}</code></span>
<span class="codeline" id="line-728"><code></code></span>
<span class="codeline" id="line-729"><code>	cycles, lost := prof.cycles, prof.cyclesLost</code></span>
<span class="codeline" id="line-730"><code>	prof.cycles, prof.cyclesLost = 0, 0</code></span>
<span class="codeline" id="line-731"><code></code></span>
<span class="codeline" id="line-732"><code>	rate := int64(atomic.Load64(&amp;mutexprofilerate))</code></span>
<span class="codeline" id="line-733"><code>	saveBlockEventStack(cycles, rate, prof.stack[:nstk], mutexProfile)</code></span>
<span class="codeline" id="line-734"><code>	if lost &gt; 0 {</code></span>
<span class="codeline" id="line-735"><code>		lostStk := [...]uintptr{</code></span>
<span class="codeline" id="line-736"><code>			abi.FuncPCABIInternal(_LostContendedRuntimeLock) + sys.PCQuantum,</code></span>
<span class="codeline" id="line-737"><code>		}</code></span>
<span class="codeline" id="line-738"><code>		saveBlockEventStack(lost, rate, lostStk[:], mutexProfile)</code></span>
<span class="codeline" id="line-739"><code>	}</code></span>
<span class="codeline" id="line-740"><code></code></span>
<span class="codeline" id="line-741"><code>	prof.disabled = false</code></span>
<span class="codeline" id="line-742"><code>	releasem(mp)</code></span>
<span class="codeline" id="line-743"><code>}</code></span>
<span class="codeline" id="line-744"><code></code></span>
<span class="codeline" id="line-745"><code>func saveBlockEventStack(cycles, rate int64, stk []uintptr, which bucketType) {</code></span>
<span class="codeline" id="line-746"><code>	b := stkbucket(which, 0, stk, true)</code></span>
<span class="codeline" id="line-747"><code>	bp := b.bp()</code></span>
<span class="codeline" id="line-748"><code></code></span>
<span class="codeline" id="line-749"><code>	lock(&amp;profBlockLock)</code></span>
<span class="codeline" id="line-750"><code>	// We want to up-scale the count and cycles according to the</code></span>
<span class="codeline" id="line-751"><code>	// probability that the event was sampled. For block profile events,</code></span>
<span class="codeline" id="line-752"><code>	// the sample probability is 1 if cycles &gt;= rate, and cycles / rate</code></span>
<span class="codeline" id="line-753"><code>	// otherwise. For mutex profile events, the sample probability is 1 / rate.</code></span>
<span class="codeline" id="line-754"><code>	// We scale the events by 1 / (probability the event was sampled).</code></span>
<span class="codeline" id="line-755"><code>	if which == blockProfile &amp;&amp; cycles &lt; rate {</code></span>
<span class="codeline" id="line-756"><code>		// Remove sampling bias, see discussion on http://golang.org/cl/299991.</code></span>
<span class="codeline" id="line-757"><code>		bp.count += float64(rate) / float64(cycles)</code></span>
<span class="codeline" id="line-758"><code>		bp.cycles += rate</code></span>
<span class="codeline" id="line-759"><code>	} else if which == mutexProfile {</code></span>
<span class="codeline" id="line-760"><code>		bp.count += float64(rate)</code></span>
<span class="codeline" id="line-761"><code>		bp.cycles += rate * cycles</code></span>
<span class="codeline" id="line-762"><code>	} else {</code></span>
<span class="codeline" id="line-763"><code>		bp.count++</code></span>
<span class="codeline" id="line-764"><code>		bp.cycles += cycles</code></span>
<span class="codeline" id="line-765"><code>	}</code></span>
<span class="codeline" id="line-766"><code>	unlock(&amp;profBlockLock)</code></span>
<span class="codeline" id="line-767"><code>}</code></span>
<span class="codeline" id="line-768"><code></code></span>
<span class="codeline" id="line-769"><code>var mutexprofilerate uint64 // fraction sampled</code></span>
<span class="codeline" id="line-770"><code></code></span>
<span class="codeline" id="line-771"><code>// SetMutexProfileFraction controls the fraction of mutex contention events</code></span>
<span class="codeline" id="line-772"><code>// that are reported in the mutex profile. On average 1/rate events are</code></span>
<span class="codeline" id="line-773"><code>// reported. The previous rate is returned.</code></span>
<span class="codeline" id="line-774"><code>//</code></span>
<span class="codeline" id="line-775"><code>// To turn off profiling entirely, pass rate 0.</code></span>
<span class="codeline" id="line-776"><code>// To just read the current rate, pass rate &lt; 0.</code></span>
<span class="codeline" id="line-777"><code>// (For n&gt;1 the details of sampling may change.)</code></span>
<span class="codeline" id="line-778"><code>func SetMutexProfileFraction(rate int) int {</code></span>
<span class="codeline" id="line-779"><code>	if rate &lt; 0 {</code></span>
<span class="codeline" id="line-780"><code>		return int(mutexprofilerate)</code></span>
<span class="codeline" id="line-781"><code>	}</code></span>
<span class="codeline" id="line-782"><code>	old := mutexprofilerate</code></span>
<span class="codeline" id="line-783"><code>	atomic.Store64(&amp;mutexprofilerate, uint64(rate))</code></span>
<span class="codeline" id="line-784"><code>	return int(old)</code></span>
<span class="codeline" id="line-785"><code>}</code></span>
<span class="codeline" id="line-786"><code></code></span>
<span class="codeline" id="line-787"><code>//go:linkname mutexevent sync.event</code></span>
<span class="codeline" id="line-788"><code>func mutexevent(cycles int64, skip int) {</code></span>
<span class="codeline" id="line-789"><code>	if cycles &lt; 0 {</code></span>
<span class="codeline" id="line-790"><code>		cycles = 0</code></span>
<span class="codeline" id="line-791"><code>	}</code></span>
<span class="codeline" id="line-792"><code>	rate := int64(atomic.Load64(&amp;mutexprofilerate))</code></span>
<span class="codeline" id="line-793"><code>	if rate &gt; 0 &amp;&amp; cheaprand64()%rate == 0 {</code></span>
<span class="codeline" id="line-794"><code>		saveblockevent(cycles, rate, skip+1, mutexProfile)</code></span>
<span class="codeline" id="line-795"><code>	}</code></span>
<span class="codeline" id="line-796"><code>}</code></span>
<span class="codeline" id="line-797"><code></code></span>
<span class="codeline" id="line-798"><code>// Go interface to profile data.</code></span>
<span class="codeline" id="line-799"><code></code></span>
<span class="codeline" id="line-800"><code>// A StackRecord describes a single execution stack.</code></span>
<span class="codeline" id="line-801"><code>type StackRecord struct {</code></span>
<span class="codeline" id="line-802"><code>	Stack0 [32]uintptr // stack trace for this record; ends at first 0 entry</code></span>
<span class="codeline" id="line-803"><code>}</code></span>
<span class="codeline" id="line-804"><code></code></span>
<span class="codeline" id="line-805"><code>// Stack returns the stack trace associated with the record,</code></span>
<span class="codeline" id="line-806"><code>// a prefix of r.Stack0.</code></span>
<span class="codeline" id="line-807"><code>func (r *StackRecord) Stack() []uintptr {</code></span>
<span class="codeline" id="line-808"><code>	for i, v := range r.Stack0 {</code></span>
<span class="codeline" id="line-809"><code>		if v == 0 {</code></span>
<span class="codeline" id="line-810"><code>			return r.Stack0[0:i]</code></span>
<span class="codeline" id="line-811"><code>		}</code></span>
<span class="codeline" id="line-812"><code>	}</code></span>
<span class="codeline" id="line-813"><code>	return r.Stack0[0:]</code></span>
<span class="codeline" id="line-814"><code>}</code></span>
<span class="codeline" id="line-815"><code></code></span>
<span class="codeline" id="line-816"><code>// MemProfileRate controls the fraction of memory allocations</code></span>
<span class="codeline" id="line-817"><code>// that are recorded and reported in the memory profile.</code></span>
<span class="codeline" id="line-818"><code>// The profiler aims to sample an average of</code></span>
<span class="codeline" id="line-819"><code>// one allocation per MemProfileRate bytes allocated.</code></span>
<span class="codeline" id="line-820"><code>//</code></span>
<span class="codeline" id="line-821"><code>// To include every allocated block in the profile, set MemProfileRate to 1.</code></span>
<span class="codeline" id="line-822"><code>// To turn off profiling entirely, set MemProfileRate to 0.</code></span>
<span class="codeline" id="line-823"><code>//</code></span>
<span class="codeline" id="line-824"><code>// The tools that process the memory profiles assume that the</code></span>
<span class="codeline" id="line-825"><code>// profile rate is constant across the lifetime of the program</code></span>
<span class="codeline" id="line-826"><code>// and equal to the current value. Programs that change the</code></span>
<span class="codeline" id="line-827"><code>// memory profiling rate should do so just once, as early as</code></span>
<span class="codeline" id="line-828"><code>// possible in the execution of the program (for example,</code></span>
<span class="codeline" id="line-829"><code>// at the beginning of main).</code></span>
<span class="codeline" id="line-830"><code>var MemProfileRate int = 512 * 1024</code></span>
<span class="codeline" id="line-831"><code></code></span>
<span class="codeline" id="line-832"><code>// disableMemoryProfiling is set by the linker if runtime.MemProfile</code></span>
<span class="codeline" id="line-833"><code>// is not used and the link type guarantees nobody else could use it</code></span>
<span class="codeline" id="line-834"><code>// elsewhere.</code></span>
<span class="codeline" id="line-835"><code>var disableMemoryProfiling bool</code></span>
<span class="codeline" id="line-836"><code></code></span>
<span class="codeline" id="line-837"><code>// A MemProfileRecord describes the live objects allocated</code></span>
<span class="codeline" id="line-838"><code>// by a particular call sequence (stack trace).</code></span>
<span class="codeline" id="line-839"><code>type MemProfileRecord struct {</code></span>
<span class="codeline" id="line-840"><code>	AllocBytes, FreeBytes     int64       // number of bytes allocated, freed</code></span>
<span class="codeline" id="line-841"><code>	AllocObjects, FreeObjects int64       // number of objects allocated, freed</code></span>
<span class="codeline" id="line-842"><code>	Stack0                    [32]uintptr // stack trace for this record; ends at first 0 entry</code></span>
<span class="codeline" id="line-843"><code>}</code></span>
<span class="codeline" id="line-844"><code></code></span>
<span class="codeline" id="line-845"><code>// InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes).</code></span>
<span class="codeline" id="line-846"><code>func (r *MemProfileRecord) InUseBytes() int64 { return r.AllocBytes - r.FreeBytes }</code></span>
<span class="codeline" id="line-847"><code></code></span>
<span class="codeline" id="line-848"><code>// InUseObjects returns the number of objects in use (AllocObjects - FreeObjects).</code></span>
<span class="codeline" id="line-849"><code>func (r *MemProfileRecord) InUseObjects() int64 {</code></span>
<span class="codeline" id="line-850"><code>	return r.AllocObjects - r.FreeObjects</code></span>
<span class="codeline" id="line-851"><code>}</code></span>
<span class="codeline" id="line-852"><code></code></span>
<span class="codeline" id="line-853"><code>// Stack returns the stack trace associated with the record,</code></span>
<span class="codeline" id="line-854"><code>// a prefix of r.Stack0.</code></span>
<span class="codeline" id="line-855"><code>func (r *MemProfileRecord) Stack() []uintptr {</code></span>
<span class="codeline" id="line-856"><code>	for i, v := range r.Stack0 {</code></span>
<span class="codeline" id="line-857"><code>		if v == 0 {</code></span>
<span class="codeline" id="line-858"><code>			return r.Stack0[0:i]</code></span>
<span class="codeline" id="line-859"><code>		}</code></span>
<span class="codeline" id="line-860"><code>	}</code></span>
<span class="codeline" id="line-861"><code>	return r.Stack0[0:]</code></span>
<span class="codeline" id="line-862"><code>}</code></span>
<span class="codeline" id="line-863"><code></code></span>
<span class="codeline" id="line-864"><code>// MemProfile returns a profile of memory allocated and freed per allocation</code></span>
<span class="codeline" id="line-865"><code>// site.</code></span>
<span class="codeline" id="line-866"><code>//</code></span>
<span class="codeline" id="line-867"><code>// MemProfile returns n, the number of records in the current memory profile.</code></span>
<span class="codeline" id="line-868"><code>// If len(p) &gt;= n, MemProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-869"><code>// If len(p) &lt; n, MemProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-870"><code>//</code></span>
<span class="codeline" id="line-871"><code>// If inuseZero is true, the profile includes allocation records</code></span>
<span class="codeline" id="line-872"><code>// where r.AllocBytes &gt; 0 but r.AllocBytes == r.FreeBytes.</code></span>
<span class="codeline" id="line-873"><code>// These are sites where memory was allocated, but it has all</code></span>
<span class="codeline" id="line-874"><code>// been released back to the runtime.</code></span>
<span class="codeline" id="line-875"><code>//</code></span>
<span class="codeline" id="line-876"><code>// The returned profile may be up to two garbage collection cycles old.</code></span>
<span class="codeline" id="line-877"><code>// This is to avoid skewing the profile toward allocations; because</code></span>
<span class="codeline" id="line-878"><code>// allocations happen in real time but frees are delayed until the garbage</code></span>
<span class="codeline" id="line-879"><code>// collector performs sweeping, the profile only accounts for allocations</code></span>
<span class="codeline" id="line-880"><code>// that have had a chance to be freed by the garbage collector.</code></span>
<span class="codeline" id="line-881"><code>//</code></span>
<span class="codeline" id="line-882"><code>// Most clients should use the runtime/pprof package or</code></span>
<span class="codeline" id="line-883"><code>// the testing package's -test.memprofile flag instead</code></span>
<span class="codeline" id="line-884"><code>// of calling MemProfile directly.</code></span>
<span class="codeline" id="line-885"><code>func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool) {</code></span>
<span class="codeline" id="line-886"><code>	cycle := mProfCycle.read()</code></span>
<span class="codeline" id="line-887"><code>	// If we're between mProf_NextCycle and mProf_Flush, take care</code></span>
<span class="codeline" id="line-888"><code>	// of flushing to the active profile so we only have to look</code></span>
<span class="codeline" id="line-889"><code>	// at the active profile below.</code></span>
<span class="codeline" id="line-890"><code>	index := cycle % uint32(len(memRecord{}.future))</code></span>
<span class="codeline" id="line-891"><code>	lock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-892"><code>	lock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-893"><code>	mProf_FlushLocked(index)</code></span>
<span class="codeline" id="line-894"><code>	unlock(&amp;profMemFutureLock[index])</code></span>
<span class="codeline" id="line-895"><code>	clear := true</code></span>
<span class="codeline" id="line-896"><code>	head := (*bucket)(mbuckets.Load())</code></span>
<span class="codeline" id="line-897"><code>	for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-898"><code>		mp := b.mp()</code></span>
<span class="codeline" id="line-899"><code>		if inuseZero || mp.active.alloc_bytes != mp.active.free_bytes {</code></span>
<span class="codeline" id="line-900"><code>			n++</code></span>
<span class="codeline" id="line-901"><code>		}</code></span>
<span class="codeline" id="line-902"><code>		if mp.active.allocs != 0 || mp.active.frees != 0 {</code></span>
<span class="codeline" id="line-903"><code>			clear = false</code></span>
<span class="codeline" id="line-904"><code>		}</code></span>
<span class="codeline" id="line-905"><code>	}</code></span>
<span class="codeline" id="line-906"><code>	if clear {</code></span>
<span class="codeline" id="line-907"><code>		// Absolutely no data, suggesting that a garbage collection</code></span>
<span class="codeline" id="line-908"><code>		// has not yet happened. In order to allow profiling when</code></span>
<span class="codeline" id="line-909"><code>		// garbage collection is disabled from the beginning of execution,</code></span>
<span class="codeline" id="line-910"><code>		// accumulate all of the cycles, and recount buckets.</code></span>
<span class="codeline" id="line-911"><code>		n = 0</code></span>
<span class="codeline" id="line-912"><code>		for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-913"><code>			mp := b.mp()</code></span>
<span class="codeline" id="line-914"><code>			for c := range mp.future {</code></span>
<span class="codeline" id="line-915"><code>				lock(&amp;profMemFutureLock[c])</code></span>
<span class="codeline" id="line-916"><code>				mp.active.add(&amp;mp.future[c])</code></span>
<span class="codeline" id="line-917"><code>				mp.future[c] = memRecordCycle{}</code></span>
<span class="codeline" id="line-918"><code>				unlock(&amp;profMemFutureLock[c])</code></span>
<span class="codeline" id="line-919"><code>			}</code></span>
<span class="codeline" id="line-920"><code>			if inuseZero || mp.active.alloc_bytes != mp.active.free_bytes {</code></span>
<span class="codeline" id="line-921"><code>				n++</code></span>
<span class="codeline" id="line-922"><code>			}</code></span>
<span class="codeline" id="line-923"><code>		}</code></span>
<span class="codeline" id="line-924"><code>	}</code></span>
<span class="codeline" id="line-925"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-926"><code>		ok = true</code></span>
<span class="codeline" id="line-927"><code>		idx := 0</code></span>
<span class="codeline" id="line-928"><code>		for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-929"><code>			mp := b.mp()</code></span>
<span class="codeline" id="line-930"><code>			if inuseZero || mp.active.alloc_bytes != mp.active.free_bytes {</code></span>
<span class="codeline" id="line-931"><code>				record(&amp;p[idx], b)</code></span>
<span class="codeline" id="line-932"><code>				idx++</code></span>
<span class="codeline" id="line-933"><code>			}</code></span>
<span class="codeline" id="line-934"><code>		}</code></span>
<span class="codeline" id="line-935"><code>	}</code></span>
<span class="codeline" id="line-936"><code>	unlock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-937"><code>	return</code></span>
<span class="codeline" id="line-938"><code>}</code></span>
<span class="codeline" id="line-939"><code></code></span>
<span class="codeline" id="line-940"><code>// Write b's data to r.</code></span>
<span class="codeline" id="line-941"><code>func record(r *MemProfileRecord, b *bucket) {</code></span>
<span class="codeline" id="line-942"><code>	mp := b.mp()</code></span>
<span class="codeline" id="line-943"><code>	r.AllocBytes = int64(mp.active.alloc_bytes)</code></span>
<span class="codeline" id="line-944"><code>	r.FreeBytes = int64(mp.active.free_bytes)</code></span>
<span class="codeline" id="line-945"><code>	r.AllocObjects = int64(mp.active.allocs)</code></span>
<span class="codeline" id="line-946"><code>	r.FreeObjects = int64(mp.active.frees)</code></span>
<span class="codeline" id="line-947"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-948"><code>		racewriterangepc(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0), getcallerpc(), abi.FuncPCABIInternal(MemProfile))</code></span>
<span class="codeline" id="line-949"><code>	}</code></span>
<span class="codeline" id="line-950"><code>	if msanenabled {</code></span>
<span class="codeline" id="line-951"><code>		msanwrite(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0))</code></span>
<span class="codeline" id="line-952"><code>	}</code></span>
<span class="codeline" id="line-953"><code>	if asanenabled {</code></span>
<span class="codeline" id="line-954"><code>		asanwrite(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0))</code></span>
<span class="codeline" id="line-955"><code>	}</code></span>
<span class="codeline" id="line-956"><code>	copy(r.Stack0[:], b.stk())</code></span>
<span class="codeline" id="line-957"><code>	for i := int(b.nstk); i &lt; len(r.Stack0); i++ {</code></span>
<span class="codeline" id="line-958"><code>		r.Stack0[i] = 0</code></span>
<span class="codeline" id="line-959"><code>	}</code></span>
<span class="codeline" id="line-960"><code>}</code></span>
<span class="codeline" id="line-961"><code></code></span>
<span class="codeline" id="line-962"><code>func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr)) {</code></span>
<span class="codeline" id="line-963"><code>	lock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-964"><code>	head := (*bucket)(mbuckets.Load())</code></span>
<span class="codeline" id="line-965"><code>	for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-966"><code>		mp := b.mp()</code></span>
<span class="codeline" id="line-967"><code>		fn(b, b.nstk, &amp;b.stk()[0], b.size, mp.active.allocs, mp.active.frees)</code></span>
<span class="codeline" id="line-968"><code>	}</code></span>
<span class="codeline" id="line-969"><code>	unlock(&amp;profMemActiveLock)</code></span>
<span class="codeline" id="line-970"><code>}</code></span>
<span class="codeline" id="line-971"><code></code></span>
<span class="codeline" id="line-972"><code>// BlockProfileRecord describes blocking events originated</code></span>
<span class="codeline" id="line-973"><code>// at a particular call sequence (stack trace).</code></span>
<span class="codeline" id="line-974"><code>type BlockProfileRecord struct {</code></span>
<span class="codeline" id="line-975"><code>	Count  int64</code></span>
<span class="codeline" id="line-976"><code>	Cycles int64</code></span>
<span class="codeline" id="line-977"><code>	StackRecord</code></span>
<span class="codeline" id="line-978"><code>}</code></span>
<span class="codeline" id="line-979"><code></code></span>
<span class="codeline" id="line-980"><code>// BlockProfile returns n, the number of records in the current blocking profile.</code></span>
<span class="codeline" id="line-981"><code>// If len(p) &gt;= n, BlockProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-982"><code>// If len(p) &lt; n, BlockProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-983"><code>//</code></span>
<span class="codeline" id="line-984"><code>// Most clients should use the [runtime/pprof] package or</code></span>
<span class="codeline" id="line-985"><code>// the [testing] package's -test.blockprofile flag instead</code></span>
<span class="codeline" id="line-986"><code>// of calling BlockProfile directly.</code></span>
<span class="codeline" id="line-987"><code>func BlockProfile(p []BlockProfileRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-988"><code>	lock(&amp;profBlockLock)</code></span>
<span class="codeline" id="line-989"><code>	head := (*bucket)(bbuckets.Load())</code></span>
<span class="codeline" id="line-990"><code>	for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-991"><code>		n++</code></span>
<span class="codeline" id="line-992"><code>	}</code></span>
<span class="codeline" id="line-993"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-994"><code>		ok = true</code></span>
<span class="codeline" id="line-995"><code>		for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-996"><code>			bp := b.bp()</code></span>
<span class="codeline" id="line-997"><code>			r := &amp;p[0]</code></span>
<span class="codeline" id="line-998"><code>			r.Count = int64(bp.count)</code></span>
<span class="codeline" id="line-999"><code>			// Prevent callers from having to worry about division by zero errors.</code></span>
<span class="codeline" id="line-1000"><code>			// See discussion on http://golang.org/cl/299991.</code></span>
<span class="codeline" id="line-1001"><code>			if r.Count == 0 {</code></span>
<span class="codeline" id="line-1002"><code>				r.Count = 1</code></span>
<span class="codeline" id="line-1003"><code>			}</code></span>
<span class="codeline" id="line-1004"><code>			r.Cycles = bp.cycles</code></span>
<span class="codeline" id="line-1005"><code>			if raceenabled {</code></span>
<span class="codeline" id="line-1006"><code>				racewriterangepc(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0), getcallerpc(), abi.FuncPCABIInternal(BlockProfile))</code></span>
<span class="codeline" id="line-1007"><code>			}</code></span>
<span class="codeline" id="line-1008"><code>			if msanenabled {</code></span>
<span class="codeline" id="line-1009"><code>				msanwrite(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0))</code></span>
<span class="codeline" id="line-1010"><code>			}</code></span>
<span class="codeline" id="line-1011"><code>			if asanenabled {</code></span>
<span class="codeline" id="line-1012"><code>				asanwrite(unsafe.Pointer(&amp;r.Stack0[0]), unsafe.Sizeof(r.Stack0))</code></span>
<span class="codeline" id="line-1013"><code>			}</code></span>
<span class="codeline" id="line-1014"><code>			i := copy(r.Stack0[:], b.stk())</code></span>
<span class="codeline" id="line-1015"><code>			for ; i &lt; len(r.Stack0); i++ {</code></span>
<span class="codeline" id="line-1016"><code>				r.Stack0[i] = 0</code></span>
<span class="codeline" id="line-1017"><code>			}</code></span>
<span class="codeline" id="line-1018"><code>			p = p[1:]</code></span>
<span class="codeline" id="line-1019"><code>		}</code></span>
<span class="codeline" id="line-1020"><code>	}</code></span>
<span class="codeline" id="line-1021"><code>	unlock(&amp;profBlockLock)</code></span>
<span class="codeline" id="line-1022"><code>	return</code></span>
<span class="codeline" id="line-1023"><code>}</code></span>
<span class="codeline" id="line-1024"><code></code></span>
<span class="codeline" id="line-1025"><code>// MutexProfile returns n, the number of records in the current mutex profile.</code></span>
<span class="codeline" id="line-1026"><code>// If len(p) &gt;= n, MutexProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-1027"><code>// Otherwise, MutexProfile does not change p, and returns n, false.</code></span>
<span class="codeline" id="line-1028"><code>//</code></span>
<span class="codeline" id="line-1029"><code>// Most clients should use the [runtime/pprof] package</code></span>
<span class="codeline" id="line-1030"><code>// instead of calling MutexProfile directly.</code></span>
<span class="codeline" id="line-1031"><code>func MutexProfile(p []BlockProfileRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-1032"><code>	lock(&amp;profBlockLock)</code></span>
<span class="codeline" id="line-1033"><code>	head := (*bucket)(xbuckets.Load())</code></span>
<span class="codeline" id="line-1034"><code>	for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-1035"><code>		n++</code></span>
<span class="codeline" id="line-1036"><code>	}</code></span>
<span class="codeline" id="line-1037"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-1038"><code>		ok = true</code></span>
<span class="codeline" id="line-1039"><code>		for b := head; b != nil; b = b.allnext {</code></span>
<span class="codeline" id="line-1040"><code>			bp := b.bp()</code></span>
<span class="codeline" id="line-1041"><code>			r := &amp;p[0]</code></span>
<span class="codeline" id="line-1042"><code>			r.Count = int64(bp.count)</code></span>
<span class="codeline" id="line-1043"><code>			r.Cycles = bp.cycles</code></span>
<span class="codeline" id="line-1044"><code>			i := copy(r.Stack0[:], b.stk())</code></span>
<span class="codeline" id="line-1045"><code>			for ; i &lt; len(r.Stack0); i++ {</code></span>
<span class="codeline" id="line-1046"><code>				r.Stack0[i] = 0</code></span>
<span class="codeline" id="line-1047"><code>			}</code></span>
<span class="codeline" id="line-1048"><code>			p = p[1:]</code></span>
<span class="codeline" id="line-1049"><code>		}</code></span>
<span class="codeline" id="line-1050"><code>	}</code></span>
<span class="codeline" id="line-1051"><code>	unlock(&amp;profBlockLock)</code></span>
<span class="codeline" id="line-1052"><code>	return</code></span>
<span class="codeline" id="line-1053"><code>}</code></span>
<span class="codeline" id="line-1054"><code></code></span>
<span class="codeline" id="line-1055"><code>// ThreadCreateProfile returns n, the number of records in the thread creation profile.</code></span>
<span class="codeline" id="line-1056"><code>// If len(p) &gt;= n, ThreadCreateProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-1057"><code>// If len(p) &lt; n, ThreadCreateProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-1058"><code>//</code></span>
<span class="codeline" id="line-1059"><code>// Most clients should use the runtime/pprof package instead</code></span>
<span class="codeline" id="line-1060"><code>// of calling ThreadCreateProfile directly.</code></span>
<span class="codeline" id="line-1061"><code>func ThreadCreateProfile(p []StackRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-1062"><code>	first := (*m)(atomic.Loadp(unsafe.Pointer(&amp;allm)))</code></span>
<span class="codeline" id="line-1063"><code>	for mp := first; mp != nil; mp = mp.alllink {</code></span>
<span class="codeline" id="line-1064"><code>		n++</code></span>
<span class="codeline" id="line-1065"><code>	}</code></span>
<span class="codeline" id="line-1066"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-1067"><code>		ok = true</code></span>
<span class="codeline" id="line-1068"><code>		i := 0</code></span>
<span class="codeline" id="line-1069"><code>		for mp := first; mp != nil; mp = mp.alllink {</code></span>
<span class="codeline" id="line-1070"><code>			p[i].Stack0 = mp.createstack</code></span>
<span class="codeline" id="line-1071"><code>			i++</code></span>
<span class="codeline" id="line-1072"><code>		}</code></span>
<span class="codeline" id="line-1073"><code>	}</code></span>
<span class="codeline" id="line-1074"><code>	return</code></span>
<span class="codeline" id="line-1075"><code>}</code></span>
<span class="codeline" id="line-1076"><code></code></span>
<span class="codeline" id="line-1077"><code>//go:linkname runtime_goroutineProfileWithLabels runtime/pprof.runtime_goroutineProfileWithLabels</code></span>
<span class="codeline" id="line-1078"><code>func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool) {</code></span>
<span class="codeline" id="line-1079"><code>	return goroutineProfileWithLabels(p, labels)</code></span>
<span class="codeline" id="line-1080"><code>}</code></span>
<span class="codeline" id="line-1081"><code></code></span>
<span class="codeline" id="line-1082"><code>// labels may be nil. If labels is non-nil, it must have the same length as p.</code></span>
<span class="codeline" id="line-1083"><code>func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool) {</code></span>
<span class="codeline" id="line-1084"><code>	if labels != nil &amp;&amp; len(labels) != len(p) {</code></span>
<span class="codeline" id="line-1085"><code>		labels = nil</code></span>
<span class="codeline" id="line-1086"><code>	}</code></span>
<span class="codeline" id="line-1087"><code></code></span>
<span class="codeline" id="line-1088"><code>	return goroutineProfileWithLabelsConcurrent(p, labels)</code></span>
<span class="codeline" id="line-1089"><code>}</code></span>
<span class="codeline" id="line-1090"><code></code></span>
<span class="codeline" id="line-1091"><code>var goroutineProfile = struct {</code></span>
<span class="codeline" id="line-1092"><code>	sema    uint32</code></span>
<span class="codeline" id="line-1093"><code>	active  bool</code></span>
<span class="codeline" id="line-1094"><code>	offset  atomic.Int64</code></span>
<span class="codeline" id="line-1095"><code>	records []StackRecord</code></span>
<span class="codeline" id="line-1096"><code>	labels  []unsafe.Pointer</code></span>
<span class="codeline" id="line-1097"><code>}{</code></span>
<span class="codeline" id="line-1098"><code>	sema: 1,</code></span>
<span class="codeline" id="line-1099"><code>}</code></span>
<span class="codeline" id="line-1100"><code></code></span>
<span class="codeline" id="line-1101"><code>// goroutineProfileState indicates the status of a goroutine's stack for the</code></span>
<span class="codeline" id="line-1102"><code>// current in-progress goroutine profile. Goroutines' stacks are initially</code></span>
<span class="codeline" id="line-1103"><code>// "Absent" from the profile, and end up "Satisfied" by the time the profile is</code></span>
<span class="codeline" id="line-1104"><code>// complete. While a goroutine's stack is being captured, its</code></span>
<span class="codeline" id="line-1105"><code>// goroutineProfileState will be "InProgress" and it will not be able to run</code></span>
<span class="codeline" id="line-1106"><code>// until the capture completes and the state moves to "Satisfied".</code></span>
<span class="codeline" id="line-1107"><code>//</code></span>
<span class="codeline" id="line-1108"><code>// Some goroutines (the finalizer goroutine, which at various times can be</code></span>
<span class="codeline" id="line-1109"><code>// either a "system" or a "user" goroutine, and the goroutine that is</code></span>
<span class="codeline" id="line-1110"><code>// coordinating the profile, any goroutines created during the profile) move</code></span>
<span class="codeline" id="line-1111"><code>// directly to the "Satisfied" state.</code></span>
<span class="codeline" id="line-1112"><code>type goroutineProfileState uint32</code></span>
<span class="codeline" id="line-1113"><code></code></span>
<span class="codeline" id="line-1114"><code>const (</code></span>
<span class="codeline" id="line-1115"><code>	goroutineProfileAbsent goroutineProfileState = iota</code></span>
<span class="codeline" id="line-1116"><code>	goroutineProfileInProgress</code></span>
<span class="codeline" id="line-1117"><code>	goroutineProfileSatisfied</code></span>
<span class="codeline" id="line-1118"><code>)</code></span>
<span class="codeline" id="line-1119"><code></code></span>
<span class="codeline" id="line-1120"><code>type goroutineProfileStateHolder atomic.Uint32</code></span>
<span class="codeline" id="line-1121"><code></code></span>
<span class="codeline" id="line-1122"><code>func (p *goroutineProfileStateHolder) Load() goroutineProfileState {</code></span>
<span class="codeline" id="line-1123"><code>	return goroutineProfileState((*atomic.Uint32)(p).Load())</code></span>
<span class="codeline" id="line-1124"><code>}</code></span>
<span class="codeline" id="line-1125"><code></code></span>
<span class="codeline" id="line-1126"><code>func (p *goroutineProfileStateHolder) Store(value goroutineProfileState) {</code></span>
<span class="codeline" id="line-1127"><code>	(*atomic.Uint32)(p).Store(uint32(value))</code></span>
<span class="codeline" id="line-1128"><code>}</code></span>
<span class="codeline" id="line-1129"><code></code></span>
<span class="codeline" id="line-1130"><code>func (p *goroutineProfileStateHolder) CompareAndSwap(old, new goroutineProfileState) bool {</code></span>
<span class="codeline" id="line-1131"><code>	return (*atomic.Uint32)(p).CompareAndSwap(uint32(old), uint32(new))</code></span>
<span class="codeline" id="line-1132"><code>}</code></span>
<span class="codeline" id="line-1133"><code></code></span>
<span class="codeline" id="line-1134"><code>func goroutineProfileWithLabelsConcurrent(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool) {</code></span>
<span class="codeline" id="line-1135"><code>	semacquire(&amp;goroutineProfile.sema)</code></span>
<span class="codeline" id="line-1136"><code></code></span>
<span class="codeline" id="line-1137"><code>	ourg := getg()</code></span>
<span class="codeline" id="line-1138"><code></code></span>
<span class="codeline" id="line-1139"><code>	stw := stopTheWorld(stwGoroutineProfile)</code></span>
<span class="codeline" id="line-1140"><code>	// Using gcount while the world is stopped should give us a consistent view</code></span>
<span class="codeline" id="line-1141"><code>	// of the number of live goroutines, minus the number of goroutines that are</code></span>
<span class="codeline" id="line-1142"><code>	// alive and permanently marked as "system". But to make this count agree</code></span>
<span class="codeline" id="line-1143"><code>	// with what we'd get from isSystemGoroutine, we need special handling for</code></span>
<span class="codeline" id="line-1144"><code>	// goroutines that can vary between user and system to ensure that the count</code></span>
<span class="codeline" id="line-1145"><code>	// doesn't change during the collection. So, check the finalizer goroutine</code></span>
<span class="codeline" id="line-1146"><code>	// in particular.</code></span>
<span class="codeline" id="line-1147"><code>	n = int(gcount())</code></span>
<span class="codeline" id="line-1148"><code>	if fingStatus.Load()&amp;fingRunningFinalizer != 0 {</code></span>
<span class="codeline" id="line-1149"><code>		n++</code></span>
<span class="codeline" id="line-1150"><code>	}</code></span>
<span class="codeline" id="line-1151"><code></code></span>
<span class="codeline" id="line-1152"><code>	if n &gt; len(p) {</code></span>
<span class="codeline" id="line-1153"><code>		// There's not enough space in p to store the whole profile, so (per the</code></span>
<span class="codeline" id="line-1154"><code>		// contract of runtime.GoroutineProfile) we're not allowed to write to p</code></span>
<span class="codeline" id="line-1155"><code>		// at all and must return n, false.</code></span>
<span class="codeline" id="line-1156"><code>		startTheWorld(stw)</code></span>
<span class="codeline" id="line-1157"><code>		semrelease(&amp;goroutineProfile.sema)</code></span>
<span class="codeline" id="line-1158"><code>		return n, false</code></span>
<span class="codeline" id="line-1159"><code>	}</code></span>
<span class="codeline" id="line-1160"><code></code></span>
<span class="codeline" id="line-1161"><code>	// Save current goroutine.</code></span>
<span class="codeline" id="line-1162"><code>	sp := getcallersp()</code></span>
<span class="codeline" id="line-1163"><code>	pc := getcallerpc()</code></span>
<span class="codeline" id="line-1164"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-1165"><code>		saveg(pc, sp, ourg, &amp;p[0])</code></span>
<span class="codeline" id="line-1166"><code>	})</code></span>
<span class="codeline" id="line-1167"><code>	if labels != nil {</code></span>
<span class="codeline" id="line-1168"><code>		labels[0] = ourg.labels</code></span>
<span class="codeline" id="line-1169"><code>	}</code></span>
<span class="codeline" id="line-1170"><code>	ourg.goroutineProfiled.Store(goroutineProfileSatisfied)</code></span>
<span class="codeline" id="line-1171"><code>	goroutineProfile.offset.Store(1)</code></span>
<span class="codeline" id="line-1172"><code></code></span>
<span class="codeline" id="line-1173"><code>	// Prepare for all other goroutines to enter the profile. Aside from ourg,</code></span>
<span class="codeline" id="line-1174"><code>	// every goroutine struct in the allgs list has its goroutineProfiled field</code></span>
<span class="codeline" id="line-1175"><code>	// cleared. Any goroutine created from this point on (while</code></span>
<span class="codeline" id="line-1176"><code>	// goroutineProfile.active is set) will start with its goroutineProfiled</code></span>
<span class="codeline" id="line-1177"><code>	// field set to goroutineProfileSatisfied.</code></span>
<span class="codeline" id="line-1178"><code>	goroutineProfile.active = true</code></span>
<span class="codeline" id="line-1179"><code>	goroutineProfile.records = p</code></span>
<span class="codeline" id="line-1180"><code>	goroutineProfile.labels = labels</code></span>
<span class="codeline" id="line-1181"><code>	// The finalizer goroutine needs special handling because it can vary over</code></span>
<span class="codeline" id="line-1182"><code>	// time between being a user goroutine (eligible for this profile) and a</code></span>
<span class="codeline" id="line-1183"><code>	// system goroutine (to be excluded). Pick one before restarting the world.</code></span>
<span class="codeline" id="line-1184"><code>	if fing != nil {</code></span>
<span class="codeline" id="line-1185"><code>		fing.goroutineProfiled.Store(goroutineProfileSatisfied)</code></span>
<span class="codeline" id="line-1186"><code>		if readgstatus(fing) != _Gdead &amp;&amp; !isSystemGoroutine(fing, false) {</code></span>
<span class="codeline" id="line-1187"><code>			doRecordGoroutineProfile(fing)</code></span>
<span class="codeline" id="line-1188"><code>		}</code></span>
<span class="codeline" id="line-1189"><code>	}</code></span>
<span class="codeline" id="line-1190"><code>	startTheWorld(stw)</code></span>
<span class="codeline" id="line-1191"><code></code></span>
<span class="codeline" id="line-1192"><code>	// Visit each goroutine that existed as of the startTheWorld call above.</code></span>
<span class="codeline" id="line-1193"><code>	//</code></span>
<span class="codeline" id="line-1194"><code>	// New goroutines may not be in this list, but we didn't want to know about</code></span>
<span class="codeline" id="line-1195"><code>	// them anyway. If they do appear in this list (via reusing a dead goroutine</code></span>
<span class="codeline" id="line-1196"><code>	// struct, or racing to launch between the world restarting and us getting</code></span>
<span class="codeline" id="line-1197"><code>	// the list), they will already have their goroutineProfiled field set to</code></span>
<span class="codeline" id="line-1198"><code>	// goroutineProfileSatisfied before their state transitions out of _Gdead.</code></span>
<span class="codeline" id="line-1199"><code>	//</code></span>
<span class="codeline" id="line-1200"><code>	// Any goroutine that the scheduler tries to execute concurrently with this</code></span>
<span class="codeline" id="line-1201"><code>	// call will start by adding itself to the profile (before the act of</code></span>
<span class="codeline" id="line-1202"><code>	// executing can cause any changes in its stack).</code></span>
<span class="codeline" id="line-1203"><code>	forEachGRace(func(gp1 *g) {</code></span>
<span class="codeline" id="line-1204"><code>		tryRecordGoroutineProfile(gp1, Gosched)</code></span>
<span class="codeline" id="line-1205"><code>	})</code></span>
<span class="codeline" id="line-1206"><code></code></span>
<span class="codeline" id="line-1207"><code>	stw = stopTheWorld(stwGoroutineProfileCleanup)</code></span>
<span class="codeline" id="line-1208"><code>	endOffset := goroutineProfile.offset.Swap(0)</code></span>
<span class="codeline" id="line-1209"><code>	goroutineProfile.active = false</code></span>
<span class="codeline" id="line-1210"><code>	goroutineProfile.records = nil</code></span>
<span class="codeline" id="line-1211"><code>	goroutineProfile.labels = nil</code></span>
<span class="codeline" id="line-1212"><code>	startTheWorld(stw)</code></span>
<span class="codeline" id="line-1213"><code></code></span>
<span class="codeline" id="line-1214"><code>	// Restore the invariant that every goroutine struct in allgs has its</code></span>
<span class="codeline" id="line-1215"><code>	// goroutineProfiled field cleared.</code></span>
<span class="codeline" id="line-1216"><code>	forEachGRace(func(gp1 *g) {</code></span>
<span class="codeline" id="line-1217"><code>		gp1.goroutineProfiled.Store(goroutineProfileAbsent)</code></span>
<span class="codeline" id="line-1218"><code>	})</code></span>
<span class="codeline" id="line-1219"><code></code></span>
<span class="codeline" id="line-1220"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-1221"><code>		raceacquire(unsafe.Pointer(&amp;labelSync))</code></span>
<span class="codeline" id="line-1222"><code>	}</code></span>
<span class="codeline" id="line-1223"><code></code></span>
<span class="codeline" id="line-1224"><code>	if n != int(endOffset) {</code></span>
<span class="codeline" id="line-1225"><code>		// It's a big surprise that the number of goroutines changed while we</code></span>
<span class="codeline" id="line-1226"><code>		// were collecting the profile. But probably better to return a</code></span>
<span class="codeline" id="line-1227"><code>		// truncated profile than to crash the whole process.</code></span>
<span class="codeline" id="line-1228"><code>		//</code></span>
<span class="codeline" id="line-1229"><code>		// For instance, needm moves a goroutine out of the _Gdead state and so</code></span>
<span class="codeline" id="line-1230"><code>		// might be able to change the goroutine count without interacting with</code></span>
<span class="codeline" id="line-1231"><code>		// the scheduler. For code like that, the race windows are small and the</code></span>
<span class="codeline" id="line-1232"><code>		// combination of features is uncommon, so it's hard to be (and remain)</code></span>
<span class="codeline" id="line-1233"><code>		// sure we've caught them all.</code></span>
<span class="codeline" id="line-1234"><code>	}</code></span>
<span class="codeline" id="line-1235"><code></code></span>
<span class="codeline" id="line-1236"><code>	semrelease(&amp;goroutineProfile.sema)</code></span>
<span class="codeline" id="line-1237"><code>	return n, true</code></span>
<span class="codeline" id="line-1238"><code>}</code></span>
<span class="codeline" id="line-1239"><code></code></span>
<span class="codeline" id="line-1240"><code>// tryRecordGoroutineProfileWB asserts that write barriers are allowed and calls</code></span>
<span class="codeline" id="line-1241"><code>// tryRecordGoroutineProfile.</code></span>
<span class="codeline" id="line-1242"><code>//</code></span>
<span class="codeline" id="line-1243"><code>//go:yeswritebarrierrec</code></span>
<span class="codeline" id="line-1244"><code>func tryRecordGoroutineProfileWB(gp1 *g) {</code></span>
<span class="codeline" id="line-1245"><code>	if getg().m.p.ptr() == nil {</code></span>
<span class="codeline" id="line-1246"><code>		throw("no P available, write barriers are forbidden")</code></span>
<span class="codeline" id="line-1247"><code>	}</code></span>
<span class="codeline" id="line-1248"><code>	tryRecordGoroutineProfile(gp1, osyield)</code></span>
<span class="codeline" id="line-1249"><code>}</code></span>
<span class="codeline" id="line-1250"><code></code></span>
<span class="codeline" id="line-1251"><code>// tryRecordGoroutineProfile ensures that gp1 has the appropriate representation</code></span>
<span class="codeline" id="line-1252"><code>// in the current goroutine profile: either that it should not be profiled, or</code></span>
<span class="codeline" id="line-1253"><code>// that a snapshot of its call stack and labels are now in the profile.</code></span>
<span class="codeline" id="line-1254"><code>func tryRecordGoroutineProfile(gp1 *g, yield func()) {</code></span>
<span class="codeline" id="line-1255"><code>	if readgstatus(gp1) == _Gdead {</code></span>
<span class="codeline" id="line-1256"><code>		// Dead goroutines should not appear in the profile. Goroutines that</code></span>
<span class="codeline" id="line-1257"><code>		// start while profile collection is active will get goroutineProfiled</code></span>
<span class="codeline" id="line-1258"><code>		// set to goroutineProfileSatisfied before transitioning out of _Gdead,</code></span>
<span class="codeline" id="line-1259"><code>		// so here we check _Gdead first.</code></span>
<span class="codeline" id="line-1260"><code>		return</code></span>
<span class="codeline" id="line-1261"><code>	}</code></span>
<span class="codeline" id="line-1262"><code>	if isSystemGoroutine(gp1, true) {</code></span>
<span class="codeline" id="line-1263"><code>		// System goroutines should not appear in the profile. (The finalizer</code></span>
<span class="codeline" id="line-1264"><code>		// goroutine is marked as "already profiled".)</code></span>
<span class="codeline" id="line-1265"><code>		return</code></span>
<span class="codeline" id="line-1266"><code>	}</code></span>
<span class="codeline" id="line-1267"><code></code></span>
<span class="codeline" id="line-1268"><code>	for {</code></span>
<span class="codeline" id="line-1269"><code>		prev := gp1.goroutineProfiled.Load()</code></span>
<span class="codeline" id="line-1270"><code>		if prev == goroutineProfileSatisfied {</code></span>
<span class="codeline" id="line-1271"><code>			// This goroutine is already in the profile (or is new since the</code></span>
<span class="codeline" id="line-1272"><code>			// start of collection, so shouldn't appear in the profile).</code></span>
<span class="codeline" id="line-1273"><code>			break</code></span>
<span class="codeline" id="line-1274"><code>		}</code></span>
<span class="codeline" id="line-1275"><code>		if prev == goroutineProfileInProgress {</code></span>
<span class="codeline" id="line-1276"><code>			// Something else is adding gp1 to the goroutine profile right now.</code></span>
<span class="codeline" id="line-1277"><code>			// Give that a moment to finish.</code></span>
<span class="codeline" id="line-1278"><code>			yield()</code></span>
<span class="codeline" id="line-1279"><code>			continue</code></span>
<span class="codeline" id="line-1280"><code>		}</code></span>
<span class="codeline" id="line-1281"><code></code></span>
<span class="codeline" id="line-1282"><code>		// While we have gp1.goroutineProfiled set to</code></span>
<span class="codeline" id="line-1283"><code>		// goroutineProfileInProgress, gp1 may appear _Grunnable but will not</code></span>
<span class="codeline" id="line-1284"><code>		// actually be able to run. Disable preemption for ourselves, to make</code></span>
<span class="codeline" id="line-1285"><code>		// sure we finish profiling gp1 right away instead of leaving it stuck</code></span>
<span class="codeline" id="line-1286"><code>		// in this limbo.</code></span>
<span class="codeline" id="line-1287"><code>		mp := acquirem()</code></span>
<span class="codeline" id="line-1288"><code>		if gp1.goroutineProfiled.CompareAndSwap(goroutineProfileAbsent, goroutineProfileInProgress) {</code></span>
<span class="codeline" id="line-1289"><code>			doRecordGoroutineProfile(gp1)</code></span>
<span class="codeline" id="line-1290"><code>			gp1.goroutineProfiled.Store(goroutineProfileSatisfied)</code></span>
<span class="codeline" id="line-1291"><code>		}</code></span>
<span class="codeline" id="line-1292"><code>		releasem(mp)</code></span>
<span class="codeline" id="line-1293"><code>	}</code></span>
<span class="codeline" id="line-1294"><code>}</code></span>
<span class="codeline" id="line-1295"><code></code></span>
<span class="codeline" id="line-1296"><code>// doRecordGoroutineProfile writes gp1's call stack and labels to an in-progress</code></span>
<span class="codeline" id="line-1297"><code>// goroutine profile. Preemption is disabled.</code></span>
<span class="codeline" id="line-1298"><code>//</code></span>
<span class="codeline" id="line-1299"><code>// This may be called via tryRecordGoroutineProfile in two ways: by the</code></span>
<span class="codeline" id="line-1300"><code>// goroutine that is coordinating the goroutine profile (running on its own</code></span>
<span class="codeline" id="line-1301"><code>// stack), or from the scheduler in preparation to execute gp1 (running on the</code></span>
<span class="codeline" id="line-1302"><code>// system stack).</code></span>
<span class="codeline" id="line-1303"><code>func doRecordGoroutineProfile(gp1 *g) {</code></span>
<span class="codeline" id="line-1304"><code>	if readgstatus(gp1) == _Grunning {</code></span>
<span class="codeline" id="line-1305"><code>		print("doRecordGoroutineProfile gp1=", gp1.goid, "\n")</code></span>
<span class="codeline" id="line-1306"><code>		throw("cannot read stack of running goroutine")</code></span>
<span class="codeline" id="line-1307"><code>	}</code></span>
<span class="codeline" id="line-1308"><code></code></span>
<span class="codeline" id="line-1309"><code>	offset := int(goroutineProfile.offset.Add(1)) - 1</code></span>
<span class="codeline" id="line-1310"><code></code></span>
<span class="codeline" id="line-1311"><code>	if offset &gt;= len(goroutineProfile.records) {</code></span>
<span class="codeline" id="line-1312"><code>		// Should be impossible, but better to return a truncated profile than</code></span>
<span class="codeline" id="line-1313"><code>		// to crash the entire process at this point. Instead, deal with it in</code></span>
<span class="codeline" id="line-1314"><code>		// goroutineProfileWithLabelsConcurrent where we have more context.</code></span>
<span class="codeline" id="line-1315"><code>		return</code></span>
<span class="codeline" id="line-1316"><code>	}</code></span>
<span class="codeline" id="line-1317"><code></code></span>
<span class="codeline" id="line-1318"><code>	// saveg calls gentraceback, which may call cgo traceback functions. When</code></span>
<span class="codeline" id="line-1319"><code>	// called from the scheduler, this is on the system stack already so</code></span>
<span class="codeline" id="line-1320"><code>	// traceback.go:cgoContextPCs will avoid calling back into the scheduler.</code></span>
<span class="codeline" id="line-1321"><code>	//</code></span>
<span class="codeline" id="line-1322"><code>	// When called from the goroutine coordinating the profile, we still have</code></span>
<span class="codeline" id="line-1323"><code>	// set gp1.goroutineProfiled to goroutineProfileInProgress and so are still</code></span>
<span class="codeline" id="line-1324"><code>	// preventing it from being truly _Grunnable. So we'll use the system stack</code></span>
<span class="codeline" id="line-1325"><code>	// to avoid schedule delays.</code></span>
<span class="codeline" id="line-1326"><code>	systemstack(func() { saveg(^uintptr(0), ^uintptr(0), gp1, &amp;goroutineProfile.records[offset]) })</code></span>
<span class="codeline" id="line-1327"><code></code></span>
<span class="codeline" id="line-1328"><code>	if goroutineProfile.labels != nil {</code></span>
<span class="codeline" id="line-1329"><code>		goroutineProfile.labels[offset] = gp1.labels</code></span>
<span class="codeline" id="line-1330"><code>	}</code></span>
<span class="codeline" id="line-1331"><code>}</code></span>
<span class="codeline" id="line-1332"><code></code></span>
<span class="codeline" id="line-1333"><code>func goroutineProfileWithLabelsSync(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool) {</code></span>
<span class="codeline" id="line-1334"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1335"><code></code></span>
<span class="codeline" id="line-1336"><code>	isOK := func(gp1 *g) bool {</code></span>
<span class="codeline" id="line-1337"><code>		// Checking isSystemGoroutine here makes GoroutineProfile</code></span>
<span class="codeline" id="line-1338"><code>		// consistent with both NumGoroutine and Stack.</code></span>
<span class="codeline" id="line-1339"><code>		return gp1 != gp &amp;&amp; readgstatus(gp1) != _Gdead &amp;&amp; !isSystemGoroutine(gp1, false)</code></span>
<span class="codeline" id="line-1340"><code>	}</code></span>
<span class="codeline" id="line-1341"><code></code></span>
<span class="codeline" id="line-1342"><code>	stw := stopTheWorld(stwGoroutineProfile)</code></span>
<span class="codeline" id="line-1343"><code></code></span>
<span class="codeline" id="line-1344"><code>	// World is stopped, no locking required.</code></span>
<span class="codeline" id="line-1345"><code>	n = 1</code></span>
<span class="codeline" id="line-1346"><code>	forEachGRace(func(gp1 *g) {</code></span>
<span class="codeline" id="line-1347"><code>		if isOK(gp1) {</code></span>
<span class="codeline" id="line-1348"><code>			n++</code></span>
<span class="codeline" id="line-1349"><code>		}</code></span>
<span class="codeline" id="line-1350"><code>	})</code></span>
<span class="codeline" id="line-1351"><code></code></span>
<span class="codeline" id="line-1352"><code>	if n &lt;= len(p) {</code></span>
<span class="codeline" id="line-1353"><code>		ok = true</code></span>
<span class="codeline" id="line-1354"><code>		r, lbl := p, labels</code></span>
<span class="codeline" id="line-1355"><code></code></span>
<span class="codeline" id="line-1356"><code>		// Save current goroutine.</code></span>
<span class="codeline" id="line-1357"><code>		sp := getcallersp()</code></span>
<span class="codeline" id="line-1358"><code>		pc := getcallerpc()</code></span>
<span class="codeline" id="line-1359"><code>		systemstack(func() {</code></span>
<span class="codeline" id="line-1360"><code>			saveg(pc, sp, gp, &amp;r[0])</code></span>
<span class="codeline" id="line-1361"><code>		})</code></span>
<span class="codeline" id="line-1362"><code>		r = r[1:]</code></span>
<span class="codeline" id="line-1363"><code></code></span>
<span class="codeline" id="line-1364"><code>		// If we have a place to put our goroutine labelmap, insert it there.</code></span>
<span class="codeline" id="line-1365"><code>		if labels != nil {</code></span>
<span class="codeline" id="line-1366"><code>			lbl[0] = gp.labels</code></span>
<span class="codeline" id="line-1367"><code>			lbl = lbl[1:]</code></span>
<span class="codeline" id="line-1368"><code>		}</code></span>
<span class="codeline" id="line-1369"><code></code></span>
<span class="codeline" id="line-1370"><code>		// Save other goroutines.</code></span>
<span class="codeline" id="line-1371"><code>		forEachGRace(func(gp1 *g) {</code></span>
<span class="codeline" id="line-1372"><code>			if !isOK(gp1) {</code></span>
<span class="codeline" id="line-1373"><code>				return</code></span>
<span class="codeline" id="line-1374"><code>			}</code></span>
<span class="codeline" id="line-1375"><code></code></span>
<span class="codeline" id="line-1376"><code>			if len(r) == 0 {</code></span>
<span class="codeline" id="line-1377"><code>				// Should be impossible, but better to return a</code></span>
<span class="codeline" id="line-1378"><code>				// truncated profile than to crash the entire process.</code></span>
<span class="codeline" id="line-1379"><code>				return</code></span>
<span class="codeline" id="line-1380"><code>			}</code></span>
<span class="codeline" id="line-1381"><code>			// saveg calls gentraceback, which may call cgo traceback functions.</code></span>
<span class="codeline" id="line-1382"><code>			// The world is stopped, so it cannot use cgocall (which will be</code></span>
<span class="codeline" id="line-1383"><code>			// blocked at exitsyscall). Do it on the system stack so it won't</code></span>
<span class="codeline" id="line-1384"><code>			// call into the schedular (see traceback.go:cgoContextPCs).</code></span>
<span class="codeline" id="line-1385"><code>			systemstack(func() { saveg(^uintptr(0), ^uintptr(0), gp1, &amp;r[0]) })</code></span>
<span class="codeline" id="line-1386"><code>			if labels != nil {</code></span>
<span class="codeline" id="line-1387"><code>				lbl[0] = gp1.labels</code></span>
<span class="codeline" id="line-1388"><code>				lbl = lbl[1:]</code></span>
<span class="codeline" id="line-1389"><code>			}</code></span>
<span class="codeline" id="line-1390"><code>			r = r[1:]</code></span>
<span class="codeline" id="line-1391"><code>		})</code></span>
<span class="codeline" id="line-1392"><code>	}</code></span>
<span class="codeline" id="line-1393"><code></code></span>
<span class="codeline" id="line-1394"><code>	if raceenabled {</code></span>
<span class="codeline" id="line-1395"><code>		raceacquire(unsafe.Pointer(&amp;labelSync))</code></span>
<span class="codeline" id="line-1396"><code>	}</code></span>
<span class="codeline" id="line-1397"><code></code></span>
<span class="codeline" id="line-1398"><code>	startTheWorld(stw)</code></span>
<span class="codeline" id="line-1399"><code>	return n, ok</code></span>
<span class="codeline" id="line-1400"><code>}</code></span>
<span class="codeline" id="line-1401"><code></code></span>
<span class="codeline" id="line-1402"><code>// GoroutineProfile returns n, the number of records in the active goroutine stack profile.</code></span>
<span class="codeline" id="line-1403"><code>// If len(p) &gt;= n, GoroutineProfile copies the profile into p and returns n, true.</code></span>
<span class="codeline" id="line-1404"><code>// If len(p) &lt; n, GoroutineProfile does not change p and returns n, false.</code></span>
<span class="codeline" id="line-1405"><code>//</code></span>
<span class="codeline" id="line-1406"><code>// Most clients should use the [runtime/pprof] package instead</code></span>
<span class="codeline" id="line-1407"><code>// of calling GoroutineProfile directly.</code></span>
<span class="codeline" id="line-1408"><code>func GoroutineProfile(p []StackRecord) (n int, ok bool) {</code></span>
<span class="codeline" id="line-1409"><code></code></span>
<span class="codeline" id="line-1410"><code>	return goroutineProfileWithLabels(p, nil)</code></span>
<span class="codeline" id="line-1411"><code>}</code></span>
<span class="codeline" id="line-1412"><code></code></span>
<span class="codeline" id="line-1413"><code>func saveg(pc, sp uintptr, gp *g, r *StackRecord) {</code></span>
<span class="codeline" id="line-1414"><code>	var u unwinder</code></span>
<span class="codeline" id="line-1415"><code>	u.initAt(pc, sp, 0, gp, unwindSilentErrors)</code></span>
<span class="codeline" id="line-1416"><code>	n := tracebackPCs(&amp;u, 0, r.Stack0[:])</code></span>
<span class="codeline" id="line-1417"><code>	if n &lt; len(r.Stack0) {</code></span>
<span class="codeline" id="line-1418"><code>		r.Stack0[n] = 0</code></span>
<span class="codeline" id="line-1419"><code>	}</code></span>
<span class="codeline" id="line-1420"><code>}</code></span>
<span class="codeline" id="line-1421"><code></code></span>
<span class="codeline" id="line-1422"><code>// Stack formats a stack trace of the calling goroutine into buf</code></span>
<span class="codeline" id="line-1423"><code>// and returns the number of bytes written to buf.</code></span>
<span class="codeline" id="line-1424"><code>// If all is true, Stack formats stack traces of all other goroutines</code></span>
<span class="codeline" id="line-1425"><code>// into buf after the trace for the current goroutine.</code></span>
<span class="codeline" id="line-1426"><code>func Stack(buf []byte, all bool) int {</code></span>
<span class="codeline" id="line-1427"><code>	var stw worldStop</code></span>
<span class="codeline" id="line-1428"><code>	if all {</code></span>
<span class="codeline" id="line-1429"><code>		stw = stopTheWorld(stwAllGoroutinesStack)</code></span>
<span class="codeline" id="line-1430"><code>	}</code></span>
<span class="codeline" id="line-1431"><code></code></span>
<span class="codeline" id="line-1432"><code>	n := 0</code></span>
<span class="codeline" id="line-1433"><code>	if len(buf) &gt; 0 {</code></span>
<span class="codeline" id="line-1434"><code>		gp := getg()</code></span>
<span class="codeline" id="line-1435"><code>		sp := getcallersp()</code></span>
<span class="codeline" id="line-1436"><code>		pc := getcallerpc()</code></span>
<span class="codeline" id="line-1437"><code>		systemstack(func() {</code></span>
<span class="codeline" id="line-1438"><code>			g0 := getg()</code></span>
<span class="codeline" id="line-1439"><code>			// Force traceback=1 to override GOTRACEBACK setting,</code></span>
<span class="codeline" id="line-1440"><code>			// so that Stack's results are consistent.</code></span>
<span class="codeline" id="line-1441"><code>			// GOTRACEBACK is only about crash dumps.</code></span>
<span class="codeline" id="line-1442"><code>			g0.m.traceback = 1</code></span>
<span class="codeline" id="line-1443"><code>			g0.writebuf = buf[0:0:len(buf)]</code></span>
<span class="codeline" id="line-1444"><code>			goroutineheader(gp)</code></span>
<span class="codeline" id="line-1445"><code>			traceback(pc, sp, 0, gp)</code></span>
<span class="codeline" id="line-1446"><code>			if all {</code></span>
<span class="codeline" id="line-1447"><code>				tracebackothers(gp)</code></span>
<span class="codeline" id="line-1448"><code>			}</code></span>
<span class="codeline" id="line-1449"><code>			g0.m.traceback = 0</code></span>
<span class="codeline" id="line-1450"><code>			n = len(g0.writebuf)</code></span>
<span class="codeline" id="line-1451"><code>			g0.writebuf = nil</code></span>
<span class="codeline" id="line-1452"><code>		})</code></span>
<span class="codeline" id="line-1453"><code>	}</code></span>
<span class="codeline" id="line-1454"><code></code></span>
<span class="codeline" id="line-1455"><code>	if all {</code></span>
<span class="codeline" id="line-1456"><code>		startTheWorld(stw)</code></span>
<span class="codeline" id="line-1457"><code>	}</code></span>
<span class="codeline" id="line-1458"><code>	return n</code></span>
<span class="codeline" id="line-1459"><code>}</code></span>
<span class="codeline" id="line-1460"><code></code></span>
<span class="codeline" id="line-1461"><code>// Tracing of alloc/free/gc.</code></span>
<span class="codeline" id="line-1462"><code></code></span>
<span class="codeline" id="line-1463"><code>var tracelock mutex</code></span>
<span class="codeline" id="line-1464"><code></code></span>
<span class="codeline" id="line-1465"><code>func tracealloc(p unsafe.Pointer, size uintptr, typ *_type) {</code></span>
<span class="codeline" id="line-1466"><code>	lock(&amp;tracelock)</code></span>
<span class="codeline" id="line-1467"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1468"><code>	gp.m.traceback = 2</code></span>
<span class="codeline" id="line-1469"><code>	if typ == nil {</code></span>
<span class="codeline" id="line-1470"><code>		print("tracealloc(", p, ", ", hex(size), ")\n")</code></span>
<span class="codeline" id="line-1471"><code>	} else {</code></span>
<span class="codeline" id="line-1472"><code>		print("tracealloc(", p, ", ", hex(size), ", ", toRType(typ).string(), ")\n")</code></span>
<span class="codeline" id="line-1473"><code>	}</code></span>
<span class="codeline" id="line-1474"><code>	if gp.m.curg == nil || gp == gp.m.curg {</code></span>
<span class="codeline" id="line-1475"><code>		goroutineheader(gp)</code></span>
<span class="codeline" id="line-1476"><code>		pc := getcallerpc()</code></span>
<span class="codeline" id="line-1477"><code>		sp := getcallersp()</code></span>
<span class="codeline" id="line-1478"><code>		systemstack(func() {</code></span>
<span class="codeline" id="line-1479"><code>			traceback(pc, sp, 0, gp)</code></span>
<span class="codeline" id="line-1480"><code>		})</code></span>
<span class="codeline" id="line-1481"><code>	} else {</code></span>
<span class="codeline" id="line-1482"><code>		goroutineheader(gp.m.curg)</code></span>
<span class="codeline" id="line-1483"><code>		traceback(^uintptr(0), ^uintptr(0), 0, gp.m.curg)</code></span>
<span class="codeline" id="line-1484"><code>	}</code></span>
<span class="codeline" id="line-1485"><code>	print("\n")</code></span>
<span class="codeline" id="line-1486"><code>	gp.m.traceback = 0</code></span>
<span class="codeline" id="line-1487"><code>	unlock(&amp;tracelock)</code></span>
<span class="codeline" id="line-1488"><code>}</code></span>
<span class="codeline" id="line-1489"><code></code></span>
<span class="codeline" id="line-1490"><code>func tracefree(p unsafe.Pointer, size uintptr) {</code></span>
<span class="codeline" id="line-1491"><code>	lock(&amp;tracelock)</code></span>
<span class="codeline" id="line-1492"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1493"><code>	gp.m.traceback = 2</code></span>
<span class="codeline" id="line-1494"><code>	print("tracefree(", p, ", ", hex(size), ")\n")</code></span>
<span class="codeline" id="line-1495"><code>	goroutineheader(gp)</code></span>
<span class="codeline" id="line-1496"><code>	pc := getcallerpc()</code></span>
<span class="codeline" id="line-1497"><code>	sp := getcallersp()</code></span>
<span class="codeline" id="line-1498"><code>	systemstack(func() {</code></span>
<span class="codeline" id="line-1499"><code>		traceback(pc, sp, 0, gp)</code></span>
<span class="codeline" id="line-1500"><code>	})</code></span>
<span class="codeline" id="line-1501"><code>	print("\n")</code></span>
<span class="codeline" id="line-1502"><code>	gp.m.traceback = 0</code></span>
<span class="codeline" id="line-1503"><code>	unlock(&amp;tracelock)</code></span>
<span class="codeline" id="line-1504"><code>}</code></span>
<span class="codeline" id="line-1505"><code></code></span>
<span class="codeline" id="line-1506"><code>func tracegc() {</code></span>
<span class="codeline" id="line-1507"><code>	lock(&amp;tracelock)</code></span>
<span class="codeline" id="line-1508"><code>	gp := getg()</code></span>
<span class="codeline" id="line-1509"><code>	gp.m.traceback = 2</code></span>
<span class="codeline" id="line-1510"><code>	print("tracegc()\n")</code></span>
<span class="codeline" id="line-1511"><code>	// running on m-&gt;g0 stack; show all non-g0 goroutines</code></span>
<span class="codeline" id="line-1512"><code>	tracebackothers(gp)</code></span>
<span class="codeline" id="line-1513"><code>	print("end tracegc\n")</code></span>
<span class="codeline" id="line-1514"><code>	print("\n")</code></span>
<span class="codeline" id="line-1515"><code>	gp.m.traceback = 0</code></span>
<span class="codeline" id="line-1516"><code>	unlock(&amp;tracelock)</code></span>
<span class="codeline" id="line-1517"><code>}</code></span>
</pre><pre id="footer">
<table><tr><td><img src="../../png/go101-twitter.png"></td>
<td>The pages are generated with <a href="https://go101.org/apps-and-libs/golds.html"><b>Golds</b></a> <i>v0.6.8</i>. (GOOS=linux GOARCH=amd64)
<b>Golds</b> is a <a href="https://go101.org">Go 101</a> project developed by <a href="https://tapirgames.com">Tapir Liu</a>.
PR and bug reports are welcome and can be submitted to <a href="https://github.com/go101/golds">the issue list</a>.
Please follow <a href="https://twitter.com/go100and1">@Go100and1</a> (reachable from the left QR code) to get the latest news of <b>Golds</b>.</td></tr></table></pre>